
R version 3.0.1 (2013-05-16) -- "Good Sport"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library( maxLik )
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> options(digits=5)
> 
> ### activePar
> # a simple two-dimensional exponential hat
> f <- function(a) exp(-a[1]^2 - a[2]^2)
> #
> # maximize wrt. both parameters 
> free <- maxNR(f, start=1:2)
> print( free )
$maximum
[1] 1

$estimate
[1] -3.3317e-11 -5.6433e-11

$gradient
[1] 1.1102e-10 1.1102e-10

$hessian
     [,1]    [,2]
[1,]   -2  0.0000
[2,]    0 -2.0002

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(free)  # results should be close to (0,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
        estimate   gradient
[1,] -3.3317e-11 1.1102e-10
[2,] -5.6433e-11 1.1102e-10
--------------------------------------------
> activePar(free)
[1] TRUE TRUE
> # allow only the second parameter to vary
> cons <- maxNR(f, start=1:2, activePar=c(FALSE,TRUE))
> print( cons )
$maximum
[1] 0.36788

$estimate
[1] 1.0000e+00 3.5151e-07

$gradient
[1]          NA -2.5863e-07

$hessian
     [,1]     [,2]
[1,]   NA       NA
[2,]   NA -0.73569

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1]  TRUE FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(cons) # result should be around (1,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.36788 
Estimates:
       estimate    gradient
[1,] 1.0000e+00          NA
[2,] 3.5151e-07 -2.5863e-07
--------------------------------------------
> activePar(cons)
[1] FALSE  TRUE
> # specify fixed par in different ways
> cons2 <- maxNR(f, start=1:2, fixed=1)
> all.equal( cons, cons2 )
[1] TRUE
> cons3 <- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
> all.equal( cons, cons3 )
[1] TRUE
> cons4 <- maxNR(f, start=c(a=1, b=2), fixed="a")
> print(summary(cons4))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.36788 
Estimates:
    estimate    gradient
a 1.0000e+00          NA
b 3.5151e-07 -2.5863e-07
--------------------------------------------
> all.equal( cons, cons4 )
[1] "Component 2: names for current but not for target"                             
[2] "Component 3: names for current but not for target"                             
[3] "Component 4: Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component 8: names for current but not for target"                             
> 
> ### compareDerivatives
> set.seed( 2 )
> ## A simple example with sin(x)' = cos(x)
> f <- sin
> compareDerivatives(f, cos, t0=1)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.84147
Dim of analytic gradient: 1 1 
       numeric          : 1 1 
      
param  theta 0 analytic numeric    rel.diff
  [1,]       1   0.5403  0.5403 -5.1299e-11
Max relative difference: 5.1299e-11 
-------- END of compare derivatives -------- 
> ##
> ## Example of log-likelihood of normal density.  Two-parameter
> ## function.
> x <- rnorm(100, 1, 2) # generate rnorm x
> l <- function(b) sum(log(dnorm((x-b[1])/b[2])/b[2]))
>               # b[1] - mu, b[2] - sigma
> gradl <- function(b) {
+    c(sum(x - b[1])/b[2]^2,
+    sum((x - b[1])^2/b[2]^3 - 1/b[2]))
+ }
> compareDerivatives(l, gradl, t0=c(1,2))
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] -227.88
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
        [,1]   [,2]
[1,] -1.5349 16.676
numeric gradient
        [,1]   [,2]
[1,] -1.5349 16.676
(anal-num)/(0.5*(abs(anal)+abs(num)))
            [,1]        [,2]
[1,] -1.9886e-09 -2.0895e-10
Max relative difference: 1.9886e-09 
-------- END of compare derivatives -------- 
> 
> 
> ### hessian
> set.seed( 3 )
> # log-likelihood for normal density
> # a[1] - mean
> # a[2] - standard deviation
> ll <- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
> x <- rnorm(1000) # sample from standard normal
> ml <- maxLik(ll, start=c(1,1))
> # ignore eventual warnings "NaNs produced in: log(x)"
> print.default( ml )
$maximum
[1] -497.57

$estimate
[1] 0.0063965 0.9975763

$gradient
[1] 0.0000e+00 5.6843e-08

$hessian
        [,1]    [,2]
[1,] -1004.8     0.0
[2,]     0.0 -2009.8

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.57 (2 free parameter(s))
Estimate(s): 0.0063965 0.99758 
> summary(ml) # result should be close to c(0,1)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.57 
2  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.0064     0.0316     0.2    0.84    
[2,]   0.9976     0.0223    44.7  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> hessian(ml) # How the Hessian looks like
        [,1]    [,2]
[1,] -1004.8     0.0
[2,]     0.0 -2009.8
> sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
         [,1]     [,2]
[1,] 0.031547 0.000000
[2,] 0.000000 0.022306
> print(stdEr(ml))
[1] 0.031547 0.022306
>                            # test vector of stdEr-s
> #
> # Now run the same example while fixing a[2] = 1
> mlf <- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
> print.default( mlf )
$maximum
[1] -497.58

$estimate
[1] 0.0063965 1.0000000

$gradient
[1]  0 NA

$hessian
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE  TRUE

$iterations
[1] 3

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( mlf )
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.58 (1 free parameter(s))
Estimate(s): 0.0063965 1 
> summary(mlf) # first parameter close to 0, the second exactly 1.0
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.58 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)
[1,]   0.0064     0.0316     0.2    0.84
[2,]   1.0000     0.0000      NA      NA
--------------------------------------------
> hessian(mlf)
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA
> # now invert only the free parameter part of the Hessian
> sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
         [,1]
[1,] 0.031622
> # gives the standard deviation for the mean
> print(stdEr(mlf))
[1] 0.031622 0.000000
>                            # test standard errors with fixed par
> 
> 
> ### maxBFGS
> set.seed( 5 )
> # Maximum Likelihood estimation of the parameter of Poissonian distribution
> n <- rpois(100, 3)
> loglik <- function(l) n*log(l) - l - lfactorial(n)
> # we use numeric gradient
> a <- maxBFGS(loglik, start=1)
> print( a )
$maximum
[1] -199.21

$estimate
[1] 3.19

$gradient
[1] 1.5834e-05

$hessian
        [,1]
[1,] -31.292

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE

$iterations
function 
      29 

$type
[1] "BFGS maximisation"

$constraints
NULL

$gradientObs
            [,1]
  [1,] -0.373041
  [2,]  0.253919
  [3,]  0.880878
  [4,] -0.373041
  [5,] -0.686520
  [6,]  0.253919
  [7,] -0.059561
  [8,]  0.253919
  [9,]  0.880878
 [10,] -0.686520
 [11,] -0.373041
 [12,] -0.059561
 [13,] -0.373041
 [14,] -0.059561
 [15,] -0.373041
 [16,] -0.373041
 [17,] -0.373041
 [18,]  0.567398
 [19,] -0.059561
 [20,]  0.567398
 [21,]  0.567398
 [22,]  0.253919
 [23,] -0.373041
 [24,] -0.373041
 [25,] -0.686520
 [26,] -0.059561
 [27,] -0.059561
 [28,]  0.880878
 [29,] -0.686520
 [30,]  0.880878
 [31,] -0.059561
 [32,] -0.686520
 [33,] -0.373041
 [34,] -1.000000
 [35,] -1.000000
 [36,] -0.059561
 [37,] -0.059561
 [38,] -0.059561
 [39,] -0.373041
 [40,] -0.373041
 [41,]  0.567398
 [42,] -0.373041
 [43,]  0.567398
 [44,] -0.059561
 [45,]  0.880878
 [46,] -0.059561
 [47,]  0.253919
 [48,] -0.686520
 [49,]  0.253919
 [50,] -0.059561
 [51,] -0.373041
 [52,] -0.059561
 [53,]  0.880878
 [54,]  1.194358
 [55,]  0.880878
 [56,] -0.373041
 [57,] -0.373041
 [58,] -0.373041
 [59,] -0.686520
 [60,] -0.686520
 [61,] -0.059561
 [62,] -0.686520
 [63,]  0.567398
 [64,]  1.507837
 [65,]  1.194358
 [66,]  0.567398
 [67,]  0.567398
 [68,] -0.686520
 [69,]  0.880878
 [70,]  0.567398
 [71,]  0.567398
 [72,]  0.253919
 [73,] -0.686520
 [74,]  0.567398
 [75,] -1.000000
 [76,]  0.880878
 [77,] -1.000000
 [78,]  0.253919
 [79,] -0.059561
 [80,] -0.373041
 [81,]  0.880878
 [82,]  0.567398
 [83,] -0.373041
 [84,] -0.686520
 [85,] -0.059561
 [86,] -0.686520
 [87,]  1.194358
 [88,]  0.253919
 [89,] -0.373041
 [90,] -0.059561
 [91,]  0.253919
 [92,] -0.373041
 [93,] -0.686520
 [94,] -0.373041
 [95,] -0.373041
 [96,]  0.567398
 [97,] -0.373041
 [98,] -0.059561
 [99,] -0.059561
[100,] -0.059561

attr(,"class")
[1] "maxim"
> summary( a )
--------------------------------------------
BFGS maximisation 
Number of iterations: 29 
Return code: 0 
successful convergence  
Function value: -199.21 
Estimates:
     estimate   gradient
[1,]     3.19 1.5834e-05
--------------------------------------------
> # you would probably prefer mean(n) instead of that ;-)
> # Note also that maxLik is better suited for Maximum Likelihood
> 
> 
> ### logLik.maxLik
> set.seed( 4 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -25.054

$estimate
[1] 2.1159

$gradient
[1] 6.0596e-08

$hessian
        [,1]
[1,] -22.337

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
             [,1]
  [1,]  0.3868207
  [2,] -1.6793514
  [3,]  0.0385682
  [4,]  0.0712975
  [5,]  0.1590468
  [6,]  0.1051919
  [7,]  0.2482151
  [8,]  0.4472711
  [9,]  0.2179460
 [10,]  0.0540462
 [11,] -0.8675279
 [12,]  0.3285826
 [13,]  0.2702262
 [14,]  0.2581125
 [15,]  0.3028197
 [16,] -0.0519880
 [17,]  0.4428435
 [18,]  0.4055088
 [19,] -0.4473662
 [20,] -0.0333854
 [21,]  0.3505646
 [22,] -0.1507886
 [23,] -2.2972631
 [24,]  0.3886909
 [25,] -0.4441226
 [26,]  0.4434079
 [27,]  0.2768729
 [28,] -0.1511725
 [29,]  0.2266922
 [30,]  0.1922164
 [31,] -0.2163525
 [32,] -0.4273121
 [33,] -0.4156724
 [34,]  0.2781988
 [35,] -0.6369704
 [36,]  0.3945168
 [37,]  0.3440609
 [38,] -0.6202597
 [39,]  0.4577671
 [40,]  0.1672038
 [41,]  0.3537757
 [42,] -0.0653407
 [43,]  0.1477482
 [44,]  0.2827205
 [45,] -0.0152426
 [46,]  0.0798823
 [47,]  0.2743718
 [48,]  0.4523042
 [49,] -1.1448885
 [50,]  0.4052806
 [51,] -0.2277299
 [52,]  0.4332522
 [53,]  0.0813733
 [54,] -0.0811258
 [55,] -0.7399386
 [56,]  0.2071833
 [57,]  0.1135232
 [58,]  0.1191927
 [59,]  0.3429898
 [60,]  0.0932402
 [61,]  0.4401751
 [62,] -0.0730233
 [63,] -0.5010368
 [64,]  0.0753789
 [65,] -0.1721999
 [66,]  0.0454466
 [67,] -0.0258034
 [68,]  0.1817070
 [69,]  0.4479886
 [70,] -0.1600981
 [71,]  0.4398219
 [72,]  0.2482874
 [73,]  0.4030983
 [74,] -0.1907326
 [75,] -0.4726510
 [76,] -0.0650579
 [77,] -0.4551502
 [78,]  0.1595065
 [79,]  0.3768186
 [80,]  0.1216058
 [81,]  0.3019208
 [82,] -0.0011572
 [83,]  0.4141184
 [84,]  0.4009943
 [85,]  0.3492885
 [86,] -0.9969851
 [87,]  0.3787406
 [88,]  0.3850313
 [89,] -0.3168358
 [90,]  0.1926212
 [91,]  0.3287184
 [92,] -0.0421730
 [93,]  0.0605840
 [94,] -0.6448725
 [95,] -0.6325595
 [96,] -0.3563268
 [97,] -0.3239788
 [98,]  0.2205288
 [99,] -0.8325962
[100,]  0.3611287

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.1159 
> ## print log likelihood value
> logLik( a )
[1] -25.054
> ## compare with log likelihood value of summary object
> all.equal( logLik( a ), logLik( summary( a ) ) )
[1] TRUE
> 
> 
> ### maxBHHH
> set.seed( 6 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxBHHH(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -45.498 
     parameter initial gradient free
[1,]         1           54.502    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
successive function values within tolerance limit 
6  iterations
estimate: 2.1979 
Function value: -21.251 
> print( a )
$maximum
[1] -21.251

$estimate
[1] 2.1979

$gradient
[1] -4.7753e-05

$hessian
        [,1]
[1,] -18.423
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
            [,1]
  [1,]  0.348716
  [2,]  0.363367
  [3,]  0.147501
  [4,]  0.278354
  [5,] -0.600551
  [6,]  0.311411
  [7,]  0.421979
  [8,]  0.185053
  [9,]  0.096624
 [10,]  0.437089
 [11,]  0.277134
 [12,] -0.327074
 [13,]  0.254458
 [14,]  0.413645
 [15,] -0.347610
 [16,] -0.104040
 [17,]  0.359884
 [18,]  0.433211
 [19,] -0.242839
 [20,]  0.407540
 [21,]  0.434459
 [22,]  0.213065
 [23,] -0.724919
 [24,]  0.168470
 [25,] -0.731132
 [26,]  0.413030
 [27,]  0.131270
 [28,]  0.301423
 [29,]  0.033158
 [30,] -0.325140
 [31,]  0.266189
 [32,]  0.337189
 [33,] -0.634943
 [34,]  0.426392
 [35,]  0.411325
 [36,]  0.219170
 [37,] -0.230504
 [38,]  0.428248
 [39,]  0.436290
 [40,] -0.490299
 [41,] -0.866382
 [42,] -0.057088
 [43,]  0.170512
 [44,] -0.064889
 [45,] -0.041417
 [46,]  0.215916
 [47,] -0.279896
 [48,] -0.041668
 [49,]  0.449306
 [50,]  0.288679
 [51,]  0.380406
 [52,] -0.294232
 [53,] -0.126504
 [54,] -0.528372
 [55,]  0.057750
 [56,]  0.392610
 [57,]  0.411303
 [58,]  0.210811
 [59,]  0.433104
 [60,] -0.110646
 [61,] -1.088856
 [62,]  0.288915
 [63,]  0.410709
 [64,] -0.579201
 [65,]  0.370204
 [66,] -0.100111
 [67,] -0.316889
 [68,]  0.310288
 [69,] -1.058719
 [70,]  0.176390
 [71,]  0.373795
 [72,]  0.027957
 [73,] -0.464220
 [74,] -0.657348
 [75,] -0.119627
 [76,] -0.088734
 [77,] -0.351613
 [78,]  0.098417
 [79,] -0.147492
 [80,]  0.369126
 [81,] -0.231462
 [82,]  0.189561
 [83,]  0.182254
 [84,]  0.127182
 [85,]  0.443559
 [86,]  0.288751
 [87,]  0.386312
 [88,] -0.960357
 [89,]  0.453981
 [90,]  0.275261
 [91,] -0.135802
 [92,] -0.195829
 [93,] -0.246983
 [94,] -0.814803
 [95,]  0.178869
 [96,] -1.185449
 [97,]  0.416956
 [98,]  0.380620
 [99,] -1.168096
[100,] -0.633460

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.251 
Estimates:
     estimate    gradient
[1,]   2.1979 -4.7753e-05
--------------------------------------------
> ## Estimate with analytic gradient
> a <- maxBHHH(loglik, gradlik, start=1)
> print( a )
$maximum
[1] -21.251

$estimate
[1] 2.1979

$gradient
[1] -4.7748e-05

$hessian
        [,1]
[1,] -18.423
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
            [,1]
  [1,]  0.348716
  [2,]  0.363367
  [3,]  0.147501
  [4,]  0.278354
  [5,] -0.600551
  [6,]  0.311411
  [7,]  0.421979
  [8,]  0.185053
  [9,]  0.096624
 [10,]  0.437089
 [11,]  0.277134
 [12,] -0.327074
 [13,]  0.254458
 [14,]  0.413645
 [15,] -0.347610
 [16,] -0.104040
 [17,]  0.359884
 [18,]  0.433211
 [19,] -0.242839
 [20,]  0.407540
 [21,]  0.434459
 [22,]  0.213065
 [23,] -0.724919
 [24,]  0.168470
 [25,] -0.731132
 [26,]  0.413030
 [27,]  0.131270
 [28,]  0.301423
 [29,]  0.033158
 [30,] -0.325140
 [31,]  0.266189
 [32,]  0.337189
 [33,] -0.634943
 [34,]  0.426392
 [35,]  0.411325
 [36,]  0.219170
 [37,] -0.230504
 [38,]  0.428248
 [39,]  0.436290
 [40,] -0.490299
 [41,] -0.866382
 [42,] -0.057088
 [43,]  0.170512
 [44,] -0.064889
 [45,] -0.041417
 [46,]  0.215916
 [47,] -0.279896
 [48,] -0.041668
 [49,]  0.449306
 [50,]  0.288679
 [51,]  0.380406
 [52,] -0.294232
 [53,] -0.126504
 [54,] -0.528372
 [55,]  0.057750
 [56,]  0.392610
 [57,]  0.411303
 [58,]  0.210811
 [59,]  0.433104
 [60,] -0.110646
 [61,] -1.088856
 [62,]  0.288915
 [63,]  0.410709
 [64,] -0.579201
 [65,]  0.370204
 [66,] -0.100111
 [67,] -0.316889
 [68,]  0.310288
 [69,] -1.058719
 [70,]  0.176390
 [71,]  0.373795
 [72,]  0.027957
 [73,] -0.464220
 [74,] -0.657348
 [75,] -0.119627
 [76,] -0.088734
 [77,] -0.351613
 [78,]  0.098417
 [79,] -0.147492
 [80,]  0.369126
 [81,] -0.231462
 [82,]  0.189561
 [83,]  0.182254
 [84,]  0.127182
 [85,]  0.443559
 [86,]  0.288751
 [87,]  0.386312
 [88,] -0.960357
 [89,]  0.453981
 [90,]  0.275261
 [91,] -0.135802
 [92,] -0.195829
 [93,] -0.246983
 [94,] -0.814803
 [95,]  0.178869
 [96,] -1.185449
 [97,]  0.416956
 [98,]  0.380620
 [99,] -1.168096
[100,] -0.633460

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.251 
Estimates:
     estimate    gradient
[1,]   2.1979 -4.7748e-05
--------------------------------------------
> 
> 
> ### maxLik
> set.seed( 7 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -47.725 
     parameter initial gradient free
[1,]         1           52.275    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.0953 
Function value: -26.028 
> print.default( a )
$maximum
[1] -26.028

$estimate
[1] 2.0953

$gradient
[1] -5.4529e-07

$hessian
        [,1]
[1,] -22.759

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
             [,1]
  [1,]  0.4530765
  [2,] -0.3337543
  [3,] -0.3793196
  [4,]  0.0711526
  [5,]  0.2044376
  [6,] -0.8328836
  [7,]  0.1013209
  [8,] -1.6592864
  [9,]  0.3749329
 [10,]  0.4544984
 [11,]  0.0051398
 [12,]  0.1032361
 [13,] -0.4587547
 [14,] -0.4564705
 [15,] -0.1279176
 [16,]  0.3046295
 [17,]  0.1387857
 [18,] -0.0016954
 [19,]  0.2175483
 [20,]  0.0199336
 [21,] -0.7230240
 [22,]  0.1609604
 [23,]  0.2786390
 [24,]  0.2571700
 [25,]  0.2031909
 [26,]  0.3494887
 [27,]  0.2542602
 [28,] -0.1429866
 [29,] -0.3670608
 [30,] -0.4648108
 [31,] -0.1518230
 [32,]  0.0828735
 [33,] -0.4835686
 [34,]  0.4728577
 [35,]  0.2134092
 [36,]  0.2835807
 [37,]  0.4331953
 [38,]  0.3181623
 [39,]  0.0138586
 [40,]  0.0312041
 [41,]  0.1365072
 [42,]  0.3564125
 [43,] -0.2541041
 [44,]  0.4095271
 [45,] -0.0211148
 [46,]  0.1727706
 [47,] -0.2432602
 [48,] -0.8211556
 [49,]  0.2151068
 [50,] -0.0651121
 [51,] -0.2028575
 [52,] -0.0573565
 [53,] -1.6404911
 [54,]  0.4141019
 [55,]  0.4052862
 [56,]  0.4372131
 [57,] -0.0792189
 [58,]  0.1531577
 [59,] -0.4604447
 [60,] -0.1620732
 [61,] -0.0034621
 [62,] -0.0569846
 [63,]  0.4624468
 [64,] -0.5309778
 [65,]  0.2924950
 [66,] -0.1616596
 [67,] -1.1256022
 [68,]  0.3480809
 [69,]  0.4113314
 [70,]  0.1879290
 [71,] -0.4522365
 [72,]  0.1445611
 [73,]  0.4468113
 [74,]  0.0540080
 [75,]  0.2705824
 [76,] -0.0117094
 [77,]  0.2177146
 [78,] -0.3569633
 [79,]  0.3521245
 [80,] -0.8323319
 [81,]  0.1981504
 [82,]  0.4721638
 [83,]  0.2493488
 [84,]  0.1677298
 [85,]  0.2190185
 [86,]  0.1717493
 [87,]  0.3098924
 [88,]  0.4639580
 [89,] -0.4283273
 [90,]  0.4144861
 [91,]  0.0925070
 [92,]  0.2582344
 [93,] -0.4881546
 [94,]  0.4597293
 [95,] -0.8020430
 [96,]  0.4124432
 [97,]  0.0535600
 [98,]  0.1786183
 [99,] -0.1364532
[100,]  0.1661500

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.028 (1 free parameter(s))
Estimate(s): 2.0953 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.028 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -26.028

$estimate
[1] 2.0953

$gradient
[1] 4.615e-08

$hessian
        [,1]
[1,] -22.777

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
             [,1]
  [1,]  0.4530765
  [2,] -0.3337543
  [3,] -0.3793196
  [4,]  0.0711526
  [5,]  0.2044376
  [6,] -0.8328836
  [7,]  0.1013209
  [8,] -1.6592864
  [9,]  0.3749330
 [10,]  0.4544985
 [11,]  0.0051398
 [12,]  0.1032361
 [13,] -0.4587547
 [14,] -0.4564705
 [15,] -0.1279176
 [16,]  0.3046295
 [17,]  0.1387857
 [18,] -0.0016954
 [19,]  0.2175483
 [20,]  0.0199336
 [21,] -0.7230240
 [22,]  0.1609605
 [23,]  0.2786390
 [24,]  0.2571700
 [25,]  0.2031909
 [26,]  0.3494887
 [27,]  0.2542602
 [28,] -0.1429866
 [29,] -0.3670608
 [30,] -0.4648108
 [31,] -0.1518230
 [32,]  0.0828735
 [33,] -0.4835686
 [34,]  0.4728577
 [35,]  0.2134092
 [36,]  0.2835807
 [37,]  0.4331953
 [38,]  0.3181623
 [39,]  0.0138586
 [40,]  0.0312041
 [41,]  0.1365072
 [42,]  0.3564125
 [43,] -0.2541041
 [44,]  0.4095271
 [45,] -0.0211148
 [46,]  0.1727706
 [47,] -0.2432602
 [48,] -0.8211556
 [49,]  0.2151068
 [50,] -0.0651121
 [51,] -0.2028575
 [52,] -0.0573565
 [53,] -1.6404911
 [54,]  0.4141019
 [55,]  0.4052862
 [56,]  0.4372131
 [57,] -0.0792189
 [58,]  0.1531577
 [59,] -0.4604447
 [60,] -0.1620732
 [61,] -0.0034620
 [62,] -0.0569846
 [63,]  0.4624468
 [64,] -0.5309778
 [65,]  0.2924950
 [66,] -0.1616596
 [67,] -1.1256022
 [68,]  0.3480809
 [69,]  0.4113314
 [70,]  0.1879290
 [71,] -0.4522365
 [72,]  0.1445611
 [73,]  0.4468113
 [74,]  0.0540080
 [75,]  0.2705824
 [76,] -0.0117094
 [77,]  0.2177146
 [78,] -0.3569633
 [79,]  0.3521245
 [80,] -0.8323319
 [81,]  0.1981504
 [82,]  0.4721638
 [83,]  0.2493489
 [84,]  0.1677298
 [85,]  0.2190185
 [86,]  0.1717493
 [87,]  0.3098924
 [88,]  0.4639581
 [89,] -0.4283272
 [90,]  0.4144861
 [91,]  0.0925070
 [92,]  0.2582344
 [93,] -0.4881546
 [94,]  0.4597293
 [95,] -0.8020430
 [96,]  0.4124432
 [97,]  0.0535600
 [98,]  0.1786183
 [99,] -0.1364532
[100,]  0.1661500

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.028 (1 free parameter(s))
Estimate(s): 2.0953 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.028 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### maxNR
> set.seed( 8 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglikSum <- function(theta) sum(log(theta) - theta*t)
> ## Note the log-likelihood and gradient are summed over observations
> gradlikSum <- function(theta) sum(1/theta - t)
> ## Estimate with numeric gradient and Hessian
> a <- maxNR(loglikSum, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -46.489 
     parameter initial gradient free
[1,]         1           53.511    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.151 
Function value: -23.405 
> print( a )
$maximum
[1] -23.405

$estimate
[1] 2.151

$gradient
[1] -2.4158e-07

$hessian
        [,1]
[1,] -21.618

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.405 
Estimates:
     estimate    gradient
[1,]    2.151 -2.4158e-07
--------------------------------------------
> ## You would probably prefer 1/mean(t) instead ;-)
> ## Estimate with analytic gradient and Hessian
> a <- maxNR(loglikSum, gradlikSum, hesslik, start=1)
> print( a )
$maximum
[1] -23.405

$estimate
[1] 2.151

$gradient
[1] 9.4935e-08

$hessian
        [,1]
[1,] -21.613

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.405 
Estimates:
     estimate   gradient
[1,]    2.151 9.4935e-08
--------------------------------------------
> 
> 
> ### maximType
> ## maximise two-dimensional exponential hat.  Maximum is at c(2,1):
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m <- maxNR(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.1102e-10 0.0000e+00

$hessian
        [,1]    [,2]
[1,] -2.0002  0.0000
[2,]  0.0000 -2.0002

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(m)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate   gradient
[1,]        2 1.1102e-10
[2,]        1 0.0000e+00
--------------------------------------------
> maximType(m)
[1] "Newton-Raphson maximisation"
> ## Now use BFGS maximisation.
> m <- maxBFGS(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.0880e-08 5.3291e-09

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
function 
      26 

$type
[1] "BFGS maximisation"

$constraints
NULL

attr(,"class")
[1] "maxim"
> summary(m)
--------------------------------------------
BFGS maximisation 
Number of iterations: 26 
Return code: 0 
successful convergence  
Function value: 1 
Estimates:
     estimate   gradient
[1,]        2 1.0880e-08
[2,]        1 5.3291e-09
--------------------------------------------
> maximType(m)
[1] "BFGS maximisation"
> 
> ### Test maxNR with 0 iterations.  Should perform no iterations
> ### Request by Yves Croissant
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m0 <- maxNR(f, start=c(1.1, 2.1), iterlim=0)
> summary(m0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 0 
Return code: 4 
Iteration limit exceeded. 
Function value: 0.13266 
Estimates:
     estimate gradient
[1,]      1.1  0.23878
[2,]      2.1 -0.29184
--------------------------------------------
> 
> ### nObs
> set.seed( 10 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> nObs(m)
[1] 100
> 
> 
> ### nParam
> set.seed( 11 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> summary(m)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3436 -0.5338 -0.0291  0.5501  2.6934 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.242      0.287    11.3   <2e-16 ***
x1             3.974      0.395    10.1   <2e-16 ***
x2             4.783      0.367    13.0   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.99 on 97 degrees of freedom
Multiple R-squared:  0.702,	Adjusted R-squared:  0.696 
F-statistic:  114 on 2 and 97 DF,  p-value: <2e-16

> nParam(m) # you get 3
[1] 3
> 
> 
> ### numericGradient
> # A simple example with Gaussian bell
> f0 <- function(t0) exp(-t0[1]^2 - t0[2]^2)
> numericGradient(f0, c(1,2))
          [,1]      [,2]
[1,] -0.013476 -0.026952
> numericHessian(f0, t0=c(1,2))
         [,1]     [,2]
[1,] 0.013487 0.053903
[2,] 0.053903 0.094329
> # An example with the analytic gradient
> gradf0 <- function(t0) -2*t0*f0(t0)
> numericHessian(f0, gradf0, t0=c(1,2))
         [,1]     [,2]
[1,] 0.013476 0.053904
[2,] 0.053904 0.094331
> # The results should be similar as in the previous case
> # The central numeric derivatives have usually quite a high precision
> compareDerivatives(f0, gradf0, t0=1:2)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.0067379
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
          [,1]      [,2]
[1,] -0.013476 -0.026952
numeric gradient
          [,1]      [,2]
[1,] -0.013476 -0.026952
(anal-num)/(0.5*(abs(anal)+abs(num)))
            [,1]       [,2]
[1,] -2.7635e-10 -5.108e-11
Max relative difference: 2.7635e-10 
-------- END of compare derivatives -------- 
> # The differenc is around 1e-10
> 
> 
> ### returnCode
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.6316e-10

$gradient
[1] -6.6613e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should be success (1 or 2)
[1] 1
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.2766

$estimate
[1] 10685

$gradient
[1] 9.3591e-05

$hessian
          [,1]
[1,] 0.0017764

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should give a failure (4)
[1] 4
> 
> 
> ### returnMessage
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.6316e-10

$gradient
[1] -6.6613e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should be success (1 or 2)
[1] "gradient close to zero"
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.2766

$estimate
[1] 10685

$gradient
[1] 9.3591e-05

$hessian
          [,1]
[1,] 0.0017764

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should give a failure (4)
[1] "Iteration limit exceeded."
> 
> 
> ### summary.maxLik
> set.seed( 15 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -41.561 
     parameter initial gradient free
[1,]         1           58.439    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.4061 
Function value: -12.199 
> print.default( a )
$maximum
[1] -12.199

$estimate
[1] 2.4061

$gradient
[1] -5.2908e-07

$hessian
        [,1]
[1,] -17.275

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.313495
  [2,] -0.557714
  [3,]  0.288391
  [4,]  0.327590
  [5,]  0.084180
  [6,] -0.961467
  [7,]  0.269458
  [8,]  0.406518
  [9,]  0.208980
 [10,]  0.260537
 [11,]  0.366677
 [12,]  0.111881
 [13,]  0.122263
 [14,] -0.146091
 [15,] -1.166758
 [16,] -0.675538
 [17,] -0.019945
 [18,]  0.028321
 [19,] -0.728667
 [20,]  0.257075
 [21,]  0.050798
 [22,] -0.118521
 [23,] -0.045431
 [24,]  0.071711
 [25,] -1.859666
 [26,]  0.248909
 [27,]  0.201489
 [28,]  0.147105
 [29,]  0.329645
 [30,]  0.287747
 [31,]  0.139478
 [32,]  0.097804
 [33,]  0.021985
 [34,] -0.074557
 [35,]  0.241672
 [36,]  0.141430
 [37,] -0.092800
 [38,]  0.082591
 [39,]  0.179842
 [40,] -0.240591
 [41,]  0.234698
 [42,]  0.325146
 [43,] -0.310133
 [44,]  0.274319
 [45,]  0.150618
 [46,]  0.359357
 [47,] -0.165951
 [48,]  0.116672
 [49,]  0.411402
 [50,] -0.855623
 [51,]  0.369210
 [52,]  0.011781
 [53,]  0.090684
 [54,] -0.418522
 [55,]  0.162725
 [56,]  0.381031
 [57,] -0.240921
 [58,]  0.393217
 [59,]  0.187286
 [60,] -0.069622
 [61,] -0.526037
 [62,]  0.367461
 [63,]  0.217083
 [64,]  0.219133
 [65,]  0.257586
 [66,]  0.397968
 [67,]  0.298231
 [68,] -0.030898
 [69,] -0.099855
 [70,] -0.656732
 [71,]  0.051772
 [72,] -0.671335
 [73,]  0.323993
 [74,] -0.752221
 [75,]  0.209419
 [76,] -1.050248
 [77,]  0.393566
 [78,] -0.130512
 [79,] -1.349235
 [80,] -0.050241
 [81,]  0.236779
 [82,] -0.016973
 [83,]  0.153872
 [84,]  0.275027
 [85,]  0.157662
 [86,] -0.429689
 [87,]  0.214831
 [88,]  0.414339
 [89,]  0.218079
 [90,]  0.221077
 [91,] -0.072481
 [92,]  0.269757
 [93,] -0.069646
 [94,]  0.308256
 [95,] -0.178160
 [96,]  0.152514
 [97,]  0.184997
 [98,]  0.074991
 [99,]  0.364831
[100,]  0.093843

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.199 (1 free parameter(s))
Estimate(s): 2.4061 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.199 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -12.199

$estimate
[1] 2.4061

$gradient
[1] 5.2236e-14

$hessian
        [,1]
[1,] -17.273

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.313495
  [2,] -0.557714
  [3,]  0.288391
  [4,]  0.327590
  [5,]  0.084180
  [6,] -0.961467
  [7,]  0.269458
  [8,]  0.406518
  [9,]  0.208980
 [10,]  0.260537
 [11,]  0.366677
 [12,]  0.111881
 [13,]  0.122263
 [14,] -0.146091
 [15,] -1.166758
 [16,] -0.675538
 [17,] -0.019945
 [18,]  0.028321
 [19,] -0.728667
 [20,]  0.257075
 [21,]  0.050798
 [22,] -0.118521
 [23,] -0.045431
 [24,]  0.071711
 [25,] -1.859666
 [26,]  0.248909
 [27,]  0.201489
 [28,]  0.147105
 [29,]  0.329645
 [30,]  0.287747
 [31,]  0.139478
 [32,]  0.097804
 [33,]  0.021985
 [34,] -0.074557
 [35,]  0.241672
 [36,]  0.141430
 [37,] -0.092800
 [38,]  0.082591
 [39,]  0.179842
 [40,] -0.240591
 [41,]  0.234698
 [42,]  0.325146
 [43,] -0.310133
 [44,]  0.274319
 [45,]  0.150618
 [46,]  0.359357
 [47,] -0.165951
 [48,]  0.116672
 [49,]  0.411402
 [50,] -0.855623
 [51,]  0.369210
 [52,]  0.011781
 [53,]  0.090684
 [54,] -0.418522
 [55,]  0.162725
 [56,]  0.381031
 [57,] -0.240921
 [58,]  0.393217
 [59,]  0.187286
 [60,] -0.069622
 [61,] -0.526037
 [62,]  0.367461
 [63,]  0.217083
 [64,]  0.219133
 [65,]  0.257586
 [66,]  0.397968
 [67,]  0.298231
 [68,] -0.030898
 [69,] -0.099855
 [70,] -0.656732
 [71,]  0.051772
 [72,] -0.671335
 [73,]  0.323993
 [74,] -0.752221
 [75,]  0.209419
 [76,] -1.050248
 [77,]  0.393566
 [78,] -0.130512
 [79,] -1.349235
 [80,] -0.050241
 [81,]  0.236779
 [82,] -0.016973
 [83,]  0.153872
 [84,]  0.275027
 [85,]  0.157662
 [86,] -0.429689
 [87,]  0.214831
 [88,]  0.414339
 [89,]  0.218079
 [90,]  0.221077
 [91,] -0.072481
 [92,]  0.269757
 [93,] -0.069646
 [94,]  0.308256
 [95,] -0.178160
 [96,]  0.152514
 [97,]  0.184997
 [98,]  0.074991
 [99,]  0.364831
[100,]  0.093843

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.199 (1 free parameter(s))
Estimate(s): 2.4061 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.199 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### summary.maxim and for "gradient"/"hessian" attributes
> ### Test for infinity
> ## maximize a 2D quadratic function:
> f <- function(b) {
+   x <- b[1]; y <- b[2];
+     val <- (x - 2)^2 + (y - 3)^2
+     attr(val, "gradient") <- c(2*x - 4, 2*y - 6)
+     attr(val, "hessian") <- matrix(c(2, 0, 0, 2), 2, 2)
+     val
+ }
> ## Use c(0,0) as initial value.  
> result1 <- maxNR( f, start = c(0,0) )
> print( result1 )
$maximum
[1] Inf

$estimate
[1] -7.0349e+155 -1.0552e+156

$gradient
[1] -1.4070e+156 -2.1105e+156

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 25

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result1 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 25 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
         estimate     gradient
[1,] -7.0349e+155 -1.4070e+156
[2,] -1.0552e+156 -2.1105e+156
--------------------------------------------
> ## Now use c(1000000, -777777) as initial value and ask for hessian
> result2 <- maxNR( f, start = c( 1000000, -777777))
> print( result2 )
$maximum
[1] Inf

$estimate
[1]  2.1105e+155 -1.6415e+155

$gradient
[1]  4.2209e+155 -3.2829e+155

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 24

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result2 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 24 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
         estimate     gradient
[1,]  2.1105e+155  4.2209e+155
[2,] -1.6415e+155 -3.2829e+155
--------------------------------------------
> 
> 
> ### Test for "gradient"/"hessian" attributes.  A case which converges.
> hub <- function(x) {
+    v <- exp(-sum(x*x))
+    val <- v
+    attr(val, "gradient") <- -2*x*v
+    attr(val, "hessian") <- 4*(x %*% t(x))*v - diag(2*c(v, v))
+    val
+ }
> summary(a <- maxNR(hub, start=c(2,1)))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
        estimate   gradient
[1,] -7.4484e-18 1.4897e-17
[2,] -3.7242e-18 7.4484e-18
--------------------------------------------
> ## Now test "gradient" attribute for BHHH/3-parameter probit
> N <- 1000
> loglikProbit <- function( beta) {
+    xb <- x %*% beta
+    loglik <- ifelse(y == 0,
+                     pnorm( xb, log=TRUE, lower.tail=FALSE),
+                     pnorm( xb, log.p=TRUE))
+    grad <- ifelse(y == 0,
+                   -dnorm(xb)/pnorm(xb, lower.tail=FALSE),
+                   dnorm(xb)/pnorm(xb))
+    grad <- grad*x
+    attr(loglik, "gradient") <- grad
+    loglik
+ }
> x <- runif(N)
> x <- cbind(x, x - runif(N), x - runif(N))
> y <- x[,1] + 2*x[,2] - x[,3] + rnorm(N) > 0
> summary(maxLik(loglikProbit, start=c(0,0,0), method="bhhh"))
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -508.35 
3  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.8578     0.0904    9.49 < 2e-16 ***
[2,]   1.9389     0.1514   12.81 < 2e-16 ***
[3,]  -0.8253     0.1339   -6.16 7.2e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> 
> ### vcov.maxLik
> set.seed( 17 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -53.666 
     parameter initial gradient free
[1,]         1           46.334    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 1.8634 
Function value: -37.762 
> print.default( a )
$maximum
[1] -37.762

$estimate
[1] 1.8634

$gradient
[1] -3.4694e-08

$hessian
        [,1]
[1,] -28.791

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,] -0.276686
  [2,]  0.395302
  [3,]  0.497867
  [4,] -0.484034
  [5,] -0.404897
  [6,]  0.256694
  [7,] -0.432259
  [8,]  0.189302
  [9,]  0.206581
 [10,]  0.328467
 [11,]  0.166476
 [12,]  0.393630
 [13,]  0.510431
 [14,]  0.326126
 [15,]  0.233531
 [16,]  0.504552
 [17,]  0.461203
 [18,]  0.460769
 [19,] -1.064331
 [20,]  0.237547
 [21,] -0.046471
 [22,]  0.473353
 [23,] -0.400352
 [24,]  0.234801
 [25,]  0.284591
 [26,]  0.402885
 [27,] -0.237845
 [28,]  0.441124
 [29,]  0.482140
 [30,]  0.495489
 [31,] -0.365213
 [32,]  0.387760
 [33,] -0.406733
 [34,] -0.180877
 [35,]  0.418545
 [36,] -0.330426
 [37,] -0.240393
 [38,] -0.415337
 [39,]  0.461170
 [40,] -3.892978
 [41,]  0.033434
 [42,] -0.629281
 [43,]  0.432492
 [44,]  0.036492
 [45,]  0.246212
 [46,] -0.226752
 [47,]  0.530619
 [48,]  0.515996
 [49,] -0.677201
 [50,]  0.152560
 [51,]  0.221948
 [52,]  0.436259
 [53,] -1.563285
 [54,] -0.050161
 [55,]  0.312415
 [56,] -0.146451
 [57,] -0.356725
 [58,]  0.480977
 [59,]  0.398514
 [60,]  0.423343
 [61,]  0.330073
 [62,]  0.352608
 [63,] -0.301808
 [64,]  0.470833
 [65,]  0.291035
 [66,]  0.119719
 [67,]  0.507396
 [68,] -0.249516
 [69,] -0.050472
 [70,]  0.280010
 [71,]  0.525545
 [72,] -0.373984
 [73,]  0.023521
 [74,] -0.413288
 [75,] -0.358502
 [76,]  0.476639
 [77,]  0.241551
 [78,] -0.041148
 [79,] -1.347725
 [80,] -0.256681
 [81,]  0.324324
 [82,]  0.345214
 [83,] -0.934159
 [84,]  0.332431
 [85,] -0.849786
 [86,]  0.315762
 [87,]  0.475184
 [88,]  0.372592
 [89,] -0.025300
 [90,]  0.376580
 [91,] -2.108295
 [92,] -0.449645
 [93,]  0.168212
 [94,]  0.351012
 [95,]  0.525437
 [96,] -0.066191
 [97,]  0.387816
 [98,]  0.253496
 [99,] -0.796653
[100,]  0.133252

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.762 (1 free parameter(s))
Estimate(s): 1.8634 
> vcov(a)
         [,1]
[1,] 0.034733
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -37.762

$estimate
[1] 1.8634

$gradient
[1] 1.0924e-09

$hessian
        [,1]
[1,] -28.801

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,] -0.276686
  [2,]  0.395302
  [3,]  0.497867
  [4,] -0.484034
  [5,] -0.404897
  [6,]  0.256694
  [7,] -0.432259
  [8,]  0.189302
  [9,]  0.206581
 [10,]  0.328467
 [11,]  0.166476
 [12,]  0.393630
 [13,]  0.510431
 [14,]  0.326126
 [15,]  0.233531
 [16,]  0.504552
 [17,]  0.461203
 [18,]  0.460769
 [19,] -1.064331
 [20,]  0.237547
 [21,] -0.046471
 [22,]  0.473353
 [23,] -0.400352
 [24,]  0.234801
 [25,]  0.284591
 [26,]  0.402885
 [27,] -0.237845
 [28,]  0.441124
 [29,]  0.482140
 [30,]  0.495489
 [31,] -0.365213
 [32,]  0.387760
 [33,] -0.406733
 [34,] -0.180877
 [35,]  0.418545
 [36,] -0.330426
 [37,] -0.240393
 [38,] -0.415337
 [39,]  0.461170
 [40,] -3.892978
 [41,]  0.033434
 [42,] -0.629281
 [43,]  0.432492
 [44,]  0.036492
 [45,]  0.246212
 [46,] -0.226752
 [47,]  0.530619
 [48,]  0.515996
 [49,] -0.677201
 [50,]  0.152560
 [51,]  0.221948
 [52,]  0.436259
 [53,] -1.563285
 [54,] -0.050161
 [55,]  0.312415
 [56,] -0.146451
 [57,] -0.356725
 [58,]  0.480977
 [59,]  0.398514
 [60,]  0.423343
 [61,]  0.330073
 [62,]  0.352608
 [63,] -0.301808
 [64,]  0.470833
 [65,]  0.291035
 [66,]  0.119719
 [67,]  0.507396
 [68,] -0.249516
 [69,] -0.050472
 [70,]  0.280010
 [71,]  0.525545
 [72,] -0.373984
 [73,]  0.023521
 [74,] -0.413288
 [75,] -0.358502
 [76,]  0.476639
 [77,]  0.241551
 [78,] -0.041148
 [79,] -1.347725
 [80,] -0.256681
 [81,]  0.324324
 [82,]  0.345214
 [83,] -0.934159
 [84,]  0.332431
 [85,] -0.849786
 [86,]  0.315762
 [87,]  0.475184
 [88,]  0.372592
 [89,] -0.025300
 [90,]  0.376580
 [91,] -2.108295
 [92,] -0.449645
 [93,]  0.168212
 [94,]  0.351012
 [95,]  0.525437
 [96,] -0.066191
 [97,]  0.387816
 [98,]  0.253496
 [99,] -0.796653
[100,]  0.133252

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.762 (1 free parameter(s))
Estimate(s): 1.8634 
> vcov(a)
         [,1]
[1,] 0.034721
> print(stdEr(a))
[1] 0.18634
>                            # test single stdEr
> 
> proc.time()
   user  system elapsed 
  0.604   0.036   0.633 
