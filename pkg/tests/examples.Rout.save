
R version 3.0.1 (2013-05-16) -- "Good Sport"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library( maxLik )
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> options(digits=4)
> 
> ### activePar
> # a simple two-dimensional exponential hat
> f <- function(a) exp(-a[1]^2 - a[2]^2)
> #
> # maximize wrt. both parameters 
> free <- maxNR(f, start=1:2)
> print( free )
$maximum
[1] 1

$estimate
[1] -3.332e-11 -5.643e-11

$gradient
[1] 1.11e-10 1.11e-10

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(free)  # results should be close to (0,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
       estimate gradient
[1,] -3.332e-11 1.11e-10
[2,] -5.643e-11 1.11e-10
--------------------------------------------
> activePar(free)
[1] TRUE TRUE
> # allow only the second parameter to vary
> cons <- maxNR(f, start=1:2, activePar=c(FALSE,TRUE))
> print( cons )
$maximum
[1] 0.3679

$estimate
[1] 1.000e+00 3.515e-07

$gradient
[1]         NA -2.586e-07

$hessian
     [,1]    [,2]
[1,]   NA      NA
[2,]   NA -0.7357

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1]  TRUE FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(cons) # result should be around (1,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
      estimate   gradient
[1,] 1.000e+00         NA
[2,] 3.515e-07 -2.586e-07
--------------------------------------------
> activePar(cons)
[1] FALSE  TRUE
> # specify fixed par in different ways
> cons2 <- maxNR(f, start=1:2, fixed=1)
> all.equal( cons, cons2 )
[1] TRUE
> cons3 <- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
> all.equal( cons, cons3 )
[1] TRUE
> cons4 <- maxNR(f, start=c(a=1, b=2), fixed="a")
> print(summary(cons4))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
   estimate   gradient
a 1.000e+00         NA
b 3.515e-07 -2.586e-07
--------------------------------------------
> all.equal( cons, cons4 )
[1] "Component 2: names for current but not for target"                             
[2] "Component 3: names for current but not for target"                             
[3] "Component 4: Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component 8: names for current but not for target"                             
> 
> ### compareDerivatives
> set.seed( 2 )
> ## A simple example with sin(x)' = cos(x)
> f <- sin
> compareDerivatives(f, cos, t0=1)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.8415
Dim of analytic gradient: 1 1 
       numeric          : 1 1 
      
param  theta 0 analytic numeric  rel.diff
  [1,]       1   0.5403  0.5403 -5.13e-11
Max relative difference: 5.13e-11 
-------- END of compare derivatives -------- 
> ##
> ## Example of log-likelihood of normal density.  Two-parameter
> ## function.
> x <- rnorm(100, 1, 2) # generate rnorm x
> l <- function(b) sum(log(dnorm((x-b[1])/b[2])/b[2]))
>               # b[1] - mu, b[2] - sigma
> gradl <- function(b) {
+    c(sum(x - b[1])/b[2]^2,
+    sum((x - b[1])^2/b[2]^3 - 1/b[2]))
+ }
> compareDerivatives(l, gradl, t0=c(1,2))
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] -227.9
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
       [,1]  [,2]
[1,] -1.535 16.68
numeric gradient
       [,1]  [,2]
[1,] -1.535 16.68
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -1.989e-09 -2.089e-10
Max relative difference: 1.989e-09 
-------- END of compare derivatives -------- 
> 
> 
> ### hessian
> set.seed( 3 )
> # log-likelihood for normal density
> # a[1] - mean
> # a[2] - standard deviation
> ll <- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
> x <- rnorm(1000) # sample from standard normal
> ml <- maxLik(ll, start=c(1,1))
> # ignore eventual warnings "NaNs produced in: log(x)"
> print.default( ml )
$maximum
[1] -497.6

$estimate
[1] 0.006397 0.997576

$gradient
[1] 0.000e+00 5.684e-08

$hessian
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (2 free parameter(s))
Estimate(s): 0.006397 0.9976 
> summary(ml) # result should be close to c(0,1)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
2  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.0064     0.0316     0.2    0.84    
[2,]   0.9976     0.0223    44.7  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> hessian(ml) # How the Hessian looks like
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010
> sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
        [,1]    [,2]
[1,] 0.03155 0.00000
[2,] 0.00000 0.02231
> print(stdEr(ml))
[1] 0.03155 0.02231
>                            # test vector of stdEr-s
> #
> # Now run the same example while fixing a[2] = 1
> mlf <- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
> print.default( mlf )
$maximum
[1] -497.6

$estimate
[1] 0.006397 1.000000

$gradient
[1]  0 NA

$hessian
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE  TRUE

$iterations
[1] 3

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( mlf )
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (1 free parameter(s))
Estimate(s): 0.006397 1 
> summary(mlf) # first parameter close to 0, the second exactly 1.0
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)
[1,]   0.0064     0.0316     0.2    0.84
[2,]   1.0000     0.0000      NA      NA
--------------------------------------------
> hessian(mlf)
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA
> # now invert only the free parameter part of the Hessian
> sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
        [,1]
[1,] 0.03162
> # gives the standard deviation for the mean
> print(stdEr(mlf))
[1] 0.03162 0.00000
>                            # test standard errors with fixed par
> 
> 
> ### maxBFGS
> set.seed( 5 )
> # Maximum Likelihood estimation of the parameter of Poissonian distribution
> n <- rpois(100, 3)
> loglik <- function(l) n*log(l) - l - lfactorial(n)
> # we use numeric gradient
> a <- maxBFGS(loglik, start=1)
> print( a )
$maximum
[1] -199.2

$estimate
[1] 3.19

$gradient
[1] 1.583e-05

$hessian
       [,1]
[1,] -31.29

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE

$iterations
function 
      29 

$type
[1] "BFGS maximisation"

$constraints
NULL

$gradientObs
           [,1]
  [1,] -0.37304
  [2,]  0.25392
  [3,]  0.88088
  [4,] -0.37304
  [5,] -0.68652
  [6,]  0.25392
  [7,] -0.05956
  [8,]  0.25392
  [9,]  0.88088
 [10,] -0.68652
 [11,] -0.37304
 [12,] -0.05956
 [13,] -0.37304
 [14,] -0.05956
 [15,] -0.37304
 [16,] -0.37304
 [17,] -0.37304
 [18,]  0.56740
 [19,] -0.05956
 [20,]  0.56740
 [21,]  0.56740
 [22,]  0.25392
 [23,] -0.37304
 [24,] -0.37304
 [25,] -0.68652
 [26,] -0.05956
 [27,] -0.05956
 [28,]  0.88088
 [29,] -0.68652
 [30,]  0.88088
 [31,] -0.05956
 [32,] -0.68652
 [33,] -0.37304
 [34,] -1.00000
 [35,] -1.00000
 [36,] -0.05956
 [37,] -0.05956
 [38,] -0.05956
 [39,] -0.37304
 [40,] -0.37304
 [41,]  0.56740
 [42,] -0.37304
 [43,]  0.56740
 [44,] -0.05956
 [45,]  0.88088
 [46,] -0.05956
 [47,]  0.25392
 [48,] -0.68652
 [49,]  0.25392
 [50,] -0.05956
 [51,] -0.37304
 [52,] -0.05956
 [53,]  0.88088
 [54,]  1.19436
 [55,]  0.88088
 [56,] -0.37304
 [57,] -0.37304
 [58,] -0.37304
 [59,] -0.68652
 [60,] -0.68652
 [61,] -0.05956
 [62,] -0.68652
 [63,]  0.56740
 [64,]  1.50784
 [65,]  1.19436
 [66,]  0.56740
 [67,]  0.56740
 [68,] -0.68652
 [69,]  0.88088
 [70,]  0.56740
 [71,]  0.56740
 [72,]  0.25392
 [73,] -0.68652
 [74,]  0.56740
 [75,] -1.00000
 [76,]  0.88088
 [77,] -1.00000
 [78,]  0.25392
 [79,] -0.05956
 [80,] -0.37304
 [81,]  0.88088
 [82,]  0.56740
 [83,] -0.37304
 [84,] -0.68652
 [85,] -0.05956
 [86,] -0.68652
 [87,]  1.19436
 [88,]  0.25392
 [89,] -0.37304
 [90,] -0.05956
 [91,]  0.25392
 [92,] -0.37304
 [93,] -0.68652
 [94,] -0.37304
 [95,] -0.37304
 [96,]  0.56740
 [97,] -0.37304
 [98,] -0.05956
 [99,] -0.05956
[100,] -0.05956

attr(,"class")
[1] "maxim"
> summary( a )
--------------------------------------------
BFGS maximisation 
Number of iterations: 29 
Return code: 0 
successful convergence  
Function value: -199.2 
Estimates:
     estimate  gradient
[1,]     3.19 1.583e-05
--------------------------------------------
> # you would probably prefer mean(n) instead of that ;-)
> # Note also that maxLik is better suited for Maximum Likelihood
> 
> 
> ### logLik.maxLik
> set.seed( 4 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -25.05

$estimate
[1] 2.116

$gradient
[1] 6.06e-08

$hessian
       [,1]
[1,] -22.34

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.386821
  [2,] -1.679351
  [3,]  0.038568
  [4,]  0.071298
  [5,]  0.159047
  [6,]  0.105192
  [7,]  0.248215
  [8,]  0.447271
  [9,]  0.217946
 [10,]  0.054046
 [11,] -0.867528
 [12,]  0.328583
 [13,]  0.270226
 [14,]  0.258113
 [15,]  0.302820
 [16,] -0.051988
 [17,]  0.442844
 [18,]  0.405509
 [19,] -0.447366
 [20,] -0.033385
 [21,]  0.350565
 [22,] -0.150789
 [23,] -2.297263
 [24,]  0.388691
 [25,] -0.444123
 [26,]  0.443408
 [27,]  0.276873
 [28,] -0.151173
 [29,]  0.226692
 [30,]  0.192216
 [31,] -0.216352
 [32,] -0.427312
 [33,] -0.415672
 [34,]  0.278199
 [35,] -0.636970
 [36,]  0.394517
 [37,]  0.344061
 [38,] -0.620260
 [39,]  0.457767
 [40,]  0.167204
 [41,]  0.353776
 [42,] -0.065341
 [43,]  0.147748
 [44,]  0.282721
 [45,] -0.015243
 [46,]  0.079882
 [47,]  0.274372
 [48,]  0.452304
 [49,] -1.144889
 [50,]  0.405281
 [51,] -0.227730
 [52,]  0.433252
 [53,]  0.081373
 [54,] -0.081126
 [55,] -0.739939
 [56,]  0.207183
 [57,]  0.113523
 [58,]  0.119193
 [59,]  0.342990
 [60,]  0.093240
 [61,]  0.440175
 [62,] -0.073023
 [63,] -0.501037
 [64,]  0.075379
 [65,] -0.172200
 [66,]  0.045447
 [67,] -0.025803
 [68,]  0.181707
 [69,]  0.447989
 [70,] -0.160098
 [71,]  0.439822
 [72,]  0.248287
 [73,]  0.403098
 [74,] -0.190733
 [75,] -0.472651
 [76,] -0.065058
 [77,] -0.455150
 [78,]  0.159506
 [79,]  0.376819
 [80,]  0.121606
 [81,]  0.301921
 [82,] -0.001157
 [83,]  0.414118
 [84,]  0.400994
 [85,]  0.349289
 [86,] -0.996985
 [87,]  0.378741
 [88,]  0.385031
 [89,] -0.316836
 [90,]  0.192621
 [91,]  0.328718
 [92,] -0.042173
 [93,]  0.060584
 [94,] -0.644872
 [95,] -0.632560
 [96,] -0.356327
 [97,] -0.323979
 [98,]  0.220529
 [99,] -0.832596
[100,]  0.361129

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> ## print log likelihood value
> logLik( a )
[1] -25.05
> ## compare with log likelihood value of summary object
> all.equal( logLik( a ), logLik( summary( a ) ) )
[1] TRUE
> 
> 
> ### maxBHHH
> set.seed( 6 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxBHHH(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -45.5 
     parameter initial gradient free
[1,]         1             54.5    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
successive function values within tolerance limit 
6  iterations
estimate: 2.198 
Function value: -21.25 
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> ## Estimate with analytic gradient
> a <- maxBHHH(loglik, gradlik, start=1)
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> 
> 
> ### maxLik
> set.seed( 7 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -47.72 
     parameter initial gradient free
[1,]         1            52.28    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.095 
Function value: -26.03 
> print.default( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] -5.453e-07

$hessian
       [,1]
[1,] -22.76

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.453076
  [2,] -0.333754
  [3,] -0.379320
  [4,]  0.071153
  [5,]  0.204438
  [6,] -0.832884
  [7,]  0.101321
  [8,] -1.659286
  [9,]  0.374933
 [10,]  0.454498
 [11,]  0.005140
 [12,]  0.103236
 [13,] -0.458755
 [14,] -0.456471
 [15,] -0.127918
 [16,]  0.304629
 [17,]  0.138786
 [18,] -0.001695
 [19,]  0.217548
 [20,]  0.019934
 [21,] -0.723024
 [22,]  0.160960
 [23,]  0.278639
 [24,]  0.257170
 [25,]  0.203191
 [26,]  0.349489
 [27,]  0.254260
 [28,] -0.142987
 [29,] -0.367061
 [30,] -0.464811
 [31,] -0.151823
 [32,]  0.082874
 [33,] -0.483569
 [34,]  0.472858
 [35,]  0.213409
 [36,]  0.283581
 [37,]  0.433195
 [38,]  0.318162
 [39,]  0.013859
 [40,]  0.031204
 [41,]  0.136507
 [42,]  0.356412
 [43,] -0.254104
 [44,]  0.409527
 [45,] -0.021115
 [46,]  0.172771
 [47,] -0.243260
 [48,] -0.821156
 [49,]  0.215107
 [50,] -0.065112
 [51,] -0.202857
 [52,] -0.057357
 [53,] -1.640491
 [54,]  0.414102
 [55,]  0.405286
 [56,]  0.437213
 [57,] -0.079219
 [58,]  0.153158
 [59,] -0.460445
 [60,] -0.162073
 [61,] -0.003462
 [62,] -0.056985
 [63,]  0.462447
 [64,] -0.530978
 [65,]  0.292495
 [66,] -0.161660
 [67,] -1.125602
 [68,]  0.348081
 [69,]  0.411331
 [70,]  0.187929
 [71,] -0.452236
 [72,]  0.144561
 [73,]  0.446811
 [74,]  0.054008
 [75,]  0.270582
 [76,] -0.011709
 [77,]  0.217715
 [78,] -0.356963
 [79,]  0.352124
 [80,] -0.832332
 [81,]  0.198150
 [82,]  0.472164
 [83,]  0.249349
 [84,]  0.167730
 [85,]  0.219019
 [86,]  0.171749
 [87,]  0.309892
 [88,]  0.463958
 [89,] -0.428327
 [90,]  0.414486
 [91,]  0.092507
 [92,]  0.258234
 [93,] -0.488155
 [94,]  0.459729
 [95,] -0.802043
 [96,]  0.412443
 [97,]  0.053560
 [98,]  0.178618
 [99,] -0.136453
[100,]  0.166150

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] 4.615e-08

$hessian
       [,1]
[1,] -22.78

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.453076
  [2,] -0.333754
  [3,] -0.379320
  [4,]  0.071153
  [5,]  0.204438
  [6,] -0.832884
  [7,]  0.101321
  [8,] -1.659286
  [9,]  0.374933
 [10,]  0.454498
 [11,]  0.005140
 [12,]  0.103236
 [13,] -0.458755
 [14,] -0.456471
 [15,] -0.127918
 [16,]  0.304629
 [17,]  0.138786
 [18,] -0.001695
 [19,]  0.217548
 [20,]  0.019934
 [21,] -0.723024
 [22,]  0.160960
 [23,]  0.278639
 [24,]  0.257170
 [25,]  0.203191
 [26,]  0.349489
 [27,]  0.254260
 [28,] -0.142987
 [29,] -0.367061
 [30,] -0.464811
 [31,] -0.151823
 [32,]  0.082874
 [33,] -0.483569
 [34,]  0.472858
 [35,]  0.213409
 [36,]  0.283581
 [37,]  0.433195
 [38,]  0.318162
 [39,]  0.013859
 [40,]  0.031204
 [41,]  0.136507
 [42,]  0.356412
 [43,] -0.254104
 [44,]  0.409527
 [45,] -0.021115
 [46,]  0.172771
 [47,] -0.243260
 [48,] -0.821156
 [49,]  0.215107
 [50,] -0.065112
 [51,] -0.202857
 [52,] -0.057357
 [53,] -1.640491
 [54,]  0.414102
 [55,]  0.405286
 [56,]  0.437213
 [57,] -0.079219
 [58,]  0.153158
 [59,] -0.460445
 [60,] -0.162073
 [61,] -0.003462
 [62,] -0.056985
 [63,]  0.462447
 [64,] -0.530978
 [65,]  0.292495
 [66,] -0.161660
 [67,] -1.125602
 [68,]  0.348081
 [69,]  0.411331
 [70,]  0.187929
 [71,] -0.452236
 [72,]  0.144561
 [73,]  0.446811
 [74,]  0.054008
 [75,]  0.270582
 [76,] -0.011709
 [77,]  0.217715
 [78,] -0.356963
 [79,]  0.352124
 [80,] -0.832332
 [81,]  0.198150
 [82,]  0.472164
 [83,]  0.249349
 [84,]  0.167730
 [85,]  0.219019
 [86,]  0.171749
 [87,]  0.309892
 [88,]  0.463958
 [89,] -0.428327
 [90,]  0.414486
 [91,]  0.092507
 [92,]  0.258234
 [93,] -0.488155
 [94,]  0.459729
 [95,] -0.802043
 [96,]  0.412443
 [97,]  0.053560
 [98,]  0.178618
 [99,] -0.136453
[100,]  0.166150

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### maxNR
> set.seed( 8 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglikSum <- function(theta) sum(log(theta) - theta*t)
> ## Note the log-likelihood and gradient are summed over observations
> gradlikSum <- function(theta) sum(1/theta - t)
> ## Estimate with numeric gradient and Hessian
> a <- maxNR(loglikSum, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -46.49 
     parameter initial gradient free
[1,]         1            53.51    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.151 
Function value: -23.41 
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] -2.416e-07

$hessian
       [,1]
[1,] -21.62

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate   gradient
[1,]    2.151 -2.416e-07
--------------------------------------------
> ## You would probably prefer 1/mean(t) instead ;-)
> ## Estimate with analytic gradient and Hessian
> a <- maxNR(loglikSum, gradlikSum, hesslik, start=1)
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] 9.493e-08

$hessian
       [,1]
[1,] -21.61

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate  gradient
[1,]    2.151 9.493e-08
--------------------------------------------
> 
> 
> ### maximType
> ## maximise two-dimensional exponential hat.  Maximum is at c(2,1):
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m <- maxNR(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.11e-10 0.00e+00

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(m)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate gradient
[1,]        2 1.11e-10
[2,]        1 0.00e+00
--------------------------------------------
> maximType(m)
[1] "Newton-Raphson maximisation"
> ## Now use BFGS maximisation.
> m <- maxBFGS(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.088e-08 5.329e-09

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
function 
      26 

$type
[1] "BFGS maximisation"

$constraints
NULL

attr(,"class")
[1] "maxim"
> summary(m)
--------------------------------------------
BFGS maximisation 
Number of iterations: 26 
Return code: 0 
successful convergence  
Function value: 1 
Estimates:
     estimate  gradient
[1,]        2 1.088e-08
[2,]        1 5.329e-09
--------------------------------------------
> maximType(m)
[1] "BFGS maximisation"
> 
> ### Test maxNR with 0 iterations.  Should perform no iterations
> ### Request by Yves Croissant
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m0 <- maxNR(f, start=c(1.1, 2.1), iterlim=0)
> summary(m0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 0 
Return code: 4 
Iteration limit exceeded. 
Function value: 0.1327 
Estimates:
     estimate gradient
[1,]      1.1   0.2388
[2,]      2.1  -0.2918
--------------------------------------------
> 
> ### nObs
> set.seed( 10 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> nObs(m)
[1] 100
> 
> 
> ### nParam
> set.seed( 11 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> summary(m)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3436 -0.5338 -0.0291  0.5501  2.6934 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.242      0.287    11.3   <2e-16 ***
x1             3.974      0.395    10.1   <2e-16 ***
x2             4.783      0.367    13.0   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.99 on 97 degrees of freedom
Multiple R-squared:  0.702,	Adjusted R-squared:  0.696 
F-statistic:  114 on 2 and 97 DF,  p-value: <2e-16

> nParam(m) # you get 3
[1] 3
> 
> 
> ### numericGradient
> # A simple example with Gaussian bell
> f0 <- function(t0) exp(-t0[1]^2 - t0[2]^2)
> numericGradient(f0, c(1,2))
         [,1]     [,2]
[1,] -0.01348 -0.02695
> numericHessian(f0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01349 0.05390
[2,] 0.05390 0.09433
> # An example with the analytic gradient
> gradf0 <- function(t0) -2*t0*f0(t0)
> numericHessian(f0, gradf0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01348 0.05390
[2,] 0.05390 0.09433
> # The results should be similar as in the previous case
> # The central numeric derivatives have usually quite a high precision
> compareDerivatives(f0, gradf0, t0=1:2)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.006738
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
numeric gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -2.764e-10 -5.108e-11
Max relative difference: 2.764e-10 
-------- END of compare derivatives -------- 
> # The differenc is around 1e-10
> 
> 
> ### returnCode
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should be success (1 or 2)
[1] 1
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should give a failure (4)
[1] 4
> 
> 
> ### returnMessage
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should be success (1 or 2)
[1] "gradient close to zero"
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should give a failure (4)
[1] "Iteration limit exceeded."
> 
> 
> ### summary.maxLik
> set.seed( 15 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -41.56 
     parameter initial gradient free
[1,]         1            58.44    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.406 
Function value: -12.2 
> print.default( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] -5.291e-07

$hessian
       [,1]
[1,] -17.28

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
           [,1]
  [1,]  0.31349
  [2,] -0.55771
  [3,]  0.28839
  [4,]  0.32759
  [5,]  0.08418
  [6,] -0.96147
  [7,]  0.26946
  [8,]  0.40652
  [9,]  0.20898
 [10,]  0.26054
 [11,]  0.36668
 [12,]  0.11188
 [13,]  0.12226
 [14,] -0.14609
 [15,] -1.16676
 [16,] -0.67554
 [17,] -0.01995
 [18,]  0.02832
 [19,] -0.72867
 [20,]  0.25708
 [21,]  0.05080
 [22,] -0.11852
 [23,] -0.04543
 [24,]  0.07171
 [25,] -1.85967
 [26,]  0.24891
 [27,]  0.20149
 [28,]  0.14710
 [29,]  0.32965
 [30,]  0.28775
 [31,]  0.13948
 [32,]  0.09780
 [33,]  0.02198
 [34,] -0.07456
 [35,]  0.24167
 [36,]  0.14143
 [37,] -0.09280
 [38,]  0.08259
 [39,]  0.17984
 [40,] -0.24059
 [41,]  0.23470
 [42,]  0.32515
 [43,] -0.31013
 [44,]  0.27432
 [45,]  0.15062
 [46,]  0.35936
 [47,] -0.16595
 [48,]  0.11667
 [49,]  0.41140
 [50,] -0.85562
 [51,]  0.36921
 [52,]  0.01178
 [53,]  0.09068
 [54,] -0.41852
 [55,]  0.16273
 [56,]  0.38103
 [57,] -0.24092
 [58,]  0.39322
 [59,]  0.18729
 [60,] -0.06962
 [61,] -0.52604
 [62,]  0.36746
 [63,]  0.21708
 [64,]  0.21913
 [65,]  0.25759
 [66,]  0.39797
 [67,]  0.29823
 [68,] -0.03090
 [69,] -0.09986
 [70,] -0.65673
 [71,]  0.05177
 [72,] -0.67134
 [73,]  0.32399
 [74,] -0.75222
 [75,]  0.20942
 [76,] -1.05025
 [77,]  0.39357
 [78,] -0.13051
 [79,] -1.34924
 [80,] -0.05024
 [81,]  0.23678
 [82,] -0.01697
 [83,]  0.15387
 [84,]  0.27503
 [85,]  0.15766
 [86,] -0.42969
 [87,]  0.21483
 [88,]  0.41434
 [89,]  0.21808
 [90,]  0.22108
 [91,] -0.07248
 [92,]  0.26976
 [93,] -0.06965
 [94,]  0.30826
 [95,] -0.17816
 [96,]  0.15251
 [97,]  0.18500
 [98,]  0.07499
 [99,]  0.36483
[100,]  0.09384

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] 5.224e-14

$hessian
       [,1]
[1,] -17.27

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
           [,1]
  [1,]  0.31349
  [2,] -0.55771
  [3,]  0.28839
  [4,]  0.32759
  [5,]  0.08418
  [6,] -0.96147
  [7,]  0.26946
  [8,]  0.40652
  [9,]  0.20898
 [10,]  0.26054
 [11,]  0.36668
 [12,]  0.11188
 [13,]  0.12226
 [14,] -0.14609
 [15,] -1.16676
 [16,] -0.67554
 [17,] -0.01995
 [18,]  0.02832
 [19,] -0.72867
 [20,]  0.25708
 [21,]  0.05080
 [22,] -0.11852
 [23,] -0.04543
 [24,]  0.07171
 [25,] -1.85967
 [26,]  0.24891
 [27,]  0.20149
 [28,]  0.14710
 [29,]  0.32965
 [30,]  0.28775
 [31,]  0.13948
 [32,]  0.09780
 [33,]  0.02198
 [34,] -0.07456
 [35,]  0.24167
 [36,]  0.14143
 [37,] -0.09280
 [38,]  0.08259
 [39,]  0.17984
 [40,] -0.24059
 [41,]  0.23470
 [42,]  0.32515
 [43,] -0.31013
 [44,]  0.27432
 [45,]  0.15062
 [46,]  0.35936
 [47,] -0.16595
 [48,]  0.11667
 [49,]  0.41140
 [50,] -0.85562
 [51,]  0.36921
 [52,]  0.01178
 [53,]  0.09068
 [54,] -0.41852
 [55,]  0.16273
 [56,]  0.38103
 [57,] -0.24092
 [58,]  0.39322
 [59,]  0.18729
 [60,] -0.06962
 [61,] -0.52604
 [62,]  0.36746
 [63,]  0.21708
 [64,]  0.21913
 [65,]  0.25759
 [66,]  0.39797
 [67,]  0.29823
 [68,] -0.03090
 [69,] -0.09986
 [70,] -0.65673
 [71,]  0.05177
 [72,] -0.67134
 [73,]  0.32399
 [74,] -0.75222
 [75,]  0.20942
 [76,] -1.05025
 [77,]  0.39357
 [78,] -0.13051
 [79,] -1.34924
 [80,] -0.05024
 [81,]  0.23678
 [82,] -0.01697
 [83,]  0.15387
 [84,]  0.27503
 [85,]  0.15766
 [86,] -0.42969
 [87,]  0.21483
 [88,]  0.41434
 [89,]  0.21808
 [90,]  0.22108
 [91,] -0.07248
 [92,]  0.26976
 [93,] -0.06965
 [94,]  0.30826
 [95,] -0.17816
 [96,]  0.15251
 [97,]  0.18500
 [98,]  0.07499
 [99,]  0.36483
[100,]  0.09384

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### summary.maxim and for "gradient"/"hessian" attributes
> ### Test for infinity
> ## maximize a 2D quadratic function:
> f <- function(b) {
+   x <- b[1]; y <- b[2];
+     val <- (x - 2)^2 + (y - 3)^2
+     attr(val, "gradient") <- c(2*x - 4, 2*y - 6)
+     attr(val, "hessian") <- matrix(c(2, 0, 0, 2), 2, 2)
+     val
+ }
> ## Use c(0,0) as initial value.  
> result1 <- maxNR( f, start = c(0,0) )
> print( result1 )
$maximum
[1] Inf

$estimate
[1] -7.035e+155 -1.055e+156

$gradient
[1] -1.407e+156 -2.110e+156

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 25

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result1 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 25 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,] -7.035e+155 -1.407e+156
[2,] -1.055e+156 -2.110e+156
--------------------------------------------
> ## Now use c(1000000, -777777) as initial value and ask for hessian
> result2 <- maxNR( f, start = c( 1000000, -777777))
> print( result2 )
$maximum
[1] Inf

$estimate
[1]  2.110e+155 -1.641e+155

$gradient
[1]  4.221e+155 -3.283e+155

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 24

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result2 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 24 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,]  2.110e+155  4.221e+155
[2,] -1.641e+155 -3.283e+155
--------------------------------------------
> 
> 
> ### Test for "gradient"/"hessian" attributes.  A case which converges.
> hub <- function(x) {
+    v <- exp(-sum(x*x))
+    val <- v
+    attr(val, "gradient") <- -2*x*v
+    attr(val, "hessian") <- 4*(x %*% t(x))*v - diag(2*c(v, v))
+    val
+ }
> summary(a <- maxNR(hub, start=c(2,1)))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
       estimate  gradient
[1,] -7.448e-18 1.490e-17
[2,] -3.724e-18 7.448e-18
--------------------------------------------
> ## Now test "gradient" attribute for BHHH/3-parameter probit
> N <- 1000
> loglikProbit <- function( beta) {
+    xb <- x %*% beta
+    loglik <- ifelse(y == 0,
+                     pnorm( xb, log=TRUE, lower.tail=FALSE),
+                     pnorm( xb, log.p=TRUE))
+    grad <- ifelse(y == 0,
+                   -dnorm(xb)/pnorm(xb, lower.tail=FALSE),
+                   dnorm(xb)/pnorm(xb))
+    grad <- grad*x
+    attr(loglik, "gradient") <- grad
+    loglik
+ }
> x <- runif(N)
> x <- cbind(x, x - runif(N), x - runif(N))
> y <- x[,1] + 2*x[,2] - x[,3] + rnorm(N) > 0
> summary(maxLik(loglikProbit, start=c(0,0,0), method="bhhh"))
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -508.4 
3  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.8578     0.0904    9.49 < 2e-16 ***
[2,]   1.9389     0.1514   12.81 < 2e-16 ***
[3,]  -0.8253     0.1339   -6.16 7.2e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> 
> ### vcov.maxLik
> set.seed( 17 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -53.67 
     parameter initial gradient free
[1,]         1            46.33    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 1.863 
Function value: -37.76 
> print.default( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] -3.469e-08

$hessian
       [,1]
[1,] -28.79

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
           [,1]
  [1,] -0.27669
  [2,]  0.39530
  [3,]  0.49787
  [4,] -0.48403
  [5,] -0.40490
  [6,]  0.25669
  [7,] -0.43226
  [8,]  0.18930
  [9,]  0.20658
 [10,]  0.32847
 [11,]  0.16648
 [12,]  0.39363
 [13,]  0.51043
 [14,]  0.32613
 [15,]  0.23353
 [16,]  0.50455
 [17,]  0.46120
 [18,]  0.46077
 [19,] -1.06433
 [20,]  0.23755
 [21,] -0.04647
 [22,]  0.47335
 [23,] -0.40035
 [24,]  0.23480
 [25,]  0.28459
 [26,]  0.40289
 [27,] -0.23785
 [28,]  0.44112
 [29,]  0.48214
 [30,]  0.49549
 [31,] -0.36521
 [32,]  0.38776
 [33,] -0.40673
 [34,] -0.18088
 [35,]  0.41854
 [36,] -0.33043
 [37,] -0.24039
 [38,] -0.41534
 [39,]  0.46117
 [40,] -3.89298
 [41,]  0.03343
 [42,] -0.62928
 [43,]  0.43249
 [44,]  0.03649
 [45,]  0.24621
 [46,] -0.22675
 [47,]  0.53062
 [48,]  0.51600
 [49,] -0.67720
 [50,]  0.15256
 [51,]  0.22195
 [52,]  0.43626
 [53,] -1.56329
 [54,] -0.05016
 [55,]  0.31242
 [56,] -0.14645
 [57,] -0.35672
 [58,]  0.48098
 [59,]  0.39851
 [60,]  0.42334
 [61,]  0.33007
 [62,]  0.35261
 [63,] -0.30181
 [64,]  0.47083
 [65,]  0.29104
 [66,]  0.11972
 [67,]  0.50740
 [68,] -0.24952
 [69,] -0.05047
 [70,]  0.28001
 [71,]  0.52555
 [72,] -0.37398
 [73,]  0.02352
 [74,] -0.41329
 [75,] -0.35850
 [76,]  0.47664
 [77,]  0.24155
 [78,] -0.04115
 [79,] -1.34772
 [80,] -0.25668
 [81,]  0.32432
 [82,]  0.34521
 [83,] -0.93416
 [84,]  0.33243
 [85,] -0.84979
 [86,]  0.31576
 [87,]  0.47518
 [88,]  0.37259
 [89,] -0.02530
 [90,]  0.37658
 [91,] -2.10830
 [92,] -0.44964
 [93,]  0.16821
 [94,]  0.35101
 [95,]  0.52544
 [96,] -0.06619
 [97,]  0.38782
 [98,]  0.25350
 [99,] -0.79665
[100,]  0.13325

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> vcov(a)
        [,1]
[1,] 0.03473
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] 1.092e-09

$hessian
      [,1]
[1,] -28.8

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
           [,1]
  [1,] -0.27669
  [2,]  0.39530
  [3,]  0.49787
  [4,] -0.48403
  [5,] -0.40490
  [6,]  0.25669
  [7,] -0.43226
  [8,]  0.18930
  [9,]  0.20658
 [10,]  0.32847
 [11,]  0.16648
 [12,]  0.39363
 [13,]  0.51043
 [14,]  0.32613
 [15,]  0.23353
 [16,]  0.50455
 [17,]  0.46120
 [18,]  0.46077
 [19,] -1.06433
 [20,]  0.23755
 [21,] -0.04647
 [22,]  0.47335
 [23,] -0.40035
 [24,]  0.23480
 [25,]  0.28459
 [26,]  0.40289
 [27,] -0.23785
 [28,]  0.44112
 [29,]  0.48214
 [30,]  0.49549
 [31,] -0.36521
 [32,]  0.38776
 [33,] -0.40673
 [34,] -0.18088
 [35,]  0.41854
 [36,] -0.33043
 [37,] -0.24039
 [38,] -0.41534
 [39,]  0.46117
 [40,] -3.89298
 [41,]  0.03343
 [42,] -0.62928
 [43,]  0.43249
 [44,]  0.03649
 [45,]  0.24621
 [46,] -0.22675
 [47,]  0.53062
 [48,]  0.51600
 [49,] -0.67720
 [50,]  0.15256
 [51,]  0.22195
 [52,]  0.43626
 [53,] -1.56329
 [54,] -0.05016
 [55,]  0.31242
 [56,] -0.14645
 [57,] -0.35672
 [58,]  0.48098
 [59,]  0.39851
 [60,]  0.42334
 [61,]  0.33007
 [62,]  0.35261
 [63,] -0.30181
 [64,]  0.47083
 [65,]  0.29104
 [66,]  0.11972
 [67,]  0.50740
 [68,] -0.24952
 [69,] -0.05047
 [70,]  0.28001
 [71,]  0.52555
 [72,] -0.37398
 [73,]  0.02352
 [74,] -0.41329
 [75,] -0.35850
 [76,]  0.47664
 [77,]  0.24155
 [78,] -0.04115
 [79,] -1.34772
 [80,] -0.25668
 [81,]  0.32432
 [82,]  0.34521
 [83,] -0.93416
 [84,]  0.33243
 [85,] -0.84979
 [86,]  0.31576
 [87,]  0.47518
 [88,]  0.37259
 [89,] -0.02530
 [90,]  0.37658
 [91,] -2.10830
 [92,] -0.44964
 [93,]  0.16821
 [94,]  0.35101
 [95,]  0.52544
 [96,] -0.06619
 [97,]  0.38782
 [98,]  0.25350
 [99,] -0.79665
[100,]  0.13325

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> vcov(a)
        [,1]
[1,] 0.03472
> print(stdEr(a))
[1] 0.1863
>                            # test single stdEr
> 
> proc.time()
   user  system elapsed 
  0.580   0.024   0.589 
