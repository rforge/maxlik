
R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library( maxLik )
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> options(digits=4)
> 
> printRounded <- function( x ) {
+    for( i in names( x ) ) {
+       cat ( "$", i, "\n", sep = "" )
+       if( is.numeric( x[[i]] ) ) {
+          print( round( x[[i]], 4 ) )
+       } else {
+          print( x[[i]] )
+       }
+       cat( "\n" )
+    }
+    cat( "attr(,\"class\")\n" )
+    print( class( x ) )
+ }
> 
> 
> ### activePar
> # a simple two-dimensional exponential hat
> f <- function(a) exp(-(a[1]-2)^2 - (a[2]-4)^2)
> #
> # maximize wrt. both parameters 
> free <- maxNR(f, start=1:2)
> printRounded( free )
$maximum
[1] 1

$estimate
[1] 2 4

$gradient
[1] 0 0

$hessian
       [,1]   [,2]
[1,] -2e+00  1e-04
[2,]  1e-04 -2e+00

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(free)  # results should be close to (2,4)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate gradient
[1,]        2        0
[2,]        4        0
--------------------------------------------
> activePar(free)
[1] TRUE TRUE
> # allow only the second parameter to vary
> cons <- maxNR(f, start=1:2, activePar=c(FALSE,TRUE))
> printRounded( cons )
$maximum
[1] 0.3679

$estimate
[1] 1 4

$gradient
[1] NA  0

$hessian
     [,1]    [,2]
[1,]   NA      NA
[2,]   NA -0.7359

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1]  TRUE FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(cons) # result should be around (1,4)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
     estimate  gradient
[1,]        1        NA
[2,]        4 5.944e-07
--------------------------------------------
> activePar(cons)
[1] FALSE  TRUE
> # specify fixed par in different ways
> cons2 <- maxNR(f, start=1:2, fixed=1)
> all.equal( cons, cons2 )
[1] TRUE
> cons3 <- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
> all.equal( cons, cons3 )
[1] TRUE
> cons4 <- maxNR(f, start=c(a=1, b=2), fixed="a")
> print(summary(cons4))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
  estimate  gradient
a        1        NA
b        4 5.944e-07
--------------------------------------------
> all.equal( cons, cons4 )
[1] "Component 2: names for current but not for target"                             
[2] "Component 3: names for current but not for target"                             
[3] "Component 4: Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component 8: names for current but not for target"                             
> 
> ### compareDerivatives
> set.seed( 2 )
> ## A simple example with sin(x)' = cos(x)
> f <- sin
> compareDerivatives(f, cos, t0=1)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.8415
Dim of analytic gradient: 1 1 
       numeric          : 1 1 
      
param  theta 0 analytic numeric  rel.diff
  [1,]       1   0.5403  0.5403 -5.13e-11
Max relative difference: 5.13e-11 
-------- END of compare derivatives -------- 
> ##
> ## Example of log-likelihood of normal density.  Two-parameter
> ## function.
> x <- rnorm(100, 1, 2) # generate rnorm x
> l <- function(b) sum(log(dnorm((x-b[1])/b[2])/b[2]))
>               # b[1] - mu, b[2] - sigma
> gradl <- function(b) {
+    c(sum(x - b[1])/b[2]^2,
+    sum((x - b[1])^2/b[2]^3 - 1/b[2]))
+ }
> compareDerivatives(l, gradl, t0=c(1,2))
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] -227.9
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
       [,1]  [,2]
[1,] -1.535 16.68
numeric gradient
       [,1]  [,2]
[1,] -1.535 16.68
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -1.989e-09 -2.089e-10
Max relative difference: 1.989e-09 
-------- END of compare derivatives -------- 
> 
> 
> ### hessian
> set.seed( 3 )
> # log-likelihood for normal density
> # a[1] - mean
> # a[2] - standard deviation
> ll <- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
> x <- rnorm(1000) # sample from standard normal
> ml <- maxLik(ll, start=c(1,1))
> # ignore eventual warnings "NaNs produced in: log(x)"
> printRounded( ml )
$maximum
[1] -497.6

$estimate
[1] 0.0064 0.9976

$gradient
[1] 0 0

$hessian
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (2 free parameter(s))
Estimate(s): 0.006397 0.9976 
> summary(ml) # result should be close to c(0,1)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
2  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.0064     0.0316     0.2    0.84    
[2,]   0.9976     0.0223    44.7  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> hessian(ml) # How the Hessian looks like
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010
> sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
        [,1]    [,2]
[1,] 0.03155 0.00000
[2,] 0.00000 0.02231
> print(stdEr(ml))
[1] 0.03155 0.02231
>                            # test vector of stdEr-s
> #
> # Now run the same example while fixing a[2] = 1
> mlf <- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
> printRounded( mlf )
$maximum
[1] -497.6

$estimate
[1] 0.0064 1.0000

$gradient
[1]  0 NA

$hessian
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE  TRUE

$iterations
[1] 3

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( mlf )
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (1 free parameter(s))
Estimate(s): 0.006397 1 
> summary(mlf) # first parameter close to 0, the second exactly 1.0
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)
[1,]   0.0064     0.0316     0.2    0.84
[2,]   1.0000     0.0000      NA      NA
--------------------------------------------
> hessian(mlf)
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA
> # now invert only the free parameter part of the Hessian
> sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
        [,1]
[1,] 0.03162
> # gives the standard deviation for the mean
> print(stdEr(mlf))
[1] 0.03162 0.00000
>                            # test standard errors with fixed par
> 
> 
> ### maxBFGS
> set.seed( 5 )
> # Maximum Likelihood estimation of the parameter of Poissonian distribution
> n <- rpois(100, 3)
> loglik <- function(l) n*log(l) - l - lfactorial(n)
> # we use numeric gradient
> a <- maxBFGS(loglik, start=1)
> print( a )
$maximum
[1] -199.2

$estimate
[1] 3.19

$gradient
[1] 1.583e-05

$hessian
       [,1]
[1,] -31.29

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE

$iterations
function 
      29 

$type
[1] "BFGS maximisation"

$constraints
NULL

$gradientObs
           [,1]
  [1,] -0.37304
  [2,]  0.25392
  [3,]  0.88088
  [4,] -0.37304
  [5,] -0.68652
  [6,]  0.25392
  [7,] -0.05956
  [8,]  0.25392
  [9,]  0.88088
 [10,] -0.68652
 [11,] -0.37304
 [12,] -0.05956
 [13,] -0.37304
 [14,] -0.05956
 [15,] -0.37304
 [16,] -0.37304
 [17,] -0.37304
 [18,]  0.56740
 [19,] -0.05956
 [20,]  0.56740
 [21,]  0.56740
 [22,]  0.25392
 [23,] -0.37304
 [24,] -0.37304
 [25,] -0.68652
 [26,] -0.05956
 [27,] -0.05956
 [28,]  0.88088
 [29,] -0.68652
 [30,]  0.88088
 [31,] -0.05956
 [32,] -0.68652
 [33,] -0.37304
 [34,] -1.00000
 [35,] -1.00000
 [36,] -0.05956
 [37,] -0.05956
 [38,] -0.05956
 [39,] -0.37304
 [40,] -0.37304
 [41,]  0.56740
 [42,] -0.37304
 [43,]  0.56740
 [44,] -0.05956
 [45,]  0.88088
 [46,] -0.05956
 [47,]  0.25392
 [48,] -0.68652
 [49,]  0.25392
 [50,] -0.05956
 [51,] -0.37304
 [52,] -0.05956
 [53,]  0.88088
 [54,]  1.19436
 [55,]  0.88088
 [56,] -0.37304
 [57,] -0.37304
 [58,] -0.37304
 [59,] -0.68652
 [60,] -0.68652
 [61,] -0.05956
 [62,] -0.68652
 [63,]  0.56740
 [64,]  1.50784
 [65,]  1.19436
 [66,]  0.56740
 [67,]  0.56740
 [68,] -0.68652
 [69,]  0.88088
 [70,]  0.56740
 [71,]  0.56740
 [72,]  0.25392
 [73,] -0.68652
 [74,]  0.56740
 [75,] -1.00000
 [76,]  0.88088
 [77,] -1.00000
 [78,]  0.25392
 [79,] -0.05956
 [80,] -0.37304
 [81,]  0.88088
 [82,]  0.56740
 [83,] -0.37304
 [84,] -0.68652
 [85,] -0.05956
 [86,] -0.68652
 [87,]  1.19436
 [88,]  0.25392
 [89,] -0.37304
 [90,] -0.05956
 [91,]  0.25392
 [92,] -0.37304
 [93,] -0.68652
 [94,] -0.37304
 [95,] -0.37304
 [96,]  0.56740
 [97,] -0.37304
 [98,] -0.05956
 [99,] -0.05956
[100,] -0.05956

attr(,"class")
[1] "maxim"
> summary( a )
--------------------------------------------
BFGS maximisation 
Number of iterations: 29 
Return code: 0 
successful convergence  
Function value: -199.2 
Estimates:
     estimate  gradient
[1,]     3.19 1.583e-05
--------------------------------------------
> # you would probably prefer mean(n) instead of that ;-)
> # Note also that maxLik is better suited for Maximum Likelihood
> 
> 
> ### logLik.maxLik
> set.seed( 4 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -25.05

$estimate
[1] 2.116

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.34

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,]  0.3868
  [2,] -1.6794
  [3,]  0.0386
  [4,]  0.0713
  [5,]  0.1590
  [6,]  0.1052
  [7,]  0.2482
  [8,]  0.4473
  [9,]  0.2179
 [10,]  0.0540
 [11,] -0.8675
 [12,]  0.3286
 [13,]  0.2702
 [14,]  0.2581
 [15,]  0.3028
 [16,] -0.0520
 [17,]  0.4428
 [18,]  0.4055
 [19,] -0.4474
 [20,] -0.0334
 [21,]  0.3506
 [22,] -0.1508
 [23,] -2.2973
 [24,]  0.3887
 [25,] -0.4441
 [26,]  0.4434
 [27,]  0.2769
 [28,] -0.1512
 [29,]  0.2267
 [30,]  0.1922
 [31,] -0.2164
 [32,] -0.4273
 [33,] -0.4157
 [34,]  0.2782
 [35,] -0.6370
 [36,]  0.3945
 [37,]  0.3441
 [38,] -0.6203
 [39,]  0.4578
 [40,]  0.1672
 [41,]  0.3538
 [42,] -0.0653
 [43,]  0.1477
 [44,]  0.2827
 [45,] -0.0152
 [46,]  0.0799
 [47,]  0.2744
 [48,]  0.4523
 [49,] -1.1449
 [50,]  0.4053
 [51,] -0.2277
 [52,]  0.4333
 [53,]  0.0814
 [54,] -0.0811
 [55,] -0.7399
 [56,]  0.2072
 [57,]  0.1135
 [58,]  0.1192
 [59,]  0.3430
 [60,]  0.0932
 [61,]  0.4402
 [62,] -0.0730
 [63,] -0.5010
 [64,]  0.0754
 [65,] -0.1722
 [66,]  0.0454
 [67,] -0.0258
 [68,]  0.1817
 [69,]  0.4480
 [70,] -0.1601
 [71,]  0.4398
 [72,]  0.2483
 [73,]  0.4031
 [74,] -0.1907
 [75,] -0.4727
 [76,] -0.0651
 [77,] -0.4552
 [78,]  0.1595
 [79,]  0.3768
 [80,]  0.1216
 [81,]  0.3019
 [82,] -0.0012
 [83,]  0.4141
 [84,]  0.4010
 [85,]  0.3493
 [86,] -0.9970
 [87,]  0.3787
 [88,]  0.3850
 [89,] -0.3168
 [90,]  0.1926
 [91,]  0.3287
 [92,] -0.0422
 [93,]  0.0606
 [94,] -0.6449
 [95,] -0.6326
 [96,] -0.3563
 [97,] -0.3240
 [98,]  0.2205
 [99,] -0.8326
[100,]  0.3611

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> ## print log likelihood value
> logLik( a )
[1] -25.05
> ## compare with log likelihood value of summary object
> all.equal( logLik( a ), logLik( summary( a ) ) )
[1] TRUE
> 
> 
> ### maxBHHH
> set.seed( 6 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxBHHH(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -45.5 
     parameter initial gradient free
[1,]         1             54.5    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
successive function values within tolerance limit 
6  iterations
estimate: 2.198 
Function value: -21.25 
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> ## Estimate with analytic gradient
> a <- maxBHHH(loglik, gradlik, start=1)
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> 
> 
> ### maxLik
> set.seed( 7 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -47.72 
     parameter initial gradient free
[1,]         1            52.28    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.095 
Function value: -26.03 
> printRounded( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.76

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,]  0.4531
  [2,] -0.3338
  [3,] -0.3793
  [4,]  0.0712
  [5,]  0.2044
  [6,] -0.8329
  [7,]  0.1013
  [8,] -1.6593
  [9,]  0.3749
 [10,]  0.4545
 [11,]  0.0051
 [12,]  0.1032
 [13,] -0.4588
 [14,] -0.4565
 [15,] -0.1279
 [16,]  0.3046
 [17,]  0.1388
 [18,] -0.0017
 [19,]  0.2175
 [20,]  0.0199
 [21,] -0.7230
 [22,]  0.1610
 [23,]  0.2786
 [24,]  0.2572
 [25,]  0.2032
 [26,]  0.3495
 [27,]  0.2543
 [28,] -0.1430
 [29,] -0.3671
 [30,] -0.4648
 [31,] -0.1518
 [32,]  0.0829
 [33,] -0.4836
 [34,]  0.4729
 [35,]  0.2134
 [36,]  0.2836
 [37,]  0.4332
 [38,]  0.3182
 [39,]  0.0139
 [40,]  0.0312
 [41,]  0.1365
 [42,]  0.3564
 [43,] -0.2541
 [44,]  0.4095
 [45,] -0.0211
 [46,]  0.1728
 [47,] -0.2433
 [48,] -0.8212
 [49,]  0.2151
 [50,] -0.0651
 [51,] -0.2029
 [52,] -0.0574
 [53,] -1.6405
 [54,]  0.4141
 [55,]  0.4053
 [56,]  0.4372
 [57,] -0.0792
 [58,]  0.1532
 [59,] -0.4604
 [60,] -0.1621
 [61,] -0.0035
 [62,] -0.0570
 [63,]  0.4624
 [64,] -0.5310
 [65,]  0.2925
 [66,] -0.1617
 [67,] -1.1256
 [68,]  0.3481
 [69,]  0.4113
 [70,]  0.1879
 [71,] -0.4522
 [72,]  0.1446
 [73,]  0.4468
 [74,]  0.0540
 [75,]  0.2706
 [76,] -0.0117
 [77,]  0.2177
 [78,] -0.3570
 [79,]  0.3521
 [80,] -0.8323
 [81,]  0.1982
 [82,]  0.4722
 [83,]  0.2493
 [84,]  0.1677
 [85,]  0.2190
 [86,]  0.1717
 [87,]  0.3099
 [88,]  0.4640
 [89,] -0.4283
 [90,]  0.4145
 [91,]  0.0925
 [92,]  0.2582
 [93,] -0.4882
 [94,]  0.4597
 [95,] -0.8020
 [96,]  0.4124
 [97,]  0.0536
 [98,]  0.1786
 [99,] -0.1365
[100,]  0.1661

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.78

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,]  0.4531
  [2,] -0.3338
  [3,] -0.3793
  [4,]  0.0712
  [5,]  0.2044
  [6,] -0.8329
  [7,]  0.1013
  [8,] -1.6593
  [9,]  0.3749
 [10,]  0.4545
 [11,]  0.0051
 [12,]  0.1032
 [13,] -0.4588
 [14,] -0.4565
 [15,] -0.1279
 [16,]  0.3046
 [17,]  0.1388
 [18,] -0.0017
 [19,]  0.2175
 [20,]  0.0199
 [21,] -0.7230
 [22,]  0.1610
 [23,]  0.2786
 [24,]  0.2572
 [25,]  0.2032
 [26,]  0.3495
 [27,]  0.2543
 [28,] -0.1430
 [29,] -0.3671
 [30,] -0.4648
 [31,] -0.1518
 [32,]  0.0829
 [33,] -0.4836
 [34,]  0.4729
 [35,]  0.2134
 [36,]  0.2836
 [37,]  0.4332
 [38,]  0.3182
 [39,]  0.0139
 [40,]  0.0312
 [41,]  0.1365
 [42,]  0.3564
 [43,] -0.2541
 [44,]  0.4095
 [45,] -0.0211
 [46,]  0.1728
 [47,] -0.2433
 [48,] -0.8212
 [49,]  0.2151
 [50,] -0.0651
 [51,] -0.2029
 [52,] -0.0574
 [53,] -1.6405
 [54,]  0.4141
 [55,]  0.4053
 [56,]  0.4372
 [57,] -0.0792
 [58,]  0.1532
 [59,] -0.4604
 [60,] -0.1621
 [61,] -0.0035
 [62,] -0.0570
 [63,]  0.4624
 [64,] -0.5310
 [65,]  0.2925
 [66,] -0.1617
 [67,] -1.1256
 [68,]  0.3481
 [69,]  0.4113
 [70,]  0.1879
 [71,] -0.4522
 [72,]  0.1446
 [73,]  0.4468
 [74,]  0.0540
 [75,]  0.2706
 [76,] -0.0117
 [77,]  0.2177
 [78,] -0.3570
 [79,]  0.3521
 [80,] -0.8323
 [81,]  0.1982
 [82,]  0.4722
 [83,]  0.2493
 [84,]  0.1677
 [85,]  0.2190
 [86,]  0.1717
 [87,]  0.3099
 [88,]  0.4640
 [89,] -0.4283
 [90,]  0.4145
 [91,]  0.0925
 [92,]  0.2582
 [93,] -0.4882
 [94,]  0.4597
 [95,] -0.8020
 [96,]  0.4124
 [97,]  0.0536
 [98,]  0.1786
 [99,] -0.1365
[100,]  0.1661

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### maxNR
> set.seed( 8 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglikSum <- function(theta) sum(log(theta) - theta*t)
> ## Note the log-likelihood and gradient are summed over observations
> gradlikSum <- function(theta) sum(1/theta - t)
> ## Estimate with numeric gradient and Hessian
> a <- maxNR(loglikSum, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -46.49 
     parameter initial gradient free
[1,]         1            53.51    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.151 
Function value: -23.41 
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] -2.416e-07

$hessian
       [,1]
[1,] -21.62

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate   gradient
[1,]    2.151 -2.416e-07
--------------------------------------------
> ## You would probably prefer 1/mean(t) instead ;-)
> ## Estimate with analytic gradient and Hessian
> a <- maxNR(loglikSum, gradlikSum, hesslik, start=1)
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] 9.493e-08

$hessian
       [,1]
[1,] -21.61

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate  gradient
[1,]    2.151 9.493e-08
--------------------------------------------
> 
> 
> ### maximType
> ## maximise two-dimensional exponential hat.  Maximum is at c(2,1):
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m <- maxNR(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.11e-10 0.00e+00

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(m)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate gradient
[1,]        2 1.11e-10
[2,]        1 0.00e+00
--------------------------------------------
> maximType(m)
[1] "Newton-Raphson maximisation"
> ## Now use BFGS maximisation.
> m <- maxBFGS(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.088e-08 5.329e-09

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
function 
      26 

$type
[1] "BFGS maximisation"

$constraints
NULL

attr(,"class")
[1] "maxim"
> summary(m)
--------------------------------------------
BFGS maximisation 
Number of iterations: 26 
Return code: 0 
successful convergence  
Function value: 1 
Estimates:
     estimate  gradient
[1,]        2 1.088e-08
[2,]        1 5.329e-09
--------------------------------------------
> maximType(m)
[1] "BFGS maximisation"
> 
> ### Test maxNR with 0 iterations.  Should perform no iterations
> ### Request by Yves Croissant
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m0 <- maxNR(f, start=c(1.1, 2.1), iterlim=0)
> summary(m0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 0 
Return code: 4 
Iteration limit exceeded. 
Function value: 0.1327 
Estimates:
     estimate gradient
[1,]      1.1   0.2388
[2,]      2.1  -0.2918
--------------------------------------------
> 
> ### nObs
> set.seed( 10 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> nObs(m)
[1] 100
> 
> 
> ### nParam
> set.seed( 11 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> summary(m)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3436 -0.5338 -0.0291  0.5501  2.6934 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.242      0.287    11.3   <2e-16 ***
x1             3.974      0.395    10.1   <2e-16 ***
x2             4.783      0.367    13.0   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.99 on 97 degrees of freedom
Multiple R-squared:  0.702,	Adjusted R-squared:  0.696 
F-statistic:  114 on 2 and 97 DF,  p-value: <2e-16

> nParam(m) # you get 3
[1] 3
> 
> 
> ### numericGradient
> # A simple example with Gaussian bell
> f0 <- function(t0) exp(-t0[1]^2 - t0[2]^2)
> numericGradient(f0, c(1,2))
         [,1]     [,2]
[1,] -0.01348 -0.02695
> numericHessian(f0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01349 0.05390
[2,] 0.05390 0.09433
> # An example with the analytic gradient
> gradf0 <- function(t0) -2*t0*f0(t0)
> numericHessian(f0, gradf0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01348 0.05390
[2,] 0.05390 0.09433
> # The results should be similar as in the previous case
> # The central numeric derivatives have usually quite a high precision
> compareDerivatives(f0, gradf0, t0=1:2)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.006738
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
numeric gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -2.764e-10 -5.108e-11
Max relative difference: 2.764e-10 
-------- END of compare derivatives -------- 
> # The differenc is around 1e-10
> 
> 
> ### returnCode
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should be success (1 or 2)
[1] 1
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should give a failure (4)
[1] 4
> 
> 
> ### returnMessage
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should be success (1 or 2)
[1] "gradient close to zero"
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should give a failure (4)
[1] "Iteration limit exceeded."
> 
> 
> ### summary.maxLik
> set.seed( 15 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -41.56 
     parameter initial gradient free
[1,]         1            58.44    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.406 
Function value: -12.2 
> printRounded( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] 0

$hessian
       [,1]
[1,] -17.28

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,]  0.3135
  [2,] -0.5577
  [3,]  0.2884
  [4,]  0.3276
  [5,]  0.0842
  [6,] -0.9615
  [7,]  0.2695
  [8,]  0.4065
  [9,]  0.2090
 [10,]  0.2605
 [11,]  0.3667
 [12,]  0.1119
 [13,]  0.1223
 [14,] -0.1461
 [15,] -1.1668
 [16,] -0.6755
 [17,] -0.0199
 [18,]  0.0283
 [19,] -0.7287
 [20,]  0.2571
 [21,]  0.0508
 [22,] -0.1185
 [23,] -0.0454
 [24,]  0.0717
 [25,] -1.8597
 [26,]  0.2489
 [27,]  0.2015
 [28,]  0.1471
 [29,]  0.3296
 [30,]  0.2877
 [31,]  0.1395
 [32,]  0.0978
 [33,]  0.0220
 [34,] -0.0746
 [35,]  0.2417
 [36,]  0.1414
 [37,] -0.0928
 [38,]  0.0826
 [39,]  0.1798
 [40,] -0.2406
 [41,]  0.2347
 [42,]  0.3251
 [43,] -0.3101
 [44,]  0.2743
 [45,]  0.1506
 [46,]  0.3594
 [47,] -0.1660
 [48,]  0.1167
 [49,]  0.4114
 [50,] -0.8556
 [51,]  0.3692
 [52,]  0.0118
 [53,]  0.0907
 [54,] -0.4185
 [55,]  0.1627
 [56,]  0.3810
 [57,] -0.2409
 [58,]  0.3932
 [59,]  0.1873
 [60,] -0.0696
 [61,] -0.5260
 [62,]  0.3675
 [63,]  0.2171
 [64,]  0.2191
 [65,]  0.2576
 [66,]  0.3980
 [67,]  0.2982
 [68,] -0.0309
 [69,] -0.0999
 [70,] -0.6567
 [71,]  0.0518
 [72,] -0.6713
 [73,]  0.3240
 [74,] -0.7522
 [75,]  0.2094
 [76,] -1.0502
 [77,]  0.3936
 [78,] -0.1305
 [79,] -1.3492
 [80,] -0.0502
 [81,]  0.2368
 [82,] -0.0170
 [83,]  0.1539
 [84,]  0.2750
 [85,]  0.1577
 [86,] -0.4297
 [87,]  0.2148
 [88,]  0.4143
 [89,]  0.2181
 [90,]  0.2211
 [91,] -0.0725
 [92,]  0.2698
 [93,] -0.0696
 [94,]  0.3083
 [95,] -0.1782
 [96,]  0.1525
 [97,]  0.1850
 [98,]  0.0750
 [99,]  0.3648
[100,]  0.0938

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] 0

$hessian
       [,1]
[1,] -17.27

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,]  0.3135
  [2,] -0.5577
  [3,]  0.2884
  [4,]  0.3276
  [5,]  0.0842
  [6,] -0.9615
  [7,]  0.2695
  [8,]  0.4065
  [9,]  0.2090
 [10,]  0.2605
 [11,]  0.3667
 [12,]  0.1119
 [13,]  0.1223
 [14,] -0.1461
 [15,] -1.1668
 [16,] -0.6755
 [17,] -0.0199
 [18,]  0.0283
 [19,] -0.7287
 [20,]  0.2571
 [21,]  0.0508
 [22,] -0.1185
 [23,] -0.0454
 [24,]  0.0717
 [25,] -1.8597
 [26,]  0.2489
 [27,]  0.2015
 [28,]  0.1471
 [29,]  0.3296
 [30,]  0.2877
 [31,]  0.1395
 [32,]  0.0978
 [33,]  0.0220
 [34,] -0.0746
 [35,]  0.2417
 [36,]  0.1414
 [37,] -0.0928
 [38,]  0.0826
 [39,]  0.1798
 [40,] -0.2406
 [41,]  0.2347
 [42,]  0.3251
 [43,] -0.3101
 [44,]  0.2743
 [45,]  0.1506
 [46,]  0.3594
 [47,] -0.1660
 [48,]  0.1167
 [49,]  0.4114
 [50,] -0.8556
 [51,]  0.3692
 [52,]  0.0118
 [53,]  0.0907
 [54,] -0.4185
 [55,]  0.1627
 [56,]  0.3810
 [57,] -0.2409
 [58,]  0.3932
 [59,]  0.1873
 [60,] -0.0696
 [61,] -0.5260
 [62,]  0.3675
 [63,]  0.2171
 [64,]  0.2191
 [65,]  0.2576
 [66,]  0.3980
 [67,]  0.2982
 [68,] -0.0309
 [69,] -0.0999
 [70,] -0.6567
 [71,]  0.0518
 [72,] -0.6713
 [73,]  0.3240
 [74,] -0.7522
 [75,]  0.2094
 [76,] -1.0502
 [77,]  0.3936
 [78,] -0.1305
 [79,] -1.3492
 [80,] -0.0502
 [81,]  0.2368
 [82,] -0.0170
 [83,]  0.1539
 [84,]  0.2750
 [85,]  0.1577
 [86,] -0.4297
 [87,]  0.2148
 [88,]  0.4143
 [89,]  0.2181
 [90,]  0.2211
 [91,] -0.0725
 [92,]  0.2698
 [93,] -0.0696
 [94,]  0.3083
 [95,] -0.1782
 [96,]  0.1525
 [97,]  0.1850
 [98,]  0.0750
 [99,]  0.3648
[100,]  0.0938

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### summary.maxim and for "gradient"/"hessian" attributes
> ### Test for infinity
> ## maximize a 2D quadratic function:
> f <- function(b) {
+   x <- b[1]; y <- b[2];
+     val <- (x - 2)^2 + (y - 3)^2
+     attr(val, "gradient") <- c(2*x - 4, 2*y - 6)
+     attr(val, "hessian") <- matrix(c(2, 0, 0, 2), 2, 2)
+     val
+ }
> ## Use c(0,0) as initial value.  
> result1 <- maxNR( f, start = c(0,0) )
> print( result1 )
$maximum
[1] Inf

$estimate
[1] -7.035e+155 -1.055e+156

$gradient
[1] -1.407e+156 -2.110e+156

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 25

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result1 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 25 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,] -7.035e+155 -1.407e+156
[2,] -1.055e+156 -2.110e+156
--------------------------------------------
> ## Now use c(1000000, -777777) as initial value and ask for hessian
> result2 <- maxNR( f, start = c( 1000000, -777777))
> print( result2 )
$maximum
[1] Inf

$estimate
[1]  2.110e+155 -1.641e+155

$gradient
[1]  4.221e+155 -3.283e+155

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 24

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result2 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 24 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,]  2.110e+155  4.221e+155
[2,] -1.641e+155 -3.283e+155
--------------------------------------------
> 
> 
> ### Test for "gradient"/"hessian" attributes.  A case which converges.
> hub <- function(x) {
+    v <- exp(-sum(x*x))
+    val <- v
+    attr(val, "gradient") <- -2*x*v
+    attr(val, "hessian") <- 4*(x %*% t(x))*v - diag(2*c(v, v))
+    val
+ }
> summary(a <- maxNR(hub, start=c(2,1)))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
       estimate  gradient
[1,] -7.448e-18 1.490e-17
[2,] -3.724e-18 7.448e-18
--------------------------------------------
> ## Now test "gradient" attribute for BHHH/3-parameter probit
> N <- 1000
> loglikProbit <- function( beta) {
+    xb <- x %*% beta
+    loglik <- ifelse(y == 0,
+                     pnorm( xb, log=TRUE, lower.tail=FALSE),
+                     pnorm( xb, log.p=TRUE))
+    grad <- ifelse(y == 0,
+                   -dnorm(xb)/pnorm(xb, lower.tail=FALSE),
+                   dnorm(xb)/pnorm(xb))
+    grad <- grad*x
+    attr(loglik, "gradient") <- grad
+    loglik
+ }
> x <- runif(N)
> x <- cbind(x, x - runif(N), x - runif(N))
> y <- x[,1] + 2*x[,2] - x[,3] + rnorm(N) > 0
> summary(maxLik(loglikProbit, start=c(0,0,0), method="bhhh"))
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -508.4 
3  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.8578     0.0904    9.49 < 2e-16 ***
[2,]   1.9389     0.1514   12.81 < 2e-16 ***
[3,]  -0.8253     0.1339   -6.16 7.2e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> 
> ### vcov.maxLik
> set.seed( 17 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -53.67 
     parameter initial gradient free
[1,]         1            46.33    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 1.863 
Function value: -37.76 
> printRounded( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] 0

$hessian
       [,1]
[1,] -28.79

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,] -0.2767
  [2,]  0.3953
  [3,]  0.4979
  [4,] -0.4840
  [5,] -0.4049
  [6,]  0.2567
  [7,] -0.4323
  [8,]  0.1893
  [9,]  0.2066
 [10,]  0.3285
 [11,]  0.1665
 [12,]  0.3936
 [13,]  0.5104
 [14,]  0.3261
 [15,]  0.2335
 [16,]  0.5046
 [17,]  0.4612
 [18,]  0.4608
 [19,] -1.0643
 [20,]  0.2375
 [21,] -0.0465
 [22,]  0.4734
 [23,] -0.4004
 [24,]  0.2348
 [25,]  0.2846
 [26,]  0.4029
 [27,] -0.2378
 [28,]  0.4411
 [29,]  0.4821
 [30,]  0.4955
 [31,] -0.3652
 [32,]  0.3878
 [33,] -0.4067
 [34,] -0.1809
 [35,]  0.4185
 [36,] -0.3304
 [37,] -0.2404
 [38,] -0.4153
 [39,]  0.4612
 [40,] -3.8930
 [41,]  0.0334
 [42,] -0.6293
 [43,]  0.4325
 [44,]  0.0365
 [45,]  0.2462
 [46,] -0.2268
 [47,]  0.5306
 [48,]  0.5160
 [49,] -0.6772
 [50,]  0.1526
 [51,]  0.2219
 [52,]  0.4363
 [53,] -1.5633
 [54,] -0.0502
 [55,]  0.3124
 [56,] -0.1465
 [57,] -0.3567
 [58,]  0.4810
 [59,]  0.3985
 [60,]  0.4233
 [61,]  0.3301
 [62,]  0.3526
 [63,] -0.3018
 [64,]  0.4708
 [65,]  0.2910
 [66,]  0.1197
 [67,]  0.5074
 [68,] -0.2495
 [69,] -0.0505
 [70,]  0.2800
 [71,]  0.5255
 [72,] -0.3740
 [73,]  0.0235
 [74,] -0.4133
 [75,] -0.3585
 [76,]  0.4766
 [77,]  0.2416
 [78,] -0.0411
 [79,] -1.3477
 [80,] -0.2567
 [81,]  0.3243
 [82,]  0.3452
 [83,] -0.9342
 [84,]  0.3324
 [85,] -0.8498
 [86,]  0.3158
 [87,]  0.4752
 [88,]  0.3726
 [89,] -0.0253
 [90,]  0.3766
 [91,] -2.1083
 [92,] -0.4496
 [93,]  0.1682
 [94,]  0.3510
 [95,]  0.5254
 [96,] -0.0662
 [97,]  0.3878
 [98,]  0.2535
 [99,] -0.7967
[100,]  0.1333

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> vcov(a)
        [,1]
[1,] 0.03473
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] 0

$hessian
      [,1]
[1,] -28.8

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
          [,1]
  [1,] -0.2767
  [2,]  0.3953
  [3,]  0.4979
  [4,] -0.4840
  [5,] -0.4049
  [6,]  0.2567
  [7,] -0.4323
  [8,]  0.1893
  [9,]  0.2066
 [10,]  0.3285
 [11,]  0.1665
 [12,]  0.3936
 [13,]  0.5104
 [14,]  0.3261
 [15,]  0.2335
 [16,]  0.5046
 [17,]  0.4612
 [18,]  0.4608
 [19,] -1.0643
 [20,]  0.2375
 [21,] -0.0465
 [22,]  0.4734
 [23,] -0.4004
 [24,]  0.2348
 [25,]  0.2846
 [26,]  0.4029
 [27,] -0.2378
 [28,]  0.4411
 [29,]  0.4821
 [30,]  0.4955
 [31,] -0.3652
 [32,]  0.3878
 [33,] -0.4067
 [34,] -0.1809
 [35,]  0.4185
 [36,] -0.3304
 [37,] -0.2404
 [38,] -0.4153
 [39,]  0.4612
 [40,] -3.8930
 [41,]  0.0334
 [42,] -0.6293
 [43,]  0.4325
 [44,]  0.0365
 [45,]  0.2462
 [46,] -0.2268
 [47,]  0.5306
 [48,]  0.5160
 [49,] -0.6772
 [50,]  0.1526
 [51,]  0.2219
 [52,]  0.4363
 [53,] -1.5633
 [54,] -0.0502
 [55,]  0.3124
 [56,] -0.1465
 [57,] -0.3567
 [58,]  0.4810
 [59,]  0.3985
 [60,]  0.4233
 [61,]  0.3301
 [62,]  0.3526
 [63,] -0.3018
 [64,]  0.4708
 [65,]  0.2910
 [66,]  0.1197
 [67,]  0.5074
 [68,] -0.2495
 [69,] -0.0505
 [70,]  0.2800
 [71,]  0.5255
 [72,] -0.3740
 [73,]  0.0235
 [74,] -0.4133
 [75,] -0.3585
 [76,]  0.4766
 [77,]  0.2416
 [78,] -0.0411
 [79,] -1.3477
 [80,] -0.2567
 [81,]  0.3243
 [82,]  0.3452
 [83,] -0.9342
 [84,]  0.3324
 [85,] -0.8498
 [86,]  0.3158
 [87,]  0.4752
 [88,]  0.3726
 [89,] -0.0253
 [90,]  0.3766
 [91,] -2.1083
 [92,] -0.4496
 [93,]  0.1682
 [94,]  0.3510
 [95,]  0.5254
 [96,] -0.0662
 [97,]  0.3878
 [98,]  0.2535
 [99,] -0.7967
[100,]  0.1333

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> vcov(a)
        [,1]
[1,] 0.03472
> print(stdEr(a))
[1] 0.1863
>                            # test single stdEr
> 
> proc.time()
   user  system elapsed 
  0.580   0.040   0.606 
