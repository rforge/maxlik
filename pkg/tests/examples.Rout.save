
R version 3.1.2 (2014-10-31) -- "Pumpkin Helmet"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library( maxLik )
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> options(digits=4)
> 
> printRounded <- function( x ) {
+    for( i in names( x ) ) {
+       cat ( "$", i, "\n", sep = "" )
+       if( is.numeric( x[[i]] ) ) {
+          print( round( x[[i]], 3 ) )
+       } else {
+          print( x[[i]] )
+       }
+       cat( "\n" )
+    }
+    cat( "attr(,\"class\")\n" )
+    print( class( x ) )
+ }
> 
> 
> ### activePar
> # a simple two-dimensional exponential hat
> f <- function(a) exp(-(a[1]-2)^2 - (a[2]-4)^2)
> #
> # maximize wrt. both parameters 
> free <- maxNR(f, start=1:2)
> printRounded( free )
$maximum
[1] 1

$estimate
[1] 2 4

$gradient
[1] 0 0

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(free)  # results should be close to (2,4)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate gradient
[1,]        2 0.00e+00
[2,]        4 1.11e-10
--------------------------------------------
> activePar(free)
[1] TRUE TRUE
> # allow only the second parameter to vary
> cons <- maxNR(f, start=1:2, activePar=c(FALSE,TRUE))
> printRounded( cons )
$maximum
[1] 0.368

$estimate
[1] 1 4

$gradient
[1] NA  0

$hessian
     [,1]   [,2]
[1,]   NA     NA
[2,]   NA -0.736

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1]  TRUE FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(cons) # result should be around (1,4)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
     estimate  gradient
[1,]        1        NA
[2,]        4 5.944e-07
--------------------------------------------
> activePar(cons)
[1] FALSE  TRUE
> # specify fixed par in different ways
> cons2 <- maxNR(f, start=1:2, fixed=1)
> all.equal( cons, cons2 )
[1] TRUE
> cons3 <- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
> all.equal( cons, cons3 )
[1] TRUE
> cons4 <- maxNR(f, start=c(a=1, b=2), fixed="a")
> print(summary(cons4))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3679 
Estimates:
  estimate  gradient
a        1        NA
b        4 5.944e-07
--------------------------------------------
> all.equal( cons, cons4 )
[1] "Component \"estimate\": names for current but not for target"                            
[2] "Component \"gradient\": names for current but not for target"                            
[3] "Component \"hessian\": Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component \"fixed\": names for current but not for target"                               
> 
> ### compareDerivatives
> set.seed( 2 )
> ## A simple example with sin(x)' = cos(x)
> f <- sin
> compareDerivatives(f, cos, t0=1)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.8415
Dim of analytic gradient: 1 1 
       numeric          : 1 1 
      
param  theta 0 analytic numeric  rel.diff
  [1,]       1   0.5403  0.5403 -5.13e-11
Max relative difference: 5.13e-11 
-------- END of compare derivatives -------- 
> ##
> ## Example of log-likelihood of normal density.  Two-parameter
> ## function.
> x <- rnorm(100, 1, 2) # generate rnorm x
> l <- function(b) sum(log(dnorm((x-b[1])/b[2])/b[2]))
>               # b[1] - mu, b[2] - sigma
> gradl <- function(b) {
+    c(sum(x - b[1])/b[2]^2,
+    sum((x - b[1])^2/b[2]^3 - 1/b[2]))
+ }
> compareDerivatives(l, gradl, t0=c(1,2))
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] -227.9
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
       [,1]  [,2]
[1,] -1.535 16.68
numeric gradient
       [,1]  [,2]
[1,] -1.535 16.68
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -1.989e-09 -2.089e-10
Max relative difference: 1.989e-09 
-------- END of compare derivatives -------- 
> 
> 
> ### hessian
> set.seed( 3 )
> # log-likelihood for normal density
> # a[1] - mean
> # a[2] - standard deviation
> ll <- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
> x <- rnorm(1000) # sample from standard normal
> ml <- maxLik(ll, start=c(1,1))
> # ignore eventual warnings "NaNs produced in: log(x)"
> printRounded( ml )
$maximum
[1] -497.6

$estimate
[1] 0.006 0.998

$gradient
[1] 0 0

$hessian
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (2 free parameter(s))
Estimate(s): 0.006397 0.9976 
> summary(ml) # result should be close to c(0,1)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
2  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.0064     0.0316     0.2    0.84    
[2,]   0.9976     0.0223    44.7  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> hessian(ml) # How the Hessian looks like
      [,1]  [,2]
[1,] -1005     0
[2,]     0 -2010
> sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
        [,1]    [,2]
[1,] 0.03155 0.00000
[2,] 0.00000 0.02231
> print(stdEr(ml))
[1] 0.03155 0.02231
>                            # test vector of stdEr-s
> #
> # Now run the same example while fixing a[2] = 1
> mlf <- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
> printRounded( mlf )
$maximum
[1] -497.6

$estimate
[1] 0.006 1.000

$gradient
[1]  0 NA

$hessian
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE  TRUE

$iterations
[1] 3

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( mlf )
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 (1 free parameter(s))
Estimate(s): 0.006397 1 
> summary(mlf) # first parameter close to 0, the second exactly 1.0
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.6 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)
[1,]   0.0064     0.0316     0.2    0.84
[2,]   1.0000     0.0000      NA      NA
--------------------------------------------
> hessian(mlf)
      [,1] [,2]
[1,] -1000   NA
[2,]    NA   NA
> # now invert only the free parameter part of the Hessian
> sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
        [,1]
[1,] 0.03162
> # gives the standard deviation for the mean
> print(stdEr(mlf))
[1] 0.03162 0.00000
>                            # test standard errors with fixed par
> 
> 
> ### maxBFGS
> set.seed( 5 )
> # Maximum Likelihood estimation of the parameter of Poissonian distribution
> n <- rpois(100, 3)
> loglik <- function(l) n*log(l) - l - lfactorial(n)
> # we use numeric gradient
> a <- maxBFGS(loglik, start=1)
> print( a[-3] )
$maximum
[1] -199.2

$estimate
[1] 3.19

$hessian
       [,1]
[1,] -31.29

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE

$iterations
function 
      29 

$type
[1] "BFGS maximisation"

$constraints
NULL

$gradientObs
           [,1]
  [1,] -0.37304
  [2,]  0.25392
  [3,]  0.88088
  [4,] -0.37304
  [5,] -0.68652
  [6,]  0.25392
  [7,] -0.05956
  [8,]  0.25392
  [9,]  0.88088
 [10,] -0.68652
 [11,] -0.37304
 [12,] -0.05956
 [13,] -0.37304
 [14,] -0.05956
 [15,] -0.37304
 [16,] -0.37304
 [17,] -0.37304
 [18,]  0.56740
 [19,] -0.05956
 [20,]  0.56740
 [21,]  0.56740
 [22,]  0.25392
 [23,] -0.37304
 [24,] -0.37304
 [25,] -0.68652
 [26,] -0.05956
 [27,] -0.05956
 [28,]  0.88088
 [29,] -0.68652
 [30,]  0.88088
 [31,] -0.05956
 [32,] -0.68652
 [33,] -0.37304
 [34,] -1.00000
 [35,] -1.00000
 [36,] -0.05956
 [37,] -0.05956
 [38,] -0.05956
 [39,] -0.37304
 [40,] -0.37304
 [41,]  0.56740
 [42,] -0.37304
 [43,]  0.56740
 [44,] -0.05956
 [45,]  0.88088
 [46,] -0.05956
 [47,]  0.25392
 [48,] -0.68652
 [49,]  0.25392
 [50,] -0.05956
 [51,] -0.37304
 [52,] -0.05956
 [53,]  0.88088
 [54,]  1.19436
 [55,]  0.88088
 [56,] -0.37304
 [57,] -0.37304
 [58,] -0.37304
 [59,] -0.68652
 [60,] -0.68652
 [61,] -0.05956
 [62,] -0.68652
 [63,]  0.56740
 [64,]  1.50784
 [65,]  1.19436
 [66,]  0.56740
 [67,]  0.56740
 [68,] -0.68652
 [69,]  0.88088
 [70,]  0.56740
 [71,]  0.56740
 [72,]  0.25392
 [73,] -0.68652
 [74,]  0.56740
 [75,] -1.00000
 [76,]  0.88088
 [77,] -1.00000
 [78,]  0.25392
 [79,] -0.05956
 [80,] -0.37304
 [81,]  0.88088
 [82,]  0.56740
 [83,] -0.37304
 [84,] -0.68652
 [85,] -0.05956
 [86,] -0.68652
 [87,]  1.19436
 [88,]  0.25392
 [89,] -0.37304
 [90,] -0.05956
 [91,]  0.25392
 [92,] -0.37304
 [93,] -0.68652
 [94,] -0.37304
 [95,] -0.37304
 [96,]  0.56740
 [97,] -0.37304
 [98,] -0.05956
 [99,] -0.05956
[100,] -0.05956

> class( a )
[1] "maxim"
> summary( a )
--------------------------------------------
BFGS maximisation 
Number of iterations: 29 
Return code: 0 
successful convergence  
Function value: -199.2 
Estimates:
     estimate  gradient
[1,]     3.19 1.583e-05
--------------------------------------------
> # you would probably prefer mean(n) instead of that ;-)
> # Note also that maxLik is better suited for Maximum Likelihood
> 
> 
> ### logLik.maxLik
> set.seed( 4 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -25.05

$estimate
[1] 2.116

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.34

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,]  0.387
  [2,] -1.679
  [3,]  0.039
  [4,]  0.071
  [5,]  0.159
  [6,]  0.105
  [7,]  0.248
  [8,]  0.447
  [9,]  0.218
 [10,]  0.054
 [11,] -0.868
 [12,]  0.329
 [13,]  0.270
 [14,]  0.258
 [15,]  0.303
 [16,] -0.052
 [17,]  0.443
 [18,]  0.406
 [19,] -0.447
 [20,] -0.033
 [21,]  0.351
 [22,] -0.151
 [23,] -2.297
 [24,]  0.389
 [25,] -0.444
 [26,]  0.443
 [27,]  0.277
 [28,] -0.151
 [29,]  0.227
 [30,]  0.192
 [31,] -0.216
 [32,] -0.427
 [33,] -0.416
 [34,]  0.278
 [35,] -0.637
 [36,]  0.395
 [37,]  0.344
 [38,] -0.620
 [39,]  0.458
 [40,]  0.167
 [41,]  0.354
 [42,] -0.065
 [43,]  0.148
 [44,]  0.283
 [45,] -0.015
 [46,]  0.080
 [47,]  0.274
 [48,]  0.452
 [49,] -1.145
 [50,]  0.405
 [51,] -0.228
 [52,]  0.433
 [53,]  0.081
 [54,] -0.081
 [55,] -0.740
 [56,]  0.207
 [57,]  0.114
 [58,]  0.119
 [59,]  0.343
 [60,]  0.093
 [61,]  0.440
 [62,] -0.073
 [63,] -0.501
 [64,]  0.075
 [65,] -0.172
 [66,]  0.045
 [67,] -0.026
 [68,]  0.182
 [69,]  0.448
 [70,] -0.160
 [71,]  0.440
 [72,]  0.248
 [73,]  0.403
 [74,] -0.191
 [75,] -0.473
 [76,] -0.065
 [77,] -0.455
 [78,]  0.160
 [79,]  0.377
 [80,]  0.122
 [81,]  0.302
 [82,] -0.001
 [83,]  0.414
 [84,]  0.401
 [85,]  0.349
 [86,] -0.997
 [87,]  0.379
 [88,]  0.385
 [89,] -0.317
 [90,]  0.193
 [91,]  0.329
 [92,] -0.042
 [93,]  0.061
 [94,] -0.645
 [95,] -0.633
 [96,] -0.356
 [97,] -0.324
 [98,]  0.221
 [99,] -0.833
[100,]  0.361

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> ## print log likelihood value
> logLik( a )
[1] -25.05
> ## compare with log likelihood value of summary object
> all.equal( logLik( a ), logLik( summary( a ) ) )
[1] TRUE
> 
> 
> ### maxBHHH
> set.seed( 6 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxBHHH(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -45.5 
     parameter initial gradient free
[1,]         1             54.5    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
successive function values within tolerance limit 
6  iterations
estimate: 2.198 
Function value: -21.25 
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> ## Estimate with analytic gradient
> a <- maxBHHH(loglik, gradlik, start=1)
> print( a )
$maximum
[1] -21.25

$estimate
[1] 2.198

$gradient
[1] -4.775e-05

$hessian
       [,1]
[1,] -18.42
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
           [,1]
  [1,]  0.34872
  [2,]  0.36337
  [3,]  0.14750
  [4,]  0.27835
  [5,] -0.60055
  [6,]  0.31141
  [7,]  0.42198
  [8,]  0.18505
  [9,]  0.09662
 [10,]  0.43709
 [11,]  0.27713
 [12,] -0.32707
 [13,]  0.25446
 [14,]  0.41365
 [15,] -0.34761
 [16,] -0.10404
 [17,]  0.35988
 [18,]  0.43321
 [19,] -0.24284
 [20,]  0.40754
 [21,]  0.43446
 [22,]  0.21306
 [23,] -0.72492
 [24,]  0.16847
 [25,] -0.73113
 [26,]  0.41303
 [27,]  0.13127
 [28,]  0.30142
 [29,]  0.03316
 [30,] -0.32514
 [31,]  0.26619
 [32,]  0.33719
 [33,] -0.63494
 [34,]  0.42639
 [35,]  0.41133
 [36,]  0.21917
 [37,] -0.23050
 [38,]  0.42825
 [39,]  0.43629
 [40,] -0.49030
 [41,] -0.86638
 [42,] -0.05709
 [43,]  0.17051
 [44,] -0.06489
 [45,] -0.04142
 [46,]  0.21592
 [47,] -0.27990
 [48,] -0.04167
 [49,]  0.44931
 [50,]  0.28868
 [51,]  0.38041
 [52,] -0.29423
 [53,] -0.12650
 [54,] -0.52837
 [55,]  0.05775
 [56,]  0.39261
 [57,]  0.41130
 [58,]  0.21081
 [59,]  0.43310
 [60,] -0.11065
 [61,] -1.08886
 [62,]  0.28892
 [63,]  0.41071
 [64,] -0.57920
 [65,]  0.37020
 [66,] -0.10011
 [67,] -0.31689
 [68,]  0.31029
 [69,] -1.05872
 [70,]  0.17639
 [71,]  0.37379
 [72,]  0.02796
 [73,] -0.46422
 [74,] -0.65735
 [75,] -0.11963
 [76,] -0.08873
 [77,] -0.35161
 [78,]  0.09842
 [79,] -0.14749
 [80,]  0.36913
 [81,] -0.23146
 [82,]  0.18956
 [83,]  0.18225
 [84,]  0.12718
 [85,]  0.44356
 [86,]  0.28875
 [87,]  0.38631
 [88,] -0.96036
 [89,]  0.45398
 [90,]  0.27526
 [91,] -0.13580
 [92,] -0.19583
 [93,] -0.24698
 [94,] -0.81480
 [95,]  0.17887
 [96,] -1.18545
 [97,]  0.41696
 [98,]  0.38062
 [99,] -1.16810
[100,] -0.63346

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25 
Estimates:
     estimate   gradient
[1,]    2.198 -4.775e-05
--------------------------------------------
> 
> 
> ### maxLik
> set.seed( 7 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -47.72 
     parameter initial gradient free
[1,]         1            52.28    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.095 
Function value: -26.03 
> printRounded( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.76

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,]  0.453
  [2,] -0.334
  [3,] -0.379
  [4,]  0.071
  [5,]  0.204
  [6,] -0.833
  [7,]  0.101
  [8,] -1.659
  [9,]  0.375
 [10,]  0.454
 [11,]  0.005
 [12,]  0.103
 [13,] -0.459
 [14,] -0.456
 [15,] -0.128
 [16,]  0.305
 [17,]  0.139
 [18,] -0.002
 [19,]  0.218
 [20,]  0.020
 [21,] -0.723
 [22,]  0.161
 [23,]  0.279
 [24,]  0.257
 [25,]  0.203
 [26,]  0.349
 [27,]  0.254
 [28,] -0.143
 [29,] -0.367
 [30,] -0.465
 [31,] -0.152
 [32,]  0.083
 [33,] -0.484
 [34,]  0.473
 [35,]  0.213
 [36,]  0.284
 [37,]  0.433
 [38,]  0.318
 [39,]  0.014
 [40,]  0.031
 [41,]  0.137
 [42,]  0.356
 [43,] -0.254
 [44,]  0.410
 [45,] -0.021
 [46,]  0.173
 [47,] -0.243
 [48,] -0.821
 [49,]  0.215
 [50,] -0.065
 [51,] -0.203
 [52,] -0.057
 [53,] -1.640
 [54,]  0.414
 [55,]  0.405
 [56,]  0.437
 [57,] -0.079
 [58,]  0.153
 [59,] -0.460
 [60,] -0.162
 [61,] -0.003
 [62,] -0.057
 [63,]  0.462
 [64,] -0.531
 [65,]  0.292
 [66,] -0.162
 [67,] -1.126
 [68,]  0.348
 [69,]  0.411
 [70,]  0.188
 [71,] -0.452
 [72,]  0.145
 [73,]  0.447
 [74,]  0.054
 [75,]  0.271
 [76,] -0.012
 [77,]  0.218
 [78,] -0.357
 [79,]  0.352
 [80,] -0.832
 [81,]  0.198
 [82,]  0.472
 [83,]  0.249
 [84,]  0.168
 [85,]  0.219
 [86,]  0.172
 [87,]  0.310
 [88,]  0.464
 [89,] -0.428
 [90,]  0.414
 [91,]  0.093
 [92,]  0.258
 [93,] -0.488
 [94,]  0.460
 [95,] -0.802
 [96,]  0.412
 [97,]  0.054
 [98,]  0.179
 [99,] -0.136
[100,]  0.166

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -26.03

$estimate
[1] 2.095

$gradient
[1] 0

$hessian
       [,1]
[1,] -22.78

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,]  0.453
  [2,] -0.334
  [3,] -0.379
  [4,]  0.071
  [5,]  0.204
  [6,] -0.833
  [7,]  0.101
  [8,] -1.659
  [9,]  0.375
 [10,]  0.454
 [11,]  0.005
 [12,]  0.103
 [13,] -0.459
 [14,] -0.456
 [15,] -0.128
 [16,]  0.305
 [17,]  0.139
 [18,] -0.002
 [19,]  0.218
 [20,]  0.020
 [21,] -0.723
 [22,]  0.161
 [23,]  0.279
 [24,]  0.257
 [25,]  0.203
 [26,]  0.349
 [27,]  0.254
 [28,] -0.143
 [29,] -0.367
 [30,] -0.465
 [31,] -0.152
 [32,]  0.083
 [33,] -0.484
 [34,]  0.473
 [35,]  0.213
 [36,]  0.284
 [37,]  0.433
 [38,]  0.318
 [39,]  0.014
 [40,]  0.031
 [41,]  0.137
 [42,]  0.356
 [43,] -0.254
 [44,]  0.410
 [45,] -0.021
 [46,]  0.173
 [47,] -0.243
 [48,] -0.821
 [49,]  0.215
 [50,] -0.065
 [51,] -0.203
 [52,] -0.057
 [53,] -1.640
 [54,]  0.414
 [55,]  0.405
 [56,]  0.437
 [57,] -0.079
 [58,]  0.153
 [59,] -0.460
 [60,] -0.162
 [61,] -0.003
 [62,] -0.057
 [63,]  0.462
 [64,] -0.531
 [65,]  0.292
 [66,] -0.162
 [67,] -1.126
 [68,]  0.348
 [69,]  0.411
 [70,]  0.188
 [71,] -0.452
 [72,]  0.145
 [73,]  0.447
 [74,]  0.054
 [75,]  0.271
 [76,] -0.012
 [77,]  0.218
 [78,] -0.357
 [79,]  0.352
 [80,] -0.832
 [81,]  0.198
 [82,]  0.472
 [83,]  0.249
 [84,]  0.168
 [85,]  0.219
 [86,]  0.172
 [87,]  0.310
 [88,]  0.464
 [89,] -0.428
 [90,]  0.414
 [91,]  0.093
 [92,]  0.258
 [93,] -0.488
 [94,]  0.460
 [95,] -0.802
 [96,]  0.412
 [97,]  0.054
 [98,]  0.179
 [99,] -0.136
[100,]  0.166

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 (1 free parameter(s))
Estimate(s): 2.095 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.03 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]     2.10       0.21      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### maxNR
> set.seed( 8 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglikSum <- function(theta) sum(log(theta) - theta*t)
> ## Note the log-likelihood and gradient are summed over observations
> gradlikSum <- function(theta) sum(1/theta - t)
> ## Estimate with numeric gradient and Hessian
> a <- maxNR(loglikSum, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -46.49 
     parameter initial gradient free
[1,]         1            53.51    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.151 
Function value: -23.41 
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] -2.416e-07

$hessian
       [,1]
[1,] -21.62

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate   gradient
[1,]    2.151 -2.416e-07
--------------------------------------------
> ## You would probably prefer 1/mean(t) instead ;-)
> ## Estimate with analytic gradient and Hessian
> a <- maxNR(loglikSum, gradlikSum, hesslik, start=1)
> print( a )
$maximum
[1] -23.41

$estimate
[1] 2.151

$gradient
[1] 9.493e-08

$hessian
       [,1]
[1,] -21.61

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.41 
Estimates:
     estimate  gradient
[1,]    2.151 9.493e-08
--------------------------------------------
> 
> 
> ### maximType
> ## maximise two-dimensional exponential hat.  Maximum is at c(2,1):
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m <- maxNR(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] -1.11e-10  1.11e-10

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(m)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate  gradient
[1,]        2 -1.11e-10
[2,]        1  1.11e-10
--------------------------------------------
> maximType(m)
[1] "Newton-Raphson maximisation"
> ## Now use BFGS maximisation.
> m <- maxBFGS(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.088e-08 5.329e-09

$hessian
     [,1] [,2]
[1,]   -2    0
[2,]    0   -2

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
function 
      26 

$type
[1] "BFGS maximisation"

$constraints
NULL

attr(,"class")
[1] "maxim"
> summary(m)
--------------------------------------------
BFGS maximisation 
Number of iterations: 26 
Return code: 0 
successful convergence  
Function value: 1 
Estimates:
     estimate  gradient
[1,]        2 1.088e-08
[2,]        1 5.329e-09
--------------------------------------------
> maximType(m)
[1] "BFGS maximisation"
> 
> ### Test maxNR with 0 iterations.  Should perform no iterations
> ### Request by Yves Croissant
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m0 <- maxNR(f, start=c(1.1, 2.1), iterlim=0)
> summary(m0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 0 
Return code: 4 
Iteration limit exceeded. 
Function value: 0.1327 
Estimates:
     estimate gradient
[1,]      1.1   0.2388
[2,]      2.1  -0.2918
--------------------------------------------
> 
> ### nObs
> set.seed( 10 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> nObs(m)
[1] 100
> 
> 
> ### nParam
> set.seed( 11 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> summary(m)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3436 -0.5338 -0.0291  0.5501  2.6934 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.242      0.287    11.3   <2e-16 ***
x1             3.974      0.395    10.1   <2e-16 ***
x2             4.783      0.367    13.0   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.99 on 97 degrees of freedom
Multiple R-squared:  0.702,	Adjusted R-squared:  0.696 
F-statistic:  114 on 2 and 97 DF,  p-value: <2e-16

> nParam(m) # you get 3
[1] 3
> 
> 
> ### numericGradient
> # A simple example with Gaussian bell
> f0 <- function(t0) exp(-t0[1]^2 - t0[2]^2)
> numericGradient(f0, c(1,2))
         [,1]     [,2]
[1,] -0.01348 -0.02695
> numericHessian(f0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01349 0.05390
[2,] 0.05390 0.09433
> # An example with the analytic gradient
> gradf0 <- function(t0) -2*t0*f0(t0)
> numericHessian(f0, gradf0, t0=c(1,2))
        [,1]    [,2]
[1,] 0.01348 0.05390
[2,] 0.05390 0.09433
> # The results should be similar as in the previous case
> # The central numeric derivatives have usually quite a high precision
> compareDerivatives(f0, gradf0, t0=1:2)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value:
[1] 0.006738
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
numeric gradient
         [,1]     [,2]
[1,] -0.01348 -0.02695
(anal-num)/(0.5*(abs(anal)+abs(num)))
           [,1]       [,2]
[1,] -2.764e-10 -5.108e-11
Max relative difference: 2.764e-10 
-------- END of compare derivatives -------- 
> # The differenc is around 1e-10
> 
> 
> ### returnCode
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should be success (1 or 2)
[1] 1
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should give a failure (4)
[1] 4
> 
> 
> ### returnMessage
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.632e-10

$gradient
[1] -6.661e-10

$hessian
     [,1]
[1,]   -2

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should be success (1 or 2)
[1] "gradient close to zero"
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.277

$estimate
[1] 10685

$gradient
[1] 9.359e-05

$hessian
         [,1]
[1,] 0.001776

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should give a failure (4)
[1] "Iteration limit exceeded."
> 
> 
> ### summary.maxLik
> set.seed( 15 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -41.56 
     parameter initial gradient free
[1,]         1            58.44    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.406 
Function value: -12.2 
> printRounded( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] 0

$hessian
       [,1]
[1,] -17.27

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,]  0.313
  [2,] -0.558
  [3,]  0.288
  [4,]  0.328
  [5,]  0.084
  [6,] -0.961
  [7,]  0.269
  [8,]  0.407
  [9,]  0.209
 [10,]  0.261
 [11,]  0.367
 [12,]  0.112
 [13,]  0.122
 [14,] -0.146
 [15,] -1.167
 [16,] -0.676
 [17,] -0.020
 [18,]  0.028
 [19,] -0.729
 [20,]  0.257
 [21,]  0.051
 [22,] -0.119
 [23,] -0.045
 [24,]  0.072
 [25,] -1.860
 [26,]  0.249
 [27,]  0.201
 [28,]  0.147
 [29,]  0.330
 [30,]  0.288
 [31,]  0.139
 [32,]  0.098
 [33,]  0.022
 [34,] -0.075
 [35,]  0.242
 [36,]  0.141
 [37,] -0.093
 [38,]  0.083
 [39,]  0.180
 [40,] -0.241
 [41,]  0.235
 [42,]  0.325
 [43,] -0.310
 [44,]  0.274
 [45,]  0.151
 [46,]  0.359
 [47,] -0.166
 [48,]  0.117
 [49,]  0.411
 [50,] -0.856
 [51,]  0.369
 [52,]  0.012
 [53,]  0.091
 [54,] -0.419
 [55,]  0.163
 [56,]  0.381
 [57,] -0.241
 [58,]  0.393
 [59,]  0.187
 [60,] -0.070
 [61,] -0.526
 [62,]  0.367
 [63,]  0.217
 [64,]  0.219
 [65,]  0.258
 [66,]  0.398
 [67,]  0.298
 [68,] -0.031
 [69,] -0.100
 [70,] -0.657
 [71,]  0.052
 [72,] -0.671
 [73,]  0.324
 [74,] -0.752
 [75,]  0.209
 [76,] -1.050
 [77,]  0.394
 [78,] -0.131
 [79,] -1.349
 [80,] -0.050
 [81,]  0.237
 [82,] -0.017
 [83,]  0.154
 [84,]  0.275
 [85,]  0.158
 [86,] -0.430
 [87,]  0.215
 [88,]  0.414
 [89,]  0.218
 [90,]  0.221
 [91,] -0.072
 [92,]  0.270
 [93,] -0.070
 [94,]  0.308
 [95,] -0.178
 [96,]  0.153
 [97,]  0.185
 [98,]  0.075
 [99,]  0.365
[100,]  0.094

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -12.2

$estimate
[1] 2.406

$gradient
[1] 0

$hessian
       [,1]
[1,] -17.27

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,]  0.313
  [2,] -0.558
  [3,]  0.288
  [4,]  0.328
  [5,]  0.084
  [6,] -0.961
  [7,]  0.269
  [8,]  0.407
  [9,]  0.209
 [10,]  0.261
 [11,]  0.367
 [12,]  0.112
 [13,]  0.122
 [14,] -0.146
 [15,] -1.167
 [16,] -0.676
 [17,] -0.020
 [18,]  0.028
 [19,] -0.729
 [20,]  0.257
 [21,]  0.051
 [22,] -0.119
 [23,] -0.045
 [24,]  0.072
 [25,] -1.860
 [26,]  0.249
 [27,]  0.201
 [28,]  0.147
 [29,]  0.330
 [30,]  0.288
 [31,]  0.139
 [32,]  0.098
 [33,]  0.022
 [34,] -0.075
 [35,]  0.242
 [36,]  0.141
 [37,] -0.093
 [38,]  0.083
 [39,]  0.180
 [40,] -0.241
 [41,]  0.235
 [42,]  0.325
 [43,] -0.310
 [44,]  0.274
 [45,]  0.151
 [46,]  0.359
 [47,] -0.166
 [48,]  0.117
 [49,]  0.411
 [50,] -0.856
 [51,]  0.369
 [52,]  0.012
 [53,]  0.091
 [54,] -0.419
 [55,]  0.163
 [56,]  0.381
 [57,] -0.241
 [58,]  0.393
 [59,]  0.187
 [60,] -0.070
 [61,] -0.526
 [62,]  0.367
 [63,]  0.217
 [64,]  0.219
 [65,]  0.258
 [66,]  0.398
 [67,]  0.298
 [68,] -0.031
 [69,] -0.100
 [70,] -0.657
 [71,]  0.052
 [72,] -0.671
 [73,]  0.324
 [74,] -0.752
 [75,]  0.209
 [76,] -1.050
 [77,]  0.394
 [78,] -0.131
 [79,] -1.349
 [80,] -0.050
 [81,]  0.237
 [82,] -0.017
 [83,]  0.154
 [84,]  0.275
 [85,]  0.158
 [86,] -0.430
 [87,]  0.215
 [88,]  0.414
 [89,]  0.218
 [90,]  0.221
 [91,] -0.072
 [92,]  0.270
 [93,] -0.070
 [94,]  0.308
 [95,] -0.178
 [96,]  0.153
 [97,]  0.185
 [98,]  0.075
 [99,]  0.365
[100,]  0.094

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 (1 free parameter(s))
Estimate(s): 2.406 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.2 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.406      0.241      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ### summary.maxim and for "gradient"/"hessian" attributes
> ### Test for infinity
> ## maximize a 2D quadratic function:
> f <- function(b) {
+   x <- b[1]; y <- b[2];
+     val <- (x - 2)^2 + (y - 3)^2
+     attr(val, "gradient") <- c(2*x - 4, 2*y - 6)
+     attr(val, "hessian") <- matrix(c(2, 0, 0, 2), 2, 2)
+     val
+ }
> ## Use c(0,0) as initial value.  
> result1 <- maxNR( f, start = c(0,0) )
> print( result1 )
$maximum
[1] Inf

$estimate
[1] -7.035e+155 -1.055e+156

$gradient
[1] -1.407e+156 -2.110e+156

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 25

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result1 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 25 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,] -7.035e+155 -1.407e+156
[2,] -1.055e+156 -2.110e+156
--------------------------------------------
> ## Now use c(1000000, -777777) as initial value and ask for hessian
> result2 <- maxNR( f, start = c( 1000000, -777777))
> print( result2 )
$maximum
[1] Inf

$estimate
[1]  2.110e+155 -1.641e+155

$gradient
[1]  4.221e+155 -3.283e+155

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 24

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result2 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 24 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
        estimate    gradient
[1,]  2.110e+155  4.221e+155
[2,] -1.641e+155 -3.283e+155
--------------------------------------------
> 
> 
> ### Test for "gradient"/"hessian" attributes.  A case which converges.
> hub <- function(x) {
+    v <- exp(-sum(x*x))
+    val <- v
+    attr(val, "gradient") <- -2*x*v
+    attr(val, "hessian") <- 4*(x %*% t(x))*v - diag(2*c(v, v))
+    val
+ }
> summary(a <- maxNR(hub, start=c(2,1)))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
       estimate  gradient
[1,] -7.448e-18 1.490e-17
[2,] -3.724e-18 7.448e-18
--------------------------------------------
> ## Now test "gradient" attribute for BHHH/3-parameter probit
> N <- 1000
> loglikProbit <- function( beta) {
+    xb <- x %*% beta
+    loglik <- ifelse(y == 0,
+                     pnorm( xb, log=TRUE, lower.tail=FALSE),
+                     pnorm( xb, log.p=TRUE))
+    grad <- ifelse(y == 0,
+                   -dnorm(xb)/pnorm(xb, lower.tail=FALSE),
+                   dnorm(xb)/pnorm(xb))
+    grad <- grad*x
+    attr(loglik, "gradient") <- grad
+    loglik
+ }
> x <- runif(N)
> x <- cbind(x, x - runif(N), x - runif(N))
> y <- x[,1] + 2*x[,2] - x[,3] + rnorm(N) > 0
> summary(maxLik(loglikProbit, start=c(0,0,0), method="bhhh"))
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -508.4 
3  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]   0.8578     0.0904    9.49 < 2e-16 ***
[2,]   1.9389     0.1514   12.81 < 2e-16 ***
[3,]  -0.8253     0.1339   -6.16 7.2e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> 
> ### vcov.maxLik
> set.seed( 17 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -53.67 
     parameter initial gradient free
[1,]         1            46.33    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 1.863 
Function value: -37.76 
> printRounded( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] 0

$hessian
       [,1]
[1,] -28.79

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,] -0.277
  [2,]  0.395
  [3,]  0.498
  [4,] -0.484
  [5,] -0.405
  [6,]  0.257
  [7,] -0.432
  [8,]  0.189
  [9,]  0.207
 [10,]  0.328
 [11,]  0.166
 [12,]  0.394
 [13,]  0.510
 [14,]  0.326
 [15,]  0.234
 [16,]  0.505
 [17,]  0.461
 [18,]  0.461
 [19,] -1.064
 [20,]  0.238
 [21,] -0.046
 [22,]  0.473
 [23,] -0.400
 [24,]  0.235
 [25,]  0.285
 [26,]  0.403
 [27,] -0.238
 [28,]  0.441
 [29,]  0.482
 [30,]  0.495
 [31,] -0.365
 [32,]  0.388
 [33,] -0.407
 [34,] -0.181
 [35,]  0.419
 [36,] -0.330
 [37,] -0.240
 [38,] -0.415
 [39,]  0.461
 [40,] -3.893
 [41,]  0.033
 [42,] -0.629
 [43,]  0.432
 [44,]  0.036
 [45,]  0.246
 [46,] -0.227
 [47,]  0.531
 [48,]  0.516
 [49,] -0.677
 [50,]  0.153
 [51,]  0.222
 [52,]  0.436
 [53,] -1.563
 [54,] -0.050
 [55,]  0.312
 [56,] -0.146
 [57,] -0.357
 [58,]  0.481
 [59,]  0.399
 [60,]  0.423
 [61,]  0.330
 [62,]  0.353
 [63,] -0.302
 [64,]  0.471
 [65,]  0.291
 [66,]  0.120
 [67,]  0.507
 [68,] -0.250
 [69,] -0.050
 [70,]  0.280
 [71,]  0.526
 [72,] -0.374
 [73,]  0.024
 [74,] -0.413
 [75,] -0.359
 [76,]  0.477
 [77,]  0.242
 [78,] -0.041
 [79,] -1.348
 [80,] -0.257
 [81,]  0.324
 [82,]  0.345
 [83,] -0.934
 [84,]  0.332
 [85,] -0.850
 [86,]  0.316
 [87,]  0.475
 [88,]  0.373
 [89,] -0.025
 [90,]  0.377
 [91,] -2.108
 [92,] -0.450
 [93,]  0.168
 [94,]  0.351
 [95,]  0.525
 [96,] -0.066
 [97,]  0.388
 [98,]  0.253
 [99,] -0.797
[100,]  0.133

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> round( vcov( a ), 3 )
      [,1]
[1,] 0.035
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> printRounded( a )
$maximum
[1] -37.76

$estimate
[1] 1.863

$gradient
[1] 0

$hessian
      [,1]
[1,] -28.8

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
         [,1]
  [1,] -0.277
  [2,]  0.395
  [3,]  0.498
  [4,] -0.484
  [5,] -0.405
  [6,]  0.257
  [7,] -0.432
  [8,]  0.189
  [9,]  0.207
 [10,]  0.328
 [11,]  0.166
 [12,]  0.394
 [13,]  0.510
 [14,]  0.326
 [15,]  0.234
 [16,]  0.505
 [17,]  0.461
 [18,]  0.461
 [19,] -1.064
 [20,]  0.238
 [21,] -0.046
 [22,]  0.473
 [23,] -0.400
 [24,]  0.235
 [25,]  0.285
 [26,]  0.403
 [27,] -0.238
 [28,]  0.441
 [29,]  0.482
 [30,]  0.495
 [31,] -0.365
 [32,]  0.388
 [33,] -0.407
 [34,] -0.181
 [35,]  0.419
 [36,] -0.330
 [37,] -0.240
 [38,] -0.415
 [39,]  0.461
 [40,] -3.893
 [41,]  0.033
 [42,] -0.629
 [43,]  0.432
 [44,]  0.036
 [45,]  0.246
 [46,] -0.227
 [47,]  0.531
 [48,]  0.516
 [49,] -0.677
 [50,]  0.153
 [51,]  0.222
 [52,]  0.436
 [53,] -1.563
 [54,] -0.050
 [55,]  0.312
 [56,] -0.146
 [57,] -0.357
 [58,]  0.481
 [59,]  0.399
 [60,]  0.423
 [61,]  0.330
 [62,]  0.353
 [63,] -0.302
 [64,]  0.471
 [65,]  0.291
 [66,]  0.120
 [67,]  0.507
 [68,] -0.250
 [69,] -0.050
 [70,]  0.280
 [71,]  0.526
 [72,] -0.374
 [73,]  0.024
 [74,] -0.413
 [75,] -0.359
 [76,]  0.477
 [77,]  0.242
 [78,] -0.041
 [79,] -1.348
 [80,] -0.257
 [81,]  0.324
 [82,]  0.345
 [83,] -0.934
 [84,]  0.332
 [85,] -0.850
 [86,]  0.316
 [87,]  0.475
 [88,]  0.373
 [89,] -0.025
 [90,]  0.377
 [91,] -2.108
 [92,] -0.450
 [93,]  0.168
 [94,]  0.351
 [95,]  0.525
 [96,] -0.066
 [97,]  0.388
 [98,]  0.253
 [99,] -0.797
[100,]  0.133

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76 (1 free parameter(s))
Estimate(s): 1.863 
> round( vcov( a ), 3 )
      [,1]
[1,] 0.035
> print(stdEr(a))
[1] 0.1863
>                            # test single stdEr
> 
> proc.time()
   user  system elapsed 
  0.607   0.033   0.635 
