
R version 2.12.2 (2011-02-25)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: i686-pc-linux-gnu (32-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library( maxLik )
Loading required package: miscTools
> 
> ### activePar
> # a simple two-dimensional exponential hat
> f <- function(a) exp(-a[1]^2 - a[2]^2)
> #
> # maximize wrt. both parameters 
> free <- maxNR(f, start=1:2)
> print( free )
$maximum
[1] 1

$estimate
[1] 3.356099e-11 6.160799e-11

$gradient
[1] -1.110223e-10 -1.110223e-10

$hessian
          [,1]      [,2]
[1,] -1.999956  0.000000
[2,]  0.000000 -2.000178

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(free)  # results should be close to (0,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
         estimate      gradient
[1,] 3.356099e-11 -1.110223e-10
[2,] 6.160799e-11 -1.110223e-10
--------------------------------------------
> activePar(free)
[1] TRUE TRUE
> # allow only the second parameter to vary
> cons <- maxNR(f, start=1:2, activePar=c(FALSE,TRUE))
> print( cons )
$maximum
[1] 0.3678794

$estimate
[1] 1.000000e+00 3.515097e-07

$gradient
[1]            NA -2.586265e-07

$hessian
     [,1]       [,2]
[1,]   NA         NA
[2,]   NA -0.7356893

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1]  TRUE FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(cons) # result should be around (1,0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3678794 
Estimates:
         estimate      gradient
[1,] 1.000000e+00            NA
[2,] 3.515097e-07 -2.586265e-07
--------------------------------------------
> activePar(cons)
[1] FALSE  TRUE
> # specify fixed par in different ways
> cons2 <- maxNR(f, start=1:2, fixed=1)
> all.equal( cons, cons2 )
[1] TRUE
> cons3 <- maxNR(f, start=1:2, fixed=c(TRUE,FALSE))
> all.equal( cons, cons3 )
[1] TRUE
> cons4 <- maxNR(f, start=c(a=1, b=2), fixed="a")
> print(summary(cons4))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 4 
Return code: 1 
gradient close to zero 
Function value: 0.3678794 
Estimates:
      estimate      gradient
a 1.000000e+00            NA
b 3.515097e-07 -2.586265e-07
--------------------------------------------
> all.equal( cons, cons4 )
[1] "Component 2: names for current but not for target"                             
[2] "Component 3: names for current but not for target"                             
[3] "Component 4: Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component 8: names for current but not for target"                             
> 
> ### compareDerivatives
> set.seed( 2 )
> ## A simple example with sin(x)' = cos(x)
> f <- sin
> compareDerivatives(f, cos, t0=1)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value: 0.841471 
Dim of analytic gradient: 1 1 
       numeric          : 1 1 
      
param  theta 0  analytic   numeric      rel.diff
  [1,]       1 0.5403023 0.5403023 -5.129895e-11
Max relative difference: 5.129895e-11 
-------- END of compare derivatives -------- 
> ##
> ## Example of log-likelihood of normal density.  Two-parameter
> ## function.
> x <- rnorm(100, 1, 2) # generate rnorm x
> l <- function(b) sum(log(dnorm((x-b[1])/b[2])/b[2]))
>               # b[1] - mu, b[2] - sigma
> gradl <- function(b) {
+    c(sum(x - b[1])/b[2]^2,
+    sum((x - b[1])^2/b[2]^3 - 1/b[2]))
+ }
> compareDerivatives(l, gradl, t0=c(1,2))
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value: -227.8846 
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
          [,1]     [,2]
[1,] -1.534908 16.67607
numeric gradient
          [,1]     [,2]
[1,] -1.534908 16.67607
(anal-num)/(0.5*(abs(anal)+abs(num)))
             [,1]          [,2]
[1,] -1.98858e-09 -2.089468e-10
Max relative difference: 1.98858e-09 
-------- END of compare derivatives -------- 
> 
> 
> ### hessian
> set.seed( 3 )
> # log-likelihood for normal density
> # a[1] - mean
> # a[2] - standard deviation
> ll <- function(a) sum(-log(a[2]) - (x - a[1])^2/(2*a[2]^2))
> x <- rnorm(1000) # sample from standard normal
> ml <- maxLik(ll, start=c(1,1))
> # ignore eventual warnings "NaNs produced in: log(x)"
> print.default( ml )
$maximum
[1] -497.5733

$estimate
[1] 0.006396535 0.997576255

$gradient
[1] -5.684342e-08 -5.684342e-08

$hessian
          [,1]      [,2]
[1,] -1004.878     0.000
[2,]     0.000 -2009.699

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.5733 (2 free parameter(s))
Estimate(s): 0.006396535 0.9975763 
> summary(ml) # result should be close to c(0,1)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.5733 
2  free parameters
Estimates:
      Estimate Std. error t value Pr(> t)    
[1,] 0.0063965  0.0315459  0.2028  0.8393    
[2,] 0.9975763  0.0223067 44.7210  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> hessian(ml) # How the Hessian looks like
          [,1]      [,2]
[1,] -1004.878     0.000
[2,]     0.000 -2009.699
> sqrt(-solve(hessian(ml))) # Note: standard deviations are on the diagonal
           [,1]       [,2]
[1,] 0.03154593 0.00000000
[2,] 0.00000000 0.02230666
> print(stdEr(ml))
[1] 0.03154593 0.02230666
>                            # test vector of stdEr-s
> #
> # Now run the same example while fixing a[2] = 1
> mlf <- maxLik(ll, start=c(1,1), activePar=c(TRUE, FALSE))
> print.default( mlf )
$maximum
[1] -497.5792

$estimate
[1] 0.006396535 1.000000000

$gradient
[1]  0 NA

$hessian
          [,1] [,2]
[1,] -999.9894   NA
[2,]        NA   NA

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE  TRUE

$iterations
[1] 3

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( mlf )
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.5792 (1 free parameter(s))
Estimate(s): 0.006396535 1 
> summary(mlf) # first parameter close to 0, the second exactly 1.0
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 3 iterations
Return code 1: gradient close to zero
Log-Likelihood: -497.5792 
1  free parameters
Estimates:
      Estimate Std. error t value Pr(> t)
[1,] 0.0063965  0.0316229  0.2023  0.8397
[2,] 1.0000000  0.0000000      NA      NA
--------------------------------------------
> hessian(mlf)
          [,1] [,2]
[1,] -999.9894   NA
[2,]        NA   NA
> # now invert only the free parameter part of the Hessian
> sqrt(-solve(hessian(mlf)[activePar(mlf), activePar(mlf)]))
           [,1]
[1,] 0.03162294
> # gives the standard deviation for the mean
> print(stdEr(mlf))
[1] 0.03162294 0.00000000
>                            # test standard errors with fixed par
> 
> 
> ### maxBFGS
> set.seed( 5 )
> # Maximum Likelihood estimation of the parameter of Poissonian distribution
> n <- rpois(100, 3)
> loglik <- function(l) n*log(l) - l - lfactorial(n)
> # we use numeric gradient
> a <- maxBFGS(loglik, start=1)
> print( a )
$maximum
[1] -199.2063

$estimate
[1] 3.189999

$gradient
[1] 1.585043e-05

$hessian
          [,1]
[1,] -31.37757

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE

$iterations
function 
      29 

$type
[1] "BFGS maximisation"

$constraints
NULL

$gradientObs
              [,1]
  [1,] -0.37304065
  [2,]  0.25391869
  [3,]  0.88087804
  [4,] -0.37304065
  [5,] -0.68652033
  [6,]  0.25391869
  [7,] -0.05956098
  [8,]  0.25391869
  [9,]  0.88087804
 [10,] -0.68652033
 [11,] -0.37304065
 [12,] -0.05956098
 [13,] -0.37304065
 [14,] -0.05956098
 [15,] -0.37304065
 [16,] -0.37304065
 [17,] -0.37304065
 [18,]  0.56739837
 [19,] -0.05956098
 [20,]  0.56739837
 [21,]  0.56739837
 [22,]  0.25391869
 [23,] -0.37304065
 [24,] -0.37304065
 [25,] -0.68652033
 [26,] -0.05956098
 [27,] -0.05956098
 [28,]  0.88087804
 [29,] -0.68652033
 [30,]  0.88087804
 [31,] -0.05956098
 [32,] -0.68652033
 [33,] -0.37304065
 [34,] -1.00000000
 [35,] -1.00000000
 [36,] -0.05956098
 [37,] -0.05956098
 [38,] -0.05956098
 [39,] -0.37304065
 [40,] -0.37304065
 [41,]  0.56739837
 [42,] -0.37304065
 [43,]  0.56739837
 [44,] -0.05956098
 [45,]  0.88087804
 [46,] -0.05956098
 [47,]  0.25391869
 [48,] -0.68652033
 [49,]  0.25391869
 [50,] -0.05956098
 [51,] -0.37304065
 [52,] -0.05956098
 [53,]  0.88087804
 [54,]  1.19435771
 [55,]  0.88087804
 [56,] -0.37304065
 [57,] -0.37304065
 [58,] -0.37304065
 [59,] -0.68652033
 [60,] -0.68652033
 [61,] -0.05956098
 [62,] -0.68652033
 [63,]  0.56739837
 [64,]  1.50783739
 [65,]  1.19435771
 [66,]  0.56739837
 [67,]  0.56739837
 [68,] -0.68652033
 [69,]  0.88087804
 [70,]  0.56739837
 [71,]  0.56739837
 [72,]  0.25391869
 [73,] -0.68652033
 [74,]  0.56739837
 [75,] -1.00000000
 [76,]  0.88087804
 [77,] -1.00000000
 [78,]  0.25391869
 [79,] -0.05956098
 [80,] -0.37304065
 [81,]  0.88087804
 [82,]  0.56739837
 [83,] -0.37304065
 [84,] -0.68652033
 [85,] -0.05956098
 [86,] -0.68652033
 [87,]  1.19435771
 [88,]  0.25391869
 [89,] -0.37304065
 [90,] -0.05956098
 [91,]  0.25391869
 [92,] -0.37304065
 [93,] -0.68652033
 [94,] -0.37304065
 [95,] -0.37304065
 [96,]  0.56739837
 [97,] -0.37304065
 [98,] -0.05956098
 [99,] -0.05956098
[100,] -0.05956098

attr(,"class")
[1] "maxim"
> summary( a )
--------------------------------------------
BFGS maximisation 
Number of iterations: 29 
Return code: 0 
successful convergence  
Function value: -199.2063 
Estimates:
     estimate     gradient
[1,] 3.189999 1.585043e-05
--------------------------------------------
> # you would probably prefer mean(n) instead of that ;-)
> # Note also that maxLik is better suited for Maximum Likelihood
> 
> 
> ### logLik.maxLik
> set.seed( 4 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -25.05386

$estimate
[1] 2.11586

$gradient
[1] 6.059581e-08

$hessian
          [,1]
[1,] -22.33706

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.386820737
  [2,] -1.679351425
  [3,]  0.038568228
  [4,]  0.071297536
  [5,]  0.159046827
  [6,]  0.105191909
  [7,]  0.248215149
  [8,]  0.447271101
  [9,]  0.217946015
 [10,]  0.054046210
 [11,] -0.867527901
 [12,]  0.328582591
 [13,]  0.270226231
 [14,]  0.258112507
 [15,]  0.302819655
 [16,] -0.051988003
 [17,]  0.442843506
 [18,]  0.405508829
 [19,] -0.447366184
 [20,] -0.033385380
 [21,]  0.350564555
 [22,] -0.150788618
 [23,] -2.297263120
 [24,]  0.388690937
 [25,] -0.444122551
 [26,]  0.443407902
 [27,]  0.276872851
 [28,] -0.151172545
 [29,]  0.226692191
 [30,]  0.192216408
 [31,] -0.216352485
 [32,] -0.427312132
 [33,] -0.415672374
 [34,]  0.278198843
 [35,] -0.636970370
 [36,]  0.394516801
 [37,]  0.344060909
 [38,] -0.620259677
 [39,]  0.457767065
 [40,]  0.167203754
 [41,]  0.353775674
 [42,] -0.065340669
 [43,]  0.147748189
 [44,]  0.282720513
 [45,] -0.015242578
 [46,]  0.079882301
 [47,]  0.274371829
 [48,]  0.452304209
 [49,] -1.144888506
 [50,]  0.405280562
 [51,] -0.227729885
 [52,]  0.433252231
 [53,]  0.081373333
 [54,] -0.081125847
 [55,] -0.739938583
 [56,]  0.207183333
 [57,]  0.113523151
 [58,]  0.119192735
 [59,]  0.342989799
 [60,]  0.093240222
 [61,]  0.440175142
 [62,] -0.073023342
 [63,] -0.501036832
 [64,]  0.075378939
 [65,] -0.172199931
 [66,]  0.045446563
 [67,] -0.025803404
 [68,]  0.181706954
 [69,]  0.447988621
 [70,] -0.160098113
 [71,]  0.439821886
 [72,]  0.248287379
 [73,]  0.403098278
 [74,] -0.190732563
 [75,] -0.472651004
 [76,] -0.065057866
 [77,] -0.455150229
 [78,]  0.159506496
 [79,]  0.376818619
 [80,]  0.121605791
 [81,]  0.301920775
 [82,] -0.001157197
 [83,]  0.414118368
 [84,]  0.400994285
 [85,]  0.349288513
 [86,] -0.996985138
 [87,]  0.378740648
 [88,]  0.385031346
 [89,] -0.316835766
 [90,]  0.192621244
 [91,]  0.328718428
 [92,] -0.042173028
 [93,]  0.060583993
 [94,] -0.644872464
 [95,] -0.632559541
 [96,] -0.356326814
 [97,] -0.323978775
 [98,]  0.220528784
 [99,] -0.832596225
[100,]  0.361128744

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> ## print log likelihood value
> logLik( a )
[1] -25.05386
> ## compare with log likelihood value of summary object
> all.equal( logLik( a ), logLik( summary( a ) ) )
[1] TRUE
> 
> 
> ### maxBHHH
> set.seed( 6 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxBHHH(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -45.49847 
     parameter initial gradient free
[1,]         1         54.50153    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
successive function values within tolerance limit 
6  iterations
estimate: 2.197878 
Function value: -21.25086 
> print( a )
$maximum
[1] -21.25086

$estimate
[1] 2.197878

$gradient
[1] -4.77513e-05

$hessian
          [,1]
[1,] -18.42347
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
              [,1]
  [1,]  0.34871596
  [2,]  0.36336682
  [3,]  0.14750087
  [4,]  0.27835354
  [5,] -0.60055087
  [6,]  0.31141150
  [7,]  0.42197916
  [8,]  0.18505256
  [9,]  0.09662362
 [10,]  0.43708854
 [11,]  0.27713433
 [12,] -0.32707420
 [13,]  0.25445822
 [14,]  0.41364538
 [15,] -0.34761003
 [16,] -0.10404020
 [17,]  0.35988393
 [18,]  0.43321077
 [19,] -0.24283947
 [20,]  0.40753982
 [21,]  0.43445912
 [22,]  0.21306477
 [23,] -0.72491920
 [24,]  0.16847020
 [25,] -0.73113228
 [26,]  0.41303027
 [27,]  0.13127048
 [28,]  0.30142302
 [29,]  0.03315791
 [30,] -0.32514040
 [31,]  0.26618894
 [32,]  0.33718900
 [33,] -0.63494263
 [34,]  0.42639154
 [35,]  0.41132544
 [36,]  0.21917026
 [37,] -0.23050379
 [38,]  0.42824761
 [39,]  0.43628995
 [40,] -0.49029917
 [41,] -0.86638246
 [42,] -0.05708778
 [43,]  0.17051215
 [44,] -0.06488892
 [45,] -0.04141683
 [46,]  0.21591643
 [47,] -0.27989636
 [48,] -0.04166768
 [49,]  0.44930602
 [50,]  0.28867922
 [51,]  0.38040583
 [52,] -0.29423163
 [53,] -0.12650387
 [54,] -0.52837228
 [55,]  0.05774993
 [56,]  0.39260983
 [57,]  0.41130336
 [58,]  0.21081070
 [59,]  0.43310410
 [60,] -0.11064620
 [61,] -1.08885628
 [62,]  0.28891548
 [63,]  0.41070950
 [64,] -0.57920101
 [65,]  0.37020413
 [66,] -0.10011063
 [67,] -0.31688939
 [68,]  0.31028811
 [69,] -1.05871910
 [70,]  0.17638991
 [71,]  0.37379464
 [72,]  0.02795701
 [73,] -0.46421968
 [74,] -0.65734841
 [75,] -0.11962676
 [76,] -0.08873386
 [77,] -0.35161339
 [78,]  0.09841722
 [79,] -0.14749167
 [80,]  0.36912561
 [81,] -0.23146248
 [82,]  0.18956102
 [83,]  0.18225390
 [84,]  0.12718154
 [85,]  0.44355943
 [86,]  0.28875134
 [87,]  0.38631241
 [88,] -0.96035657
 [89,]  0.45398108
 [90,]  0.27526089
 [91,] -0.13580156
 [92,] -0.19582938
 [93,] -0.24698267
 [94,] -0.81480262
 [95,]  0.17886902
 [96,] -1.18544908
 [97,]  0.41695599
 [98,]  0.38061981
 [99,] -1.16809620
[100,] -0.63345987

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25086 
Estimates:
     estimate     gradient
[1,] 2.197878 -4.77513e-05
--------------------------------------------
> ## Estimate with analytic gradient
> a <- maxBHHH(loglik, gradlik, start=1)
> print( a )
$maximum
[1] -21.25086

$estimate
[1] 2.197878

$gradient
[1] -4.774766e-05

$hessian
          [,1]
[1,] -18.42347
attr(,"type")
[1] "BHHH"

$code
[1] 2

$message
[1] "successive function values within tolerance limit"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "BHHH maximisation"

$gradientObs
              [,1]
  [1,]  0.34871596
  [2,]  0.36336682
  [3,]  0.14750087
  [4,]  0.27835354
  [5,] -0.60055087
  [6,]  0.31141150
  [7,]  0.42197916
  [8,]  0.18505256
  [9,]  0.09662362
 [10,]  0.43708854
 [11,]  0.27713433
 [12,] -0.32707420
 [13,]  0.25445822
 [14,]  0.41364538
 [15,] -0.34761003
 [16,] -0.10404020
 [17,]  0.35988393
 [18,]  0.43321077
 [19,] -0.24283947
 [20,]  0.40753982
 [21,]  0.43445912
 [22,]  0.21306477
 [23,] -0.72491920
 [24,]  0.16847020
 [25,] -0.73113228
 [26,]  0.41303027
 [27,]  0.13127048
 [28,]  0.30142302
 [29,]  0.03315791
 [30,] -0.32514040
 [31,]  0.26618894
 [32,]  0.33718900
 [33,] -0.63494263
 [34,]  0.42639154
 [35,]  0.41132544
 [36,]  0.21917026
 [37,] -0.23050379
 [38,]  0.42824761
 [39,]  0.43628995
 [40,] -0.49029917
 [41,] -0.86638246
 [42,] -0.05708778
 [43,]  0.17051215
 [44,] -0.06488892
 [45,] -0.04141683
 [46,]  0.21591643
 [47,] -0.27989636
 [48,] -0.04166768
 [49,]  0.44930602
 [50,]  0.28867922
 [51,]  0.38040583
 [52,] -0.29423163
 [53,] -0.12650387
 [54,] -0.52837228
 [55,]  0.05774993
 [56,]  0.39260983
 [57,]  0.41130336
 [58,]  0.21081070
 [59,]  0.43310410
 [60,] -0.11064620
 [61,] -1.08885628
 [62,]  0.28891548
 [63,]  0.41070950
 [64,] -0.57920101
 [65,]  0.37020413
 [66,] -0.10011063
 [67,] -0.31688939
 [68,]  0.31028811
 [69,] -1.05871910
 [70,]  0.17638991
 [71,]  0.37379464
 [72,]  0.02795701
 [73,] -0.46421968
 [74,] -0.65734841
 [75,] -0.11962676
 [76,] -0.08873386
 [77,] -0.35161339
 [78,]  0.09841722
 [79,] -0.14749167
 [80,]  0.36912561
 [81,] -0.23146248
 [82,]  0.18956102
 [83,]  0.18225390
 [84,]  0.12718154
 [85,]  0.44355943
 [86,]  0.28875134
 [87,]  0.38631241
 [88,] -0.96035657
 [89,]  0.45398108
 [90,]  0.27526089
 [91,] -0.13580156
 [92,] -0.19582938
 [93,] -0.24698267
 [94,] -0.81480262
 [95,]  0.17886902
 [96,] -1.18544908
 [97,]  0.41695599
 [98,]  0.38061981
 [99,] -1.16809620
[100,] -0.63345987

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
BHHH maximisation 
Number of iterations: 6 
Return code: 2 
successive function values within tolerance limit 
Function value: -21.25086 
Estimates:
     estimate      gradient
[1,] 2.197878 -4.774766e-05
--------------------------------------------
> 
> 
> ### maxLik
> set.seed( 7 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -47.72495 
     parameter initial gradient free
[1,]         1         52.27505    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.09534 
Function value: -26.02842 
> print.default( a )
$maximum
[1] -26.02842

$estimate
[1] 2.09534

$gradient
[1] 3.867462e-07

$hessian
       [,1]
[1,] -22.78

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.453076486
  [2,] -0.333754275
  [3,] -0.379319583
  [4,]  0.071152598
  [5,]  0.204437590
  [6,] -0.832883569
  [7,]  0.101320915
  [8,] -1.659286401
  [9,]  0.374932959
 [10,]  0.454498456
 [11,]  0.005139766
 [12,]  0.103236085
 [13,] -0.458754690
 [14,] -0.456470531
 [15,] -0.127917548
 [16,]  0.304629502
 [17,]  0.138785664
 [18,] -0.001695432
 [19,]  0.217548304
 [20,]  0.019933583
 [21,] -0.723023977
 [22,]  0.160960457
 [23,]  0.278638996
 [24,]  0.257170039
 [25,]  0.203190912
 [26,]  0.349488735
 [27,]  0.254260194
 [28,] -0.142986612
 [29,] -0.367060756
 [30,] -0.464810778
 [31,] -0.151823003
 [32,]  0.082873526
 [33,] -0.483568638
 [34,]  0.472857729
 [35,]  0.213409179
 [36,]  0.283580665
 [37,]  0.433195348
 [38,]  0.318162310
 [39,]  0.013858604
 [40,]  0.031204081
 [41,]  0.136507210
 [42,]  0.356412488
 [43,] -0.254104122
 [44,]  0.409527106
 [45,] -0.021114769
 [46,]  0.172770579
 [47,] -0.243260232
 [48,] -0.821155636
 [49,]  0.215106805
 [50,] -0.065112106
 [51,] -0.202857472
 [52,] -0.057356516
 [53,] -1.640491060
 [54,]  0.414101950
 [55,]  0.405286205
 [56,]  0.437213122
 [57,] -0.079218885
 [58,]  0.153157698
 [59,] -0.460444690
 [60,] -0.162073225
 [61,] -0.003462042
 [62,] -0.056984548
 [63,]  0.462446774
 [64,] -0.530977815
 [65,]  0.292494971
 [66,] -0.161659562
 [67,] -1.125602184
 [68,]  0.348080895
 [69,]  0.411331377
 [70,]  0.187929030
 [71,] -0.452236459
 [72,]  0.144561096
 [73,]  0.446811315
 [74,]  0.054008039
 [75,]  0.270582387
 [76,] -0.011709379
 [77,]  0.217714597
 [78,] -0.356963294
 [79,]  0.352124491
 [80,] -0.832331888
 [81,]  0.198150426
 [82,]  0.472163851
 [83,]  0.249348856
 [84,]  0.167729767
 [85,]  0.219018543
 [86,]  0.171749325
 [87,]  0.309892365
 [88,]  0.463958057
 [89,] -0.428327243
 [90,]  0.414486131
 [91,]  0.092506977
 [92,]  0.258234393
 [93,] -0.488154635
 [94,]  0.459729268
 [95,] -0.802043026
 [96,]  0.412443161
 [97,]  0.053559978
 [98,]  0.178618263
 [99,] -0.136453211
[100,]  0.166150001

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.02842 (1 free parameter(s))
Estimate(s): 2.09534 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.02842 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.09534    0.20952  10.001 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -26.02842

$estimate
[1] 2.09534

$gradient
[1] 4.614954e-08

$hessian
          [,1]
[1,] -22.77671

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.453076482
  [2,] -0.333754279
  [3,] -0.379319586
  [4,]  0.071152595
  [5,]  0.204437587
  [6,] -0.832883572
  [7,]  0.101320912
  [8,] -1.659286404
  [9,]  0.374932956
 [10,]  0.454498453
 [11,]  0.005139763
 [12,]  0.103236082
 [13,] -0.458754694
 [14,] -0.456470535
 [15,] -0.127917551
 [16,]  0.304629499
 [17,]  0.138785660
 [18,] -0.001695436
 [19,]  0.217548301
 [20,]  0.019933580
 [21,] -0.723023980
 [22,]  0.160960453
 [23,]  0.278638992
 [24,]  0.257170036
 [25,]  0.203190908
 [26,]  0.349488732
 [27,]  0.254260191
 [28,] -0.142986616
 [29,] -0.367060760
 [30,] -0.464810782
 [31,] -0.151823006
 [32,]  0.082873523
 [33,] -0.483568641
 [34,]  0.472857726
 [35,]  0.213409176
 [36,]  0.283580662
 [37,]  0.433195344
 [38,]  0.318162306
 [39,]  0.013858601
 [40,]  0.031204078
 [41,]  0.136507206
 [42,]  0.356412485
 [43,] -0.254104125
 [44,]  0.409527103
 [45,] -0.021114773
 [46,]  0.172770576
 [47,] -0.243260236
 [48,] -0.821155639
 [49,]  0.215106801
 [50,] -0.065112109
 [51,] -0.202857476
 [52,] -0.057356519
 [53,] -1.640491063
 [54,]  0.414101946
 [55,]  0.405286202
 [56,]  0.437213119
 [57,] -0.079218889
 [58,]  0.153157694
 [59,] -0.460444693
 [60,] -0.162073229
 [61,] -0.003462046
 [62,] -0.056984551
 [63,]  0.462446771
 [64,] -0.530977819
 [65,]  0.292494968
 [66,] -0.161659565
 [67,] -1.125602188
 [68,]  0.348080891
 [69,]  0.411331374
 [70,]  0.187929026
 [71,] -0.452236462
 [72,]  0.144561092
 [73,]  0.446811311
 [74,]  0.054008035
 [75,]  0.270582383
 [76,] -0.011709382
 [77,]  0.217714593
 [78,] -0.356963298
 [79,]  0.352124487
 [80,] -0.832331892
 [81,]  0.198150422
 [82,]  0.472163848
 [83,]  0.249348852
 [84,]  0.167729763
 [85,]  0.219018540
 [86,]  0.171749322
 [87,]  0.309892362
 [88,]  0.463958054
 [89,] -0.428327246
 [90,]  0.414486127
 [91,]  0.092506974
 [92,]  0.258234390
 [93,] -0.488154639
 [94,]  0.459729264
 [95,] -0.802043030
 [96,]  0.412443158
 [97,]  0.053559975
 [98,]  0.178618259
 [99,] -0.136453214
[100,]  0.166149998

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.02842 (1 free parameter(s))
Estimate(s): 2.09534 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -26.02842 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.09534    0.20953      10 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> 
> ### maxNR
> set.seed( 8 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglikSum <- function(theta) sum(log(theta) - theta*t)
> ## Note the log-likelihood and gradient are summed over observations
> gradlikSum <- function(theta) sum(1/theta - t)
> ## Estimate with numeric gradient and Hessian
> a <- maxNR(loglikSum, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -46.48941 
     parameter initial gradient free
[1,]         1         53.51059    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 2.151027 
Function value: -23.40544 
> print( a )
$maximum
[1] -23.40544

$estimate
[1] 2.151027

$gradient
[1] -6.07514e-07

$hessian
         [,1]
[1,] -21.6005

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.40544 
Estimates:
     estimate     gradient
[1,] 2.151027 -6.07514e-07
--------------------------------------------
> ## You would probably prefer 1/mean(t) instead ;-)
> ## Estimate with analytic gradient and Hessian
> a <- maxNR(loglikSum, gradlikSum, hesslik, start=1)
> print( a )
$maximum
[1] -23.40544

$estimate
[1] 2.151027

$gradient
[1] 9.493475e-08

$hessian
          [,1]
[1,] -21.61265

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(a)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 5 
Return code: 1 
gradient close to zero 
Function value: -23.40544 
Estimates:
     estimate     gradient
[1,] 2.151027 9.493475e-08
--------------------------------------------
> 
> 
> ### maximType
> ## maximise two-dimensional exponential hat.  Maximum is at c(2,1):
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m <- maxNR(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 1.110223e-10 1.110223e-10

$hessian
          [,1]      [,2]
[1,] -1.999956  0.000000
[2,]  0.000000 -2.000178

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 7

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary(m)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
     estimate     gradient
[1,]        2 1.110223e-10
[2,]        1 1.110223e-10
--------------------------------------------
> maximType(m)
[1] "Newton-Raphson maximisation"
> ## Now use BFGS maximisation.
> m <- maxBFGS(f, start=c(0,0))
> print( m )
$maximum
[1] 1

$estimate
[1] 2 1

$gradient
[1] 6.661338e-09 3.108624e-09

$hessian
          [,1]      [,2]
[1,] -1.999956  0.000000
[2,]  0.000000 -2.000178

$code
[1] 0

$message
[1] "successful convergence "

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
function 
      31 

$type
[1] "BFGS maximisation"

$constraints
NULL

attr(,"class")
[1] "maxim"
> summary(m)
--------------------------------------------
BFGS maximisation 
Number of iterations: 31 
Return code: 0 
successful convergence  
Function value: 1 
Estimates:
     estimate     gradient
[1,]        2 6.661338e-09
[2,]        1 3.108624e-09
--------------------------------------------
> maximType(m)
[1] "BFGS maximisation"
> 
> ### Test maxNR with 0 iterations.  Should perform no iterations
> ### Request by Yves Croissant
> f <- function(a) exp(-(a[1] - 2)^2 - (a[2] - 1)^2)
> m0 <- maxNR(f, start=c(1.1, 2.1), iterlim=0)
> summary(m0)
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 0 
Return code: 4 
Iteration limit exceeded. 
Function value: 0.1326555 
Estimates:
     estimate   gradient
[1,]      1.1  0.2387798
[2,]      2.1 -0.2918420
--------------------------------------------
> 
> ### nObs
> set.seed( 10 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> nObs(m)
[1] 100
> 
> 
> ### nParam
> set.seed( 11 )
> # Construct a simple OLS regression:
> x1 <- runif(100)
> x2 <- runif(100)
> y <- 3 + 4*x1 + 5*x2 + rnorm(100)
> m <- lm(y~x1+x2)  # estimate it
> summary(m)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.34360 -0.53383 -0.02911  0.55009  2.69344 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   3.2421     0.2871   11.29   <2e-16 ***
x1            3.9745     0.3953   10.05   <2e-16 ***
x2            4.7827     0.3667   13.04   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.9897 on 97 degrees of freedom
Multiple R-squared: 0.7019,	Adjusted R-squared: 0.6957 
F-statistic: 114.2 on 2 and 97 DF,  p-value: < 2.2e-16 

> nParam(m) # you get 3
[1] 3
> 
> 
> ### numericGradient
> # A simple example with Gaussian bell
> f0 <- function(t0) exp(-t0[1]^2 - t0[2]^2)
> numericGradient(f0, c(1,2))
            [,1]        [,2]
[1,] -0.01347589 -0.02695179
> numericHessian(f0, t0=c(1,2))
           [,1]       [,2]
[1,] 0.01348748 0.05390306
[2,] 0.05390306 0.09432906
> # An example with the analytic gradient
> gradf0 <- function(t0) -2*t0*f0(t0)
> numericHessian(f0, gradf0, t0=c(1,2))
           [,1]       [,2]
[1,] 0.01347589 0.05390358
[2,] 0.05390358 0.09433126
> # The results should be similar as in the previous case
> # The central numeric derivatives have usually quite a high precision
> compareDerivatives(f0, gradf0, t0=1:2)
-------- compare derivatives -------- 
Note: analytic gradient is vector.  Transforming into a matrix form
Function value: 0.006737947 
Dim of analytic gradient: 1 2 
       numeric          : 1 2 
t0
[1] 1 2
analytic gradient
            [,1]        [,2]
[1,] -0.01347589 -0.02695179
numeric gradient
            [,1]        [,2]
[1,] -0.01347589 -0.02695179
(anal-num)/(0.5*(abs(anal)+abs(num)))
              [,1]       [,2]
[1,] -2.763538e-10 -5.108e-11
Max relative difference: 2.763538e-10 
-------- END of compare derivatives -------- 
> # The differenc is around 1e-10
> 
> 
> ### returnCode
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.631625e-10

$gradient
[1] -6.661338e-10

$hessian
          [,1]
[1,] -1.999956

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should be success (1 or 2)
[1] 1
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.276586

$estimate
[1] 10684.89

$gradient
[1] 9.359091e-05

$hessian
            [,1]
[1,] 0.001776357

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnCode(a) # should give a failure (4)
[1] 4
> 
> 
> ### returnMessage
> ## maximise the exponential bell
> f1 <- function(x) exp(-x^2)
> a <- maxNR(f1, start=2)
> print( a )
$maximum
[1] 1

$estimate
[1] 3.631625e-10

$gradient
[1] -6.661338e-10

$hessian
          [,1]
[1,] -1.999956

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 4

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should be success (1 or 2)
[1] "gradient close to zero"
> ## Now try to maximise log() function
> f2 <- function(x) log(x)
> a <- maxNR(f2, start=2)
> print( a )
$maximum
[1] 9.276586

$estimate
[1] 10684.89

$gradient
[1] 9.359091e-05

$hessian
            [,1]
[1,] 0.001776357

$code
[1] 4

$message
[1] "Iteration limit exceeded."

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 150

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> returnMessage(a) # should give a failure (4)
[1] "Iteration limit exceeded."
> 
> 
> ### summary.maxLik
> set.seed( 15 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> loglik <- function(theta) log(theta) - theta*t
> gradlik <- function(theta) 1/theta - t
> hesslik <- function(theta) -100/theta^2
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -41.56087 
     parameter initial gradient free
[1,]         1         58.43913    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
-----Iteration 6 -----
--------------
gradient close to zero 
6  iterations
estimate: 2.406109 
Function value: -12.19890 
> print.default( a )
$maximum
[1] -12.19890

$estimate
[1] 2.406109

$gradient
[1] 2.664535e-09

$hessian
          [,1]
[1,] -17.26619

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
              [,1]
  [1,]  0.31349470
  [2,] -0.55771415
  [3,]  0.28839132
  [4,]  0.32759001
  [5,]  0.08417972
  [6,] -0.96146694
  [7,]  0.26945783
  [8,]  0.40651835
  [9,]  0.20898018
 [10,]  0.26053657
 [11,]  0.36667703
 [12,]  0.11188134
 [13,]  0.12226279
 [14,] -0.14609108
 [15,] -1.16675829
 [16,] -0.67553795
 [17,] -0.01994508
 [18,]  0.02832111
 [19,] -0.72866672
 [20,]  0.25707536
 [21,]  0.05079755
 [22,] -0.11852099
 [23,] -0.04543070
 [24,]  0.07171067
 [25,] -1.85966636
 [26,]  0.24890867
 [27,]  0.20148880
 [28,]  0.14710495
 [29,]  0.32964511
 [30,]  0.28774665
 [31,]  0.13947842
 [32,]  0.09780431
 [33,]  0.02198454
 [34,] -0.07455749
 [35,]  0.24167211
 [36,]  0.14143013
 [37,] -0.09279971
 [38,]  0.08259086
 [39,]  0.17984229
 [40,] -0.24059134
 [41,]  0.23469764
 [42,]  0.32514557
 [43,] -0.31013347
 [44,]  0.27431879
 [45,]  0.15061831
 [46,]  0.35935745
 [47,] -0.16595145
 [48,]  0.11667232
 [49,]  0.41140164
 [50,] -0.85562289
 [51,]  0.36920973
 [52,]  0.01178107
 [53,]  0.09068439
 [54,] -0.41852212
 [55,]  0.16272515
 [56,]  0.38103066
 [57,] -0.24092055
 [58,]  0.39321672
 [59,]  0.18728636
 [60,] -0.06962234
 [61,] -0.52603689
 [62,]  0.36746125
 [63,]  0.21708343
 [64,]  0.21913252
 [65,]  0.25758625
 [66,]  0.39796764
 [67,]  0.29823077
 [68,] -0.03089786
 [69,] -0.09985511
 [70,] -0.65673214
 [71,]  0.05177162
 [72,] -0.67133518
 [73,]  0.32399269
 [74,] -0.75222129
 [75,]  0.20941896
 [76,] -1.05024846
 [77,]  0.39356564
 [78,] -0.13051165
 [79,] -1.34923501
 [80,] -0.05024056
 [81,]  0.23677884
 [82,] -0.01697319
 [83,]  0.15387168
 [84,]  0.27502748
 [85,]  0.15766217
 [86,] -0.42968887
 [87,]  0.21483083
 [88,]  0.41433889
 [89,]  0.21807875
 [90,]  0.22107735
 [91,] -0.07248145
 [92,]  0.26975749
 [93,] -0.06964576
 [94,]  0.30825551
 [95,] -0.17815966
 [96,]  0.15251409
 [97,]  0.18499674
 [98,]  0.07499089
 [99,]  0.36483072
[100,]  0.09384337

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.19890 (1 free parameter(s))
Estimate(s): 2.406109 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.19890 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.40611    0.24066   9.998 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -12.19890

$estimate
[1] 2.406109

$gradient
[1] 5.245804e-14

$hessian
          [,1]
[1,] -17.27306

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 6

$type
[1] "Newton-Raphson maximisation"

$gradientObs
              [,1]
  [1,]  0.31349470
  [2,] -0.55771415
  [3,]  0.28839132
  [4,]  0.32759001
  [5,]  0.08417972
  [6,] -0.96146694
  [7,]  0.26945783
  [8,]  0.40651835
  [9,]  0.20898018
 [10,]  0.26053657
 [11,]  0.36667703
 [12,]  0.11188134
 [13,]  0.12226279
 [14,] -0.14609108
 [15,] -1.16675829
 [16,] -0.67553795
 [17,] -0.01994508
 [18,]  0.02832111
 [19,] -0.72866673
 [20,]  0.25707536
 [21,]  0.05079755
 [22,] -0.11852099
 [23,] -0.04543070
 [24,]  0.07171067
 [25,] -1.85966636
 [26,]  0.24890867
 [27,]  0.20148880
 [28,]  0.14710495
 [29,]  0.32964511
 [30,]  0.28774665
 [31,]  0.13947842
 [32,]  0.09780431
 [33,]  0.02198454
 [34,] -0.07455749
 [35,]  0.24167211
 [36,]  0.14143013
 [37,] -0.09279971
 [38,]  0.08259086
 [39,]  0.17984229
 [40,] -0.24059134
 [41,]  0.23469764
 [42,]  0.32514557
 [43,] -0.31013347
 [44,]  0.27431879
 [45,]  0.15061831
 [46,]  0.35935745
 [47,] -0.16595145
 [48,]  0.11667232
 [49,]  0.41140164
 [50,] -0.85562289
 [51,]  0.36920973
 [52,]  0.01178107
 [53,]  0.09068439
 [54,] -0.41852212
 [55,]  0.16272515
 [56,]  0.38103066
 [57,] -0.24092055
 [58,]  0.39321672
 [59,]  0.18728636
 [60,] -0.06962234
 [61,] -0.52603689
 [62,]  0.36746125
 [63,]  0.21708343
 [64,]  0.21913252
 [65,]  0.25758625
 [66,]  0.39796764
 [67,]  0.29823077
 [68,] -0.03089786
 [69,] -0.09985511
 [70,] -0.65673214
 [71,]  0.05177162
 [72,] -0.67133518
 [73,]  0.32399269
 [74,] -0.75222129
 [75,]  0.20941896
 [76,] -1.05024846
 [77,]  0.39356564
 [78,] -0.13051165
 [79,] -1.34923501
 [80,] -0.05024056
 [81,]  0.23677884
 [82,] -0.01697319
 [83,]  0.15387168
 [84,]  0.27502748
 [85,]  0.15766217
 [86,] -0.42968887
 [87,]  0.21483083
 [88,]  0.41433889
 [89,]  0.21807875
 [90,]  0.22107735
 [91,] -0.07248145
 [92,]  0.26975749
 [93,] -0.06964576
 [94,]  0.30825551
 [95,] -0.17815966
 [96,]  0.15251409
 [97,]  0.18499674
 [98,]  0.07499089
 [99,]  0.36483072
[100,]  0.09384337

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.19890 (1 free parameter(s))
Estimate(s): 2.406109 
> summary(a)
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 6 iterations
Return code 1: gradient close to zero
Log-Likelihood: -12.19890 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.40611    0.24061      10 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> 
> ### summary.maxim and for "gradient"/"hessian" attributes
> ### Test for infinity
> ## maximize a 2D quadratic function:
> f <- function(b) {
+   x <- b[1]; y <- b[2];
+     val <- (x - 2)^2 + (y - 3)^2
+     attr(val, "gradient") <- c(2*x - 4, 2*y - 6)
+     attr(val, "hessian") <- matrix(c(2, 0, 0, 2), 2, 2)
+     val
+ }
> ## Use c(0,0) as initial value.  
> result1 <- maxNR( f, start = c(0,0) )
> print( result1 )
$maximum
[1] Inf

$estimate
[1] -7.034857e+155 -1.055228e+156

$gradient
[1] -1.406971e+156 -2.110457e+156

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 25

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result1 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 25 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
           estimate       gradient
[1,] -7.034857e+155 -1.406971e+156
[2,] -1.055228e+156 -2.110457e+156
--------------------------------------------
> ## Now use c(1000000, -777777) as initial value and ask for hessian
> result2 <- maxNR( f, start = c( 1000000, -777777))
> print( result2 )
$maximum
[1] Inf

$estimate
[1]  2.110451e+155 -1.641470e+155

$gradient
[1]  4.220903e+155 -3.282940e+155

$hessian
     [,1] [,2]
[1,]    2    0
[2,]    0    2

$code
[1] 5

$message
[1] "Infinite value"

$last.step
NULL

$fixed
[1] FALSE FALSE

$iterations
[1] 24

$type
[1] "Newton-Raphson maximisation"

attr(,"class")
[1] "maxim" "list" 
> summary( result2 )
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 24 
Return code: 5 
Infinite value 
Function value: Inf 
Estimates:
           estimate       gradient
[1,]  2.110451e+155  4.220903e+155
[2,] -1.641470e+155 -3.282940e+155
--------------------------------------------
> 
> 
> ### Test for "gradient"/"hessian" attributes.  A case which converges.
> hub <- function(x) {
+    v <- exp(-sum(x*x))
+    val <- v
+    attr(val, "gradient") <- -2*x*v
+    attr(val, "hessian") <- 4*(x %*% t(x))*v - diag(2*c(v, v))
+    val
+ }
> summary(a <- maxNR(hub, start=c(2,1)))
--------------------------------------------
Newton-Raphson maximisation 
Number of iterations: 7 
Return code: 1 
gradient close to zero 
Function value: 1 
Estimates:
          estimate     gradient
[1,] -7.448384e-18 1.489677e-17
[2,] -3.724192e-18 7.448384e-18
--------------------------------------------
> ## Now test "gradient" attribute for BHHH/3-parameter probit
> N <- 1000
> loglikProbit <- function( beta) {
+    xb <- x %*% beta
+    loglik <- ifelse(y == 0,
+                     pnorm( xb, log=TRUE, lower.tail=FALSE),
+                     pnorm( xb, log.p=TRUE))
+    grad <- ifelse(y == 0,
+                   -dnorm(xb)/pnorm(xb, lower.tail=FALSE),
+                   dnorm(xb)/pnorm(xb))
+    grad <- grad*x
+    attr(loglik, "gradient") <- grad
+    loglik
+ }
> x <- runif(N)
> x <- cbind(x, x - runif(N), x - runif(N))
> y <- x[,1] + 2*x[,2] - x[,3] + rnorm(N) > 0
> summary(maxLik(loglikProbit, start=c(0,0,0), method="bhhh"))
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -508.3503 
3  free parameters
Estimates:
      Estimate Std. error t value   Pr(> t)    
[1,]  0.857811   0.090356  9.4937 < 2.2e-16 ***
[2,]  1.938900   0.151361 12.8098 < 2.2e-16 ***
[3,] -0.825310   0.133938 -6.1619 7.189e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> 
> 
> ### vcov.maxLik
> set.seed( 17 )
> ## ML estimation of exponential duration model:
> t <- rexp(100, 2)
> ## Estimate with numeric gradient and hessian
> a <- maxLik(loglik, start=1, print.level=2)
----- Initial parameters: -----
fcn value: -53.66641 
     parameter initial gradient free
[1,]         1         46.33359    1
Condition number of the (active) hessian: 1 
-----Iteration 1 -----
-----Iteration 2 -----
-----Iteration 3 -----
-----Iteration 4 -----
-----Iteration 5 -----
--------------
gradient close to zero 
5  iterations
estimate: 1.863363 
Function value: -37.76171 
> print.default( a )
$maximum
[1] -37.76171

$estimate
[1] 1.863363

$gradient
[1] 2.264855e-08

$hessian
          [,1]
[1,] -28.81251

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
              [,1]
  [1,] -0.27668643
  [2,]  0.39530166
  [3,]  0.49786694
  [4,] -0.48403375
  [5,] -0.40489732
  [6,]  0.25669440
  [7,] -0.43225880
  [8,]  0.18930221
  [9,]  0.20658112
 [10,]  0.32846694
 [11,]  0.16647569
 [12,]  0.39362984
 [13,]  0.51043058
 [14,]  0.32612615
 [15,]  0.23353080
 [16,]  0.50455224
 [17,]  0.46120276
 [18,]  0.46076885
 [19,] -1.06433090
 [20,]  0.23754717
 [21,] -0.04647125
 [22,]  0.47335272
 [23,] -0.40035195
 [24,]  0.23480144
 [25,]  0.28459108
 [26,]  0.40288506
 [27,] -0.23784509
 [28,]  0.44112441
 [29,]  0.48213991
 [30,]  0.49548928
 [31,] -0.36521320
 [32,]  0.38776048
 [33,] -0.40673276
 [34,] -0.18087740
 [35,]  0.41854462
 [36,] -0.33042595
 [37,] -0.24039322
 [38,] -0.41533743
 [39,]  0.46116957
 [40,] -3.89297763
 [41,]  0.03343392
 [42,] -0.62928097
 [43,]  0.43249191
 [44,]  0.03649238
 [45,]  0.24621151
 [46,] -0.22675193
 [47,]  0.53061906
 [48,]  0.51599611
 [49,] -0.67720133
 [50,]  0.15255978
 [51,]  0.22194835
 [52,]  0.43625861
 [53,] -1.56328546
 [54,] -0.05016088
 [55,]  0.31241537
 [56,] -0.14645100
 [57,] -0.35672478
 [58,]  0.48097677
 [59,]  0.39851400
 [60,]  0.42334260
 [61,]  0.33007324
 [62,]  0.35260824
 [63,] -0.30180798
 [64,]  0.47083319
 [65,]  0.29103526
 [66,]  0.11971942
 [67,]  0.50739640
 [68,] -0.24951643
 [69,] -0.05047207
 [70,]  0.28001004
 [71,]  0.52554537
 [72,] -0.37398442
 [73,]  0.02352064
 [74,] -0.41328813
 [75,] -0.35850191
 [76,]  0.47663895
 [77,]  0.24155123
 [78,] -0.04114780
 [79,] -1.34772491
 [80,] -0.25668088
 [81,]  0.32432372
 [82,]  0.34521415
 [83,] -0.93415876
 [84,]  0.33243095
 [85,] -0.84978609
 [86,]  0.31576246
 [87,]  0.47518450
 [88,]  0.37259225
 [89,] -0.02529961
 [90,]  0.37657999
 [91,] -2.10829532
 [92,] -0.44964484
 [93,]  0.16821224
 [94,]  0.35101197
 [95,]  0.52543731
 [96,] -0.06619079
 [97,]  0.38781628
 [98,]  0.25349617
 [99,] -0.79665317
[100,]  0.13325230

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76171 (1 free parameter(s))
Estimate(s): 1.863363 
> vcov(a)
           [,1]
[1,] 0.03470715
> ## Estimate with analytic gradient and hessian
> a <- maxLik(loglik, gradlik, hesslik, start=1)
> print.default( a )
$maximum
[1] -37.76171

$estimate
[1] 1.863363

$gradient
[1] 1.092372e-09

$hessian
          [,1]
[1,] -28.80083

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
              [,1]
  [1,] -0.27668643
  [2,]  0.39530166
  [3,]  0.49786694
  [4,] -0.48403375
  [5,] -0.40489732
  [6,]  0.25669440
  [7,] -0.43225880
  [8,]  0.18930221
  [9,]  0.20658112
 [10,]  0.32846694
 [11,]  0.16647569
 [12,]  0.39362983
 [13,]  0.51043058
 [14,]  0.32612615
 [15,]  0.23353080
 [16,]  0.50455224
 [17,]  0.46120276
 [18,]  0.46076885
 [19,] -1.06433090
 [20,]  0.23754717
 [21,] -0.04647125
 [22,]  0.47335272
 [23,] -0.40035195
 [24,]  0.23480144
 [25,]  0.28459108
 [26,]  0.40288506
 [27,] -0.23784509
 [28,]  0.44112441
 [29,]  0.48213991
 [30,]  0.49548928
 [31,] -0.36521320
 [32,]  0.38776048
 [33,] -0.40673276
 [34,] -0.18087740
 [35,]  0.41854462
 [36,] -0.33042595
 [37,] -0.24039322
 [38,] -0.41533743
 [39,]  0.46116957
 [40,] -3.89297763
 [41,]  0.03343392
 [42,] -0.62928097
 [43,]  0.43249191
 [44,]  0.03649238
 [45,]  0.24621151
 [46,] -0.22675193
 [47,]  0.53061906
 [48,]  0.51599611
 [49,] -0.67720133
 [50,]  0.15255978
 [51,]  0.22194835
 [52,]  0.43625861
 [53,] -1.56328546
 [54,] -0.05016088
 [55,]  0.31241537
 [56,] -0.14645101
 [57,] -0.35672478
 [58,]  0.48097677
 [59,]  0.39851400
 [60,]  0.42334260
 [61,]  0.33007324
 [62,]  0.35260824
 [63,] -0.30180798
 [64,]  0.47083319
 [65,]  0.29103526
 [66,]  0.11971942
 [67,]  0.50739640
 [68,] -0.24951643
 [69,] -0.05047207
 [70,]  0.28001004
 [71,]  0.52554537
 [72,] -0.37398442
 [73,]  0.02352064
 [74,] -0.41328813
 [75,] -0.35850191
 [76,]  0.47663895
 [77,]  0.24155123
 [78,] -0.04114780
 [79,] -1.34772491
 [80,] -0.25668088
 [81,]  0.32432372
 [82,]  0.34521415
 [83,] -0.93415876
 [84,]  0.33243095
 [85,] -0.84978609
 [86,]  0.31576246
 [87,]  0.47518450
 [88,]  0.37259225
 [89,] -0.02529961
 [90,]  0.37657999
 [91,] -2.10829531
 [92,] -0.44964484
 [93,]  0.16821224
 [94,]  0.35101197
 [95,]  0.52543731
 [96,] -0.06619079
 [97,]  0.38781628
 [98,]  0.25349617
 [99,] -0.79665317
[100,]  0.13325230

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> print( a )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -37.76171 (1 free parameter(s))
Estimate(s): 1.863363 
> vcov(a)
           [,1]
[1,] 0.03472122
> print(stdEr(a))
[1] 0.1863363
>                            # test single stdEr
> 
