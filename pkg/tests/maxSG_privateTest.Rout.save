
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ### tests for stochastic gradient ascent
> ### Test the following things:
> ###
> ### 1. basic 2-D SGA
> ###    SGA without function, only gradient
> ###    SGA neither function nor gradient
> ###    SGA in 1-D case
> ### 2. SGA w/momentum
> ### 3. SGA full batch
> ### 4. SGA, no gradient supplied
> ###    SGA, return numeric hessian, gradient provided
> ###    SGA, return numeric hessian, no gradient provided
> ###    SGA, printlevel 1, storeValues
> ###    SGA, NA as iterlim: should give informative error
> ###    SGA, storeValues but no fn (should fail)
> ###
> ### using highly unequally scaled data
> ###    SGA without gradient clipping (fails)
> ###    SGA with gradient clipping (works, although does not converge)
> 
> library(maxLik)
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> library(testthat)
> 
> ## ---------- OLS 
> ## log-likelihood function(s):
> ## return log-likelihood on validation data
> loglik <- function(beta, index) {
+    e <- yValid - XValid %*% beta
+    -crossprod(e)/length(y)
+ }
> ## gradlik: work on training data
> gradlik <- function(beta, index) {
+    e <- yTrain[index] - XTrain[index,,drop=FALSE] %*% beta
+    g <- t(-2*t(XTrain[index,,drop=FALSE]) %*% e)
+    -g/length(index)
+ }
> 
> ### create random data
> set.seed(1)
> N <- 1000
> x <- rnorm(N)
> X <- cbind(1, x)
> y <- 1 + x + rnorm(N)
> ## training-validation
> iTrain <- sample(N, 0.8*N)
> XTrain <- X[iTrain,,drop=FALSE]
> XValid <- X[-iTrain,,drop=FALSE]
> yTrain <- y[iTrain]
> yValid <- y[-iTrain]
> cat("Analytic solution (training data):\n")
Analytic solution (training data):
> start <- c(const=10, x=10)
> b0 <- drop(solve(crossprod(XTrain)) %*% crossprod(XTrain, yTrain))
> names(b0) <- names(start)
> tol <- 1e-1  # coefficient tolerance
> 
> ## ---------- 1. working example
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SG_batchSize=100, SG_learningRate=0.1,
+                          storeValues=TRUE),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
>                            # SGA usually ends with gradient not equal to 0 so we don't test that
> 
> ## ---------- store parameters
> res <- maxSGA(loglik, gradlik, start=start,
+               control=list(printLevel=0, iterlim=20,
+                            SG_batchSize=100, SG_learningRate=0.1,
+                            storeParameters=TRUE),
+               nObs=length(yTrain))
> expect_equal(dim(storedParameters(res)), c(1 + nIter(res), 2))
> 
> ## ---------- no function, only gradient
> res <- maxSGA(grad=gradlik, start=start,
+             control=list(printLevel=0, iterlim=10, SG_batchSize=100),
+             nObs=length(yTrain))
> 
> ## ---------- neither function nor gradient
> try(res <- maxSGA(start=start,
+                   control=list(printLevel=0, iterlim=10, SG_batchSize=100),
+                   nObs=length(yTrain))
+     )
Error in maxSGA(start = start, control = list(printLevel = 0, iterlim = 10,  : 
  maxSGA/maxAdam requires at least 'fn' or 'grad' to be supplied
> 
> ## ---------- 1D case
> N1 <- 1000
> t <- rexp(N1, 2)
> loglik1 <- function(theta, index) sum(log(theta) - theta*t[index])
> gradlik1 <- function(theta, index) sum(1/theta - t[index])
> res <- maxSGA(loglik1, gradlik1, start=1,
+               control=list(iterlim=300, SG_batchSize=20), nObs=length(t))
> expect_equal(coef(res), 1/mean(t), tolerance=0.2)
> expect_null(hessian(res))
> 
> ## ---------- 2. SGA with momentum
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SG_batchSize=100, SG_learningRate=0.1, SGA_momentum=0.9),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- 3. full batch
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SG_batchSize=NULL, SG_learningRate=0.1),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- 4. no gradient
> res <- maxSGA(loglik, start=start,
+               control=list(iterlim=1000, SG_learningRate=0.02), nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- return Hessian, gradient provided
> res <- maxSGA(loglik, gradlik, start=start,
+               control=list(iterlim=1000, SG_learningRate=0.02),
+               nObs=length(yTrain),
+               finalHessian=TRUE)
> expect_equal(coef(res), b0, tolerance=tol)
> expect_equal(dim(hessian(res)), c(2,2))
> 
> ## ---------- return Hessian, no gradient
> res <- maxSGA(loglik, start=start,
+               control=list(iterlim=1000, SG_learningRate=0.02),
+               nObs=length(yTrain),
+               finalHessian=TRUE)
> expect_equal(coef(res), b0, tolerance=tol)
> expect_equal(dim(hessian(res)), c(2,2))
> 
> ### ---------- SGA, printlevel 1, storeValues ----------
> ### it should just work
> res <- maxSGA(loglik, gradlik, start=start,
+               control=list(iterlim=2, storeValues=TRUE, printLevel=1),
+               nObs=length(yTrain),
+               finalHessian=TRUE)
Initial function value: -34.64061 
--------------
Iteration limit exceeded. 
2  iterations
estimate: 6.83044305069514, 6.61130329662377 
Function value: -14.086 
> 
> ### ---------- SGA, NA as iterlim ----------
> ### should give informative error
> try(res <- maxSGA(loglik, gradlik, start=start,
+                   control=list(iterlim=NA),
+                   nObs=length(yTrain),
+                   finalHessian=TRUE)
+     )
Error in validObject(x) : 
  invalid class "MaxControl" object: NA in 'iterlim'
> 
> ### ---------- SGA, fn missing but storeValues=TRUE
> ### should give informative error
> try(res <- maxSGA(grad=gradlik, start=start,
+                   control=list(iterlim=10, storeValues=TRUE),
+                   nObs=length(yTrain))
+                   )
Error in fn(start, fixed = fixed, sumObs = TRUE, index = index, ...) : 
  Cannot compute the objective function value: no objective function supplied
> 
> ## ---------- gradient by observations
> gradlikO <- function(beta, index) {
+    e <- yTrain[index] - XTrain[index,,drop=FALSE] %*% beta
+    g <- -2*drop(e)*XTrain[index,,drop=FALSE]
+    -g/length(index)
+ }
> res <- maxSGA(grad=gradlikO, start=start,
+               control=list(printLevel=0, iterlim=100,
+                            SG_batchSize=100),
+               nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- 0 observations
> res <- maxSGA(grad=gradlik, start=start,
+               control=list(iterlim=0),
+               nObs=length(yTrain))
> expect_equal(coef(res), start)
>                            # should return start values exactly
> 
> ### -------------------- create unequally scaled data
> set.seed(1)
> N <- 1000
> x <- rnorm(N, sd=100)
> XTrain <- cbind(1, x)
> yTrain <- 1 + x + rnorm(N)
> start <- c(const=10, x=10)
> 
> ## ---------- no gradient clipping:
> ## should fail with informative "NA/Inf in gradient" message
> try(res <- maxSGA(loglik, gradlik, start=start,
+                   control=list(iterlim=100, SG_learningRate=0.5),
+                   nObs=length(yTrain))
+     )
Iteration 75 
Parameter:
[1] 1.56787659379279e+299, -1.44027448056927e+303
Gradient:
               [,1] [,2]
[1,] -3.355618e+303  Inf
Error in maxSGACompute(fn = function (theta, fnOrig, gradOrig, hessOrig,  : 
  NA/Inf in gradient
> 
> ## ---------- gradient clipping: should not fail
> res <- maxSGA(loglik, gradlik, start=start,
+               control=list(iterlim=100, SG_learningRate=0.5,
+                            SG_clip=1e6),
+               nObs=length(yTrain)
+               )
> 
> proc.time()
   user  system elapsed 
  2.207   0.555   1.889 
