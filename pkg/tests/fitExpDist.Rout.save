
R version 2.11.1 (2010-05-31)
Copyright (C) 2010 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## load the maxLik package
> library( maxLik )
Loading required package: miscTools
> 
> ## fitting an exponential distribution by ML,
> ## e.g. estimation of an exponential duration model
> 
> # generate data
> set.seed( 4 )
> t <- rexp( 100, 2 )
> 
> # log-likelihood function, gradient, and Hessian
> loglik <- function(theta) log(theta) - theta*t
> loglikSum <- function(theta) sum( log(theta) - theta*t )
> gradlik <- function(theta) 1/theta - t
> gradlikSum <- function(theta) sum( 1/theta - t )
> hesslik <- function(theta) -100/theta^2
> 
> 
> ## NR estimation
> # Estimate with only function values
> ml <- maxLik( loglik, start = 1 )
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( ml )
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21163  9.9982 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( ml )
[1] 100
> print.default( ml )
$maximum
[1] -25.05386

$estimate
[1] 2.11586

$gradient
[1] -4.226064e-07

$hessian
          [,1]
[1,] -22.32881

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$activePar
[1] TRUE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.386820732
  [2,] -1.679351430
  [3,]  0.038568223
  [4,]  0.071297531
  [5,]  0.159046822
  [6,]  0.105191904
  [7,]  0.248215144
  [8,]  0.447271096
  [9,]  0.217946010
 [10,]  0.054046205
 [11,] -0.867527906
 [12,]  0.328582586
 [13,]  0.270226226
 [14,]  0.258112502
 [15,]  0.302819650
 [16,] -0.051988008
 [17,]  0.442843502
 [18,]  0.405508825
 [19,] -0.447366189
 [20,] -0.033385385
 [21,]  0.350564550
 [22,] -0.150788622
 [23,] -2.297263126
 [24,]  0.388690932
 [25,] -0.444122556
 [26,]  0.443407897
 [27,]  0.276872846
 [28,] -0.151172549
 [29,]  0.226692187
 [30,]  0.192216403
 [31,] -0.216352490
 [32,] -0.427312137
 [33,] -0.415672379
 [34,]  0.278198838
 [35,] -0.636970375
 [36,]  0.394516796
 [37,]  0.344060904
 [38,] -0.620259683
 [39,]  0.457767061
 [40,]  0.167203749
 [41,]  0.353775670
 [42,] -0.065340674
 [43,]  0.147748185
 [44,]  0.282720509
 [45,] -0.015242583
 [46,]  0.079882297
 [47,]  0.274371824
 [48,]  0.452304205
 [49,] -1.144888511
 [50,]  0.405280557
 [51,] -0.227729890
 [52,]  0.433252226
 [53,]  0.081373328
 [54,] -0.081125852
 [55,] -0.739938588
 [56,]  0.207183328
 [57,]  0.113523147
 [58,]  0.119192731
 [59,]  0.342989794
 [60,]  0.093240217
 [61,]  0.440175137
 [62,] -0.073023347
 [63,] -0.501036837
 [64,]  0.075378934
 [65,] -0.172199936
 [66,]  0.045446558
 [67,] -0.025803408
 [68,]  0.181706949
 [69,]  0.447988616
 [70,] -0.160098118
 [71,]  0.439821881
 [72,]  0.248287374
 [73,]  0.403098273
 [74,] -0.190732568
 [75,] -0.472651009
 [76,] -0.065057871
 [77,] -0.455150234
 [78,]  0.159506492
 [79,]  0.376818614
 [80,]  0.121605787
 [81,]  0.301920770
 [82,] -0.001157201
 [83,]  0.414118363
 [84,]  0.400994281
 [85,]  0.349288508
 [86,] -0.996985142
 [87,]  0.378740644
 [88,]  0.385031341
 [89,] -0.316835771
 [90,]  0.192621239
 [91,]  0.328718423
 [92,] -0.042173033
 [93,]  0.060583989
 [94,] -0.644872469
 [95,] -0.632559546
 [96,] -0.356326819
 [97,] -0.323978779
 [98,]  0.220528780
 [99,] -0.832596230
[100,]  0.361128739

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> # log-likelihood value summed over all observations
> mlSum <- maxLik( loglikSum, start = 1 )
> all.equal( mlSum[], ml[-11] )
[1] "Component 3: Mean relative difference: 0.02497439"  
[2] "Component 4: Mean relative difference: 0.0001590837"
> 
> # Estimate with analytic gradient
> mlg <- maxLik( loglik, gradlik, start = 1 )
> nObs( mlg )
[1] 100
> all.equal( mlg, ml )
[1] "Component 3: Mean relative difference: 7.974163"    
[2] "Component 4: Mean relative difference: 0.0003697648"
> # gradient summed over all observations
> mlgSum <- maxLik( loglikSum, gradlikSum, start = 1 )
> all.equal( mlgSum[], mlg[-11] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian
> mlgh <- maxLik( loglik, gradlik, hesslik, start = 1 )
> all.equal( mlgh, mlg )
[1] "Component 3: Mean relative difference: 3.100045e-06"
> 
> 
> ## BHHH estimation
> # Estimate with only function values
> mlBhhh <- maxLik( loglik, start = 1, method = "BHHH" )
> print( mlBhhh )
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBhhh )
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21453  9.8629 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBhhh )
[1] 100
> all.equal( mlBhhh[ -c( 5, 6, 10 ) ], ml[ -c( 5, 6, 10 ) ] )
[1] "Component 2: Mean relative difference: 3.584792e-08"
[2] "Component 3: Mean relative difference: 1.330612"    
[3] "Component 4: Mean relative difference: 0.02761294"  
[4] "Component 8: Mean relative difference: 5.009301e-08"
> 
> # Estimate with analytic gradient
> mlgBhhh <- maxLik( loglik, gradlik, start = 1, method = "BHHH" )
> nObs( mlgBhhh )
[1] 100
> all.equal( mlgBhhh, mlBhhh )
[1] "Component 3: Mean relative difference: 0.006972723"
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBhhh <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BHHH" )
> all.equal( mlghBhhh, mlgBhhh )
[1] TRUE
> 
> ## BFGS estimation
> # Estimate with only function values
> mlBfgs <- maxLik( loglik, start = 1, method = "BFGS" )
> print( mlBfgs )
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBfgs )
--------------------------------------------
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21154  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBfgs )
[1] 100
> all.equal( mlBfgs[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 2.972020e-08"
[2] "Component 3: Mean relative difference: 1.432754"    
[3] "Component 4: Mean relative difference: 0.0007949126"
[4] "Component 7: Mean relative difference: 4.120738e-08"
> # log-likelihood value summed over all observations
> mlSumBfgs <- maxLik( loglikSum, start = 1, method = "BFGS" )
> all.equal( mlSumBfgs[], mlBfgs[-12] )
[1] "Component 3: Mean relative difference: 0.003193431"
> 
> # Estimate with analytic gradient
> mlgBfgs <- maxLik( loglik, gradlik, start = 1, method = "BFGS" )
> nObs( mlgBfgs )
[1] 100
> all.equal( mlgBfgs, mlBfgs )
[1] "Component 3: Mean relative difference: 0.003724576" 
[2] "Component 4: Mean relative difference: 0.0004254472"
> # gradient summed over all observations
> mlgSumBfgs <- maxLik( loglikSum, gradlikSum, start = 1, method = "BFGS" )
> all.equal( mlgSumBfgs[], mlgBfgs[-12] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBfgs <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BFGS" )
> all.equal( mlghBfgs, mlgBfgs )
[1] TRUE
> 
> 
> ## NM estimation
> # Estimate with only function values
> mlNm <- maxLik( loglik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> print( mlNm )
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.116016 
> summary( mlNm )
--------------------------------------------
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11602    0.21154  10.003 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlNm )
[1] 100
> all.equal( mlNm[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 7.349382e-05"
[2] "Component 3: Mean relative difference: 0.9998783"   
[3] "Component 4: Mean relative difference: 0.0007949126"
[4] "Component 7: Mean relative difference: 0.0001023017"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgNm <- maxLik( loglik, gradlik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> nObs( mlgNm )
[1] 100
> all.equal( mlgNm, mlNm )
[1] "Component 3: Mean relative difference: 1.818244e-07"
[2] "Component 4: Mean relative difference: 0.000572573" 
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghNm <- maxLik( loglik, gradlik, hesslik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> all.equal( mlghNm, mlgNm )
[1] TRUE
> 
> ## SANN estimation
> # Estimate with only function values
> mlSann <- maxLik( loglik, start = 1, method = "SANN" )
> print( mlSann )
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.115882 
> summary( mlSann )
--------------------------------------------
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11588    0.21154  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlSann )
[1] 100
> all.equal( mlSann[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 1.012146e-05"
[2] "Component 3: Mean relative difference: 0.9991173"   
[3] "Component 4: Mean relative difference: 0.0007949126"
[4] "Component 7: Mean relative difference: 1.408849e-05"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgSann <- maxLik( loglik, gradlik, start = 1, method = "SANN" )
> nObs( mlgSann )
[1] 100
> all.equal( mlgSann, mlSann )
[1] "Component 3: Mean relative difference: 6.125469e-06"
[2] "Component 4: Mean relative difference: 0.0004457583"
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghSann <- maxLik( loglik, gradlik, hesslik, start = 1, method = "SANN" )
> all.equal( mlghSann, mlgSann )
[1] TRUE
> 
> 
> ## test for method "estfun"
> library( sandwich )
Loading required package: zoo
> try( estfun( mlSum ) )
Error in estfun.maxLik(mlSum) : 
  cannot return the gradients of the log-likelihood function evaluated at each observation: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> estfun( ml )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682073
[2,] -1.67935143
[3,]  0.03856822
[4,]  0.07129753
[5,]  0.15904682
> estfun( mlg )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682074
[2,] -1.67935142
[3,]  0.03856823
[4,]  0.07129754
[5,]  0.15904683
> estfun( mlBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlgBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlgBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlgNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> estfun( mlgSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> 
> 
> ## test for method "bread"
> try( bread( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> bread( ml )
        [,1]
[1,] 4.47852
> bread( mlg )
         [,1]
[1,] 4.476864
> bread( mlBhhh )
         [,1]
[1,] 4.602185
> bread( mlgBhhh )
         [,1]
[1,] 4.602185
> bread( mlBfgs )
        [,1]
[1,] 4.47496
> bread( mlgBfgs )
         [,1]
[1,] 4.476864
> bread( mlNm )
        [,1]
[1,] 4.47496
> bread( mlgNm )
         [,1]
[1,] 4.477522
> bread( mlSann )
        [,1]
[1,] 4.47496
> bread( mlgSann )
         [,1]
[1,] 4.476955
> 
> 
> ## test for method "sandwich"
> try( sandwich( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> printSandwich <- function( x ) {
+    print( sandwich( x ) )
+    print( all.equal( sandwich( x ), vcov( x ) ) )
+ }
> printSandwich( ml )
           [,1]
[1,] 0.04358178
[1] "Mean relative difference: 0.02761294"
> printSandwich( mlg )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799305"
> printSandwich( mlBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlgBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlBfgs )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843045"
> printSandwich( mlgBfgs )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799309"
> printSandwich( mlNm )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843044"
> printSandwich( mlgNm )
           [,1]
[1,] 0.04356236
[1] "Mean relative difference: 0.02784193"
> printSandwich( mlSann )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843045"
> printSandwich( mlgSann )
           [,1]
[1,] 0.04355132
[1] "Mean relative difference: 0.02797222"
> 
