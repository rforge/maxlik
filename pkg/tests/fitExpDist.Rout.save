
R version 2.11.0 (2010-04-22)
Copyright (C) 2010 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## load the maxLik package
> library( maxLik )
> 
> ## fitting an exponential distribution by ML,
> ## e.g. estimation of an exponential duration model
> 
> # generate data
> set.seed( 4 )
> t <- rexp( 100, 2 )
> 
> # log-likelihood function, gradient, and Hessian
> loglik <- function(theta) log(theta) - theta*t
> loglikSum <- function(theta) sum( log(theta) - theta*t )
> gradlik <- function(theta) 1/theta - t
> gradlikSum <- function(theta) sum( 1/theta - t )
> hesslik <- function(theta) -100/theta^2
> 
> 
> ## NR estimation
> # Estimate with only function values
> ml <- maxLik( loglik, start = 1 )
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( ml )
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21163  9.9982 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( ml )
[1] 100
> print.default( ml )
$maximum
[1] -25.05386

$estimate
[1] 2.11586

$gradient
[1] 7.354117e-07

$hessian
          [,1]
[1,] -22.32881

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$activePar
[1] TRUE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.386820744
  [2,] -1.679351418
  [3,]  0.038568234
  [4,]  0.071297543
  [5,]  0.159046834
  [6,]  0.105191916
  [7,]  0.248215156
  [8,]  0.447271107
  [9,]  0.217946022
 [10,]  0.054046217
 [11,] -0.867527895
 [12,]  0.328582598
 [13,]  0.270226238
 [14,]  0.258112513
 [15,]  0.302819662
 [16,] -0.051987996
 [17,]  0.442843513
 [18,]  0.405508836
 [19,] -0.447366177
 [20,] -0.033385373
 [21,]  0.350564562
 [22,] -0.150788611
 [23,] -2.297263115
 [24,]  0.388690944
 [25,] -0.444122545
 [26,]  0.443407909
 [27,]  0.276872858
 [28,] -0.151172538
 [29,]  0.226692198
 [30,]  0.192216415
 [31,] -0.216352479
 [32,] -0.427312126
 [33,] -0.415672368
 [34,]  0.278198850
 [35,] -0.636970364
 [36,]  0.394516808
 [37,]  0.344060916
 [38,] -0.620259670
 [39,]  0.457767072
 [40,]  0.167203761
 [41,]  0.353775681
 [42,] -0.065340662
 [43,]  0.147748196
 [44,]  0.282720520
 [45,] -0.015242571
 [46,]  0.079882308
 [47,]  0.274371836
 [48,]  0.452304216
 [49,] -1.144888499
 [50,]  0.405280569
 [51,] -0.227729879
 [52,]  0.433252238
 [53,]  0.081373340
 [54,] -0.081125841
 [55,] -0.739938576
 [56,]  0.207183339
 [57,]  0.113523158
 [58,]  0.119192742
 [59,]  0.342989805
 [60,]  0.093240228
 [61,]  0.440175149
 [62,] -0.073023336
 [63,] -0.501036825
 [64,]  0.075378945
 [65,] -0.172199925
 [66,]  0.045446570
 [67,] -0.025803397
 [68,]  0.181706961
 [69,]  0.447988628
 [70,] -0.160098107
 [71,]  0.439821893
 [72,]  0.248287386
 [73,]  0.403098285
 [74,] -0.190732557
 [75,] -0.472650997
 [76,] -0.065057859
 [77,] -0.455150222
 [78,]  0.159506503
 [79,]  0.376818626
 [80,]  0.121605798
 [81,]  0.301920782
 [82,] -0.001157190
 [83,]  0.414118375
 [84,]  0.400994292
 [85,]  0.349288519
 [86,] -0.996985131
 [87,]  0.378740655
 [88,]  0.385031353
 [89,] -0.316835760
 [90,]  0.192621251
 [91,]  0.328718435
 [92,] -0.042173022
 [93,]  0.060584000
 [94,] -0.644872457
 [95,] -0.632559534
 [96,] -0.356326807
 [97,] -0.323978768
 [98,]  0.220528791
 [99,] -0.832596218
[100,]  0.361128751

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> # log-likelihood value summed over all observations
> mlSum <- maxLik( loglikSum, start = 1 )
> all.equal( mlSum[], ml[-11] )
[1] TRUE
> 
> # Estimate with analytic gradient
> mlg <- maxLik( loglik, gradlik, start = 1 )
> nObs( mlg )
[1] 100
> all.equal( mlg, ml )
[1] "Component 3: Mean relative difference: 11.13631"     
[2] "Component 4: Mean relative difference: 0.0003697648" 
[3] "Component 11: Mean relative difference: 1.987437e-08"
> # gradient summed over all observations
> mlgSum <- maxLik( loglikSum, gradlikSum, start = 1 )
> all.equal( mlgSum[], mlg[-11] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian
> mlgh <- maxLik( loglik, gradlik, hesslik, start = 1 )
> all.equal( mlgh, mlg )
[1] "Component 3: Mean relative difference: 3.100045e-06"
> 
> 
> ## BHHH estimation
> # Estimate with only function values
> mlBhhh <- maxLik( loglik, start = 1, method = "BHHH" )
> print( mlBhhh )
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBhhh )
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21453  9.8629 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBhhh )
[1] 100
> all.equal( mlBhhh[ -c( 5, 6, 10 ) ], ml[ -c( 5, 6, 10 ) ] )
[1] "Component 3: Mean relative difference: 0.4246754"   
[2] "Component 4: Mean relative difference: 0.02761294"  
[3] "Component 8: Mean relative difference: 1.598758e-08"
> 
> # Estimate with analytic gradient
> mlgBhhh <- maxLik( loglik, gradlik, start = 1, method = "BHHH" )
> nObs( mlgBhhh )
[1] 100
> all.equal( mlgBhhh, mlBhhh )
[1] "Component 3: Mean relative difference: 0.006972723"
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBhhh <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BHHH" )
> all.equal( mlghBhhh, mlgBhhh )
[1] TRUE
> 
> ## BFGS estimation
> # Estimate with only function values
> mlBfgs <- maxLik( loglik, start = 1, method = "BFGS" )
> print( mlBfgs )
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBfgs )
--------------------------------------------
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21154  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBfgs )
[1] 100
> all.equal( mlBfgs[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 3: Mean relative difference: 0.2469304"   
[2] "Component 4: Mean relative difference: 0.0007949126"
> # log-likelihood value summed over all observations
> mlSumBfgs <- maxLik( loglikSum, start = 1, method = "BFGS" )
> all.equal( mlSumBfgs[], mlBfgs[-12] )
[1] "Component 3: Mean relative difference: 0.003193431"
> 
> # Estimate with analytic gradient
> mlgBfgs <- maxLik( loglik, gradlik, start = 1, method = "BFGS" )
> nObs( mlgBfgs )
[1] 100
> all.equal( mlgBfgs, mlBfgs )
[1] "Component 3: Mean relative difference: 0.003724576" 
[2] "Component 4: Mean relative difference: 0.0004254472"
> # gradient summed over all observations
> mlgSumBfgs <- maxLik( loglikSum, gradlikSum, start = 1, method = "BFGS" )
> all.equal( mlgSumBfgs[], mlgBfgs[-12] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBfgs <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BFGS" )
> all.equal( mlghBfgs, mlgBfgs )
[1] TRUE
> 
> ## NM estimation
> # Estimate with only function values
> mlNm <- maxLik( loglik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> print( mlNm )
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.116016 
> summary( mlNm )
--------------------------------------------
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11602    0.21154  10.003 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlNm )
[1] 100
> all.equal( mlNm[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 7.351838e-05"
[2] "Component 3: Mean relative difference: 1.000212"    
[3] "Component 4: Mean relative difference: 0.0007949126"
[4] "Component 7: Mean relative difference: 0.0001023358"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgNm <- maxLik( loglik, gradlik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> nObs( mlgNm )
[1] 100
> all.equal( mlgNm, mlNm )
[1] "Component 3: Mean relative difference: 1.818244e-07"
[2] "Component 4: Mean relative difference: 0.000572573" 
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghNm <- maxLik( loglik, gradlik, hesslik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> all.equal( mlghNm, mlgNm )
[1] TRUE
> 
> ## SANN estimation
> # Estimate with only function values
> mlSann <- maxLik( loglik, start = 1, method = "SANN" )
> print( mlSann )
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.115882 
> summary( mlSann )
--------------------------------------------
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11588    0.21154  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlSann )
[1] 100
> all.equal( mlSann[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 1.014602e-05"
[2] "Component 3: Mean relative difference: 1.001536"    
[3] "Component 4: Mean relative difference: 0.0007949126"
[4] "Component 7: Mean relative difference: 1.41226e-05" 
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgSann <- maxLik( loglik, gradlik, start = 1, method = "SANN" )
> nObs( mlgSann )
[1] 100
> all.equal( mlgSann, mlSann )
[1] "Component 3: Mean relative difference: 6.125469e-06"
[2] "Component 4: Mean relative difference: 0.0004457583"
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghSann <- maxLik( loglik, gradlik, hesslik, start = 1, method = "SANN" )
> all.equal( mlghSann, mlgSann )
[1] TRUE
> 
> 
> ## test for method "estfun"
> library( sandwich )
Loading required package: zoo
> try( estfun( mlSum ) )
Error in estfun.maxLik(mlSum) : 
  cannot return the gradients of the log-likelihood function evaluated at each observation: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> estfun( ml )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682074
[2,] -1.67935142
[3,]  0.03856823
[4,]  0.07129754
[5,]  0.15904683
> estfun( mlg )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682074
[2,] -1.67935142
[3,]  0.03856823
[4,]  0.07129754
[5,]  0.15904683
> estfun( mlBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlgBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlgBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlgNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> estfun( mlgSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> 
> 
> ## test for method "bread"
> try( bread( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> bread( ml )
        [,1]
[1,] 4.47852
> bread( mlg )
         [,1]
[1,] 4.476864
> bread( mlBhhh )
         [,1]
[1,] 4.602185
> bread( mlgBhhh )
         [,1]
[1,] 4.602185
> bread( mlBfgs )
        [,1]
[1,] 4.47496
> bread( mlgBfgs )
         [,1]
[1,] 4.476864
> bread( mlNm )
        [,1]
[1,] 4.47496
> bread( mlgNm )
         [,1]
[1,] 4.477522
> bread( mlSann )
        [,1]
[1,] 4.47496
> bread( mlgSann )
         [,1]
[1,] 4.476955
> 
> 
> ## test for method "sandwich"
> try( sandwich( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> printSandwich <- function( x ) {
+    print( sandwich( x ) )
+    print( all.equal( sandwich( x ), vcov( x ) ) )
+ }
> printSandwich( ml )
           [,1]
[1,] 0.04358178
[1] "Mean relative difference: 0.02761294"
> printSandwich( mlg )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799305"
> printSandwich( mlBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlgBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlBfgs )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843045"
> printSandwich( mlgBfgs )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799309"
> printSandwich( mlNm )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843044"
> printSandwich( mlgNm )
           [,1]
[1,] 0.04356236
[1] "Mean relative difference: 0.02784193"
> printSandwich( mlSann )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843045"
> printSandwich( mlgSann )
           [,1]
[1,] 0.04355132
[1] "Mean relative difference: 0.02797222"
> 
