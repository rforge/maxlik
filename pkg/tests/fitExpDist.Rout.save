
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## load the maxLik package
> library( maxLik )
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> 
> ## fitting an exponential distribution by ML,
> ## e.g. estimation of an exponential duration model
> 
> # generate data
> options(digits=4)
>                            # less differences b/w different platforms
> set.seed( 4 )
> t <- rexp( 100, 2 )
> 
> # log-likelihood function, gradient, and Hessian
> loglik <- function(theta) log(theta) - theta*t
> loglikSum <- function(theta) sum( log(theta) - theta*t )
> gradlik <- function(theta) 1/theta - t
> gradlikSum <- function(theta) sum( 1/theta - t )
> hesslik <- function(theta) -100/theta^2
> 
> 
> ## NR estimation
> # Estimate with only function values
> ml <- maxLik( loglik, start = 1 )
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> summary( ml )
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> nObs( ml )
[1] 100
> print.default( ml[ -3 ] )  # no gradient
$maximum
[1] -25.05

$estimate
[1] 2.116

$hessian
       [,1]
[1,] -22.33

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
            [,1]
  [1,]  0.386821
  [2,] -1.679351
  [3,]  0.038568
  [4,]  0.071298
  [5,]  0.159047
  [6,]  0.105192
  [7,]  0.248215
  [8,]  0.447271
  [9,]  0.217946
 [10,]  0.054046
 [11,] -0.867528
 [12,]  0.328583
 [13,]  0.270226
 [14,]  0.258113
 [15,]  0.302820
 [16,] -0.051988
 [17,]  0.442844
 [18,]  0.405509
 [19,] -0.447366
 [20,] -0.033385
 [21,]  0.350565
 [22,] -0.150789
 [23,] -2.297263
 [24,]  0.388691
 [25,] -0.444123
 [26,]  0.443408
 [27,]  0.276873
 [28,] -0.151173
 [29,]  0.226692
 [30,]  0.192216
 [31,] -0.216352
 [32,] -0.427312
 [33,] -0.415672
 [34,]  0.278199
 [35,] -0.636970
 [36,]  0.394517
 [37,]  0.344061
 [38,] -0.620260
 [39,]  0.457767
 [40,]  0.167204
 [41,]  0.353776
 [42,] -0.065341
 [43,]  0.147748
 [44,]  0.282721
 [45,] -0.015243
 [46,]  0.079882
 [47,]  0.274372
 [48,]  0.452304
 [49,] -1.144889
 [50,]  0.405281
 [51,] -0.227730
 [52,]  0.433252
 [53,]  0.081373
 [54,] -0.081126
 [55,] -0.739939
 [56,]  0.207183
 [57,]  0.113523
 [58,]  0.119193
 [59,]  0.342990
 [60,]  0.093240
 [61,]  0.440175
 [62,] -0.073023
 [63,] -0.501037
 [64,]  0.075379
 [65,] -0.172200
 [66,]  0.045447
 [67,] -0.025803
 [68,]  0.181707
 [69,]  0.447989
 [70,] -0.160098
 [71,]  0.439822
 [72,]  0.248287
 [73,]  0.403098
 [74,] -0.190733
 [75,] -0.472651
 [76,] -0.065058
 [77,] -0.455150
 [78,]  0.159506
 [79,]  0.376819
 [80,]  0.121606
 [81,]  0.301921
 [82,] -0.001157
 [83,]  0.414118
 [84,]  0.400994
 [85,]  0.349289
 [86,] -0.996985
 [87,]  0.378741
 [88,]  0.385031
 [89,] -0.316836
 [90,]  0.192621
 [91,]  0.328718
 [92,] -0.042173
 [93,]  0.060584
 [94,] -0.644872
 [95,] -0.632560
 [96,] -0.356327
 [97,] -0.323979
 [98,]  0.220529
 [99,] -0.832596
[100,]  0.361129

$control
A 'MaxControl' object with slots:
tol = 1e-08 
reltol = 1.49e-08 
gradtol = 1e-06 
steptol = 1e-10 
lambdatol = 1e-06 
qrtol = 1e-10 
qac = stephalving 
lambda0 = 0.01 
lambdaStep = 2 
maxLambda = 1e+12 
nm_alpha = 1 
nm_beta = 0.5 
nm_gamma = 2 
sann_cand = <default Gaussian Markov kernel>
sann_temp = 10 
sann_tmax = 10 
sann_randomSeed = 123 
iterlim = 150 
printLevel = 0 

> class( ml )
[1] "maxLik" "maxim"  "list"  
> # log-likelihood value summed over all observations
> mlSum <- maxLik( loglikSum, start = 1 )
> all.equal( mlSum[], ml[-11], tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient
> mlg <- maxLik( loglik, gradlik, start = 1 )
> nObs( mlg )
[1] 100
> all.equal( mlg, ml, tolerance = 1e-3 )
[1] TRUE
> # gradient summed over all observations
> mlgSum <- maxLik( loglikSum, gradlikSum, start = 1 )
> all.equal( mlgSum[], mlg[-11], tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian
> mlgh <- maxLik( loglik, gradlik, hesslik, start = 1 )
> all.equal( mlgh, mlg, tolerance = 1e-3 )
[1] TRUE
> 
> 
> ## BHHH estimation
> # Estimate with only function values
> mlBhhh <- maxLik( loglik, start = 1, method = "BHHH" )
> print( mlBhhh )
Maximum Likelihood estimation
BHHH maximisation, 4 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> summary( mlBhhh )
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 4 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.215    9.86  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> nObs( mlBhhh )
[1] 100
> all.equal( mlBhhh[ -c( 4, 5, 6, 10 ) ], ml[ -c( 4, 5, 6, 10 ) ], 
+    check.attributes = FALSE, tolerance = 1e-3 )
[1] "Component \"iterations\": Mean relative difference: 0.25"
> all.equal( mlBhhh[ 4 ], ml[ 4 ],  # hessian
+    check.attributes = FALSE, tolerance = 5e-2 )
[1] TRUE
> 
> # Estimate with analytic gradient
> mlgBhhh <- maxLik( loglik, gradlik, start = 1, method = "BHHH" )
> nObs( mlgBhhh )
[1] 100
> all.equal( mlgBhhh, mlBhhh, tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBhhh <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BHHH" )
> all.equal( mlghBhhh, mlgBhhh, tolerance = 1e-3 )
[1] TRUE
> 
> ## BFGS estimation
> # Estimate with only function values
> mlBfgs <- maxLik( loglik, start = 1, method = "BFGS" )
> print( mlBfgs )
Maximum Likelihood estimation
BFGS maximization, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> summary( mlBfgs )
--------------------------------------------
Maximum Likelihood estimation
BFGS maximization, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> nObs( mlBfgs )
[1] 100
> all.equal( mlBfgs[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ], 
+    tolerance = 1e-3 )
[1] "Component \"control\": Attributes: < Component \"iterlim\": Mean relative difference: 0.25 >"
> # log-likelihood value summed over all observations
> mlSumBfgs <- maxLik( loglikSum, start = 1, method = "BFGS" )
> all.equal( mlSumBfgs[], mlBfgs[-12], tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient
> mlgBfgs <- maxLik( loglik, gradlik, start = 1, method = "BFGS" )
> nObs( mlgBfgs )
[1] 100
> all.equal( mlgBfgs, mlBfgs, tolerance = 1e-3 )
[1] TRUE
> # gradient summed over all observations
> mlgSumBfgs <- maxLik( loglikSum, gradlikSum, start = 1, method = "BFGS" )
> all.equal( mlgSumBfgs[], mlgBfgs[-12], tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBfgs <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BFGS" )
> all.equal( mlghBfgs, mlgBfgs, tolerance = 1e-3 )
[1] TRUE
> 
> 
> ## NM estimation
> # Estimate with only function values
> mlNm <- maxLik( loglik, start = 1, method = "NM" )
Warning message:
In optim(par = 1, fn = function (theta, fnOrig, gradOrig, hessOrig,  :
  one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> print( mlNm )
Maximum Likelihood estimation
Nelder-Mead maximization, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> summary( mlNm )
--------------------------------------------
Maximum Likelihood estimation
Nelder-Mead maximization, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> nObs( mlNm )
[1] 100
> all.equal( mlNm[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ], 
+    tolerance = 1e-3 )
[1] "Component \"gradient\": Mean relative difference: 0.9999"                                   
[2] "Component \"control\": Attributes: < Component \"iterlim\": Mean relative difference: 0.7 >"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgNm <- maxLik( loglik, gradlik, start = 1, method = "NM" )
Warning message:
In optim(par = 1, fn = function (theta, fnOrig, gradOrig, hessOrig,  :
  one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> nObs( mlgNm )
[1] 100
> all.equal( mlgNm, mlNm, tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghNm <- maxLik( loglik, gradlik, hesslik, start = 1, method = "NM" )
Warning message:
In optim(par = 1, fn = function (theta, fnOrig, gradOrig, hessOrig,  :
  one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> all.equal( mlghNm, mlgNm, tolerance = 1e-3 )
[1] TRUE
> 
> ## SANN estimation
> # Estimate with only function values
> mlSann <- maxLik( loglik, start = 1, method = "SANN" )
> print( mlSann )
Maximum Likelihood estimation
SANN maximization, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 (1 free parameter(s))
Estimate(s): 2.116 
> summary( mlSann )
--------------------------------------------
Maximum Likelihood estimation
SANN maximization, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> nObs( mlSann )
[1] 100
> all.equal( mlSann[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ], 
+    tolerance = 1e-3 )
[1] "Component \"control\": Attributes: < Component \"iterlim\": Mean relative difference: 0.985 >"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgSann <- maxLik( loglik, gradlik, start = 1, method = "SANN" )
> nObs( mlgSann )
[1] 100
> all.equal( mlgSann, mlSann, tolerance = 1e-3 )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghSann <- maxLik( loglik, gradlik, hesslik, start = 1, method = "SANN" )
> all.equal( mlghSann, mlgSann, tolerance = 1e-3 )
[1] TRUE
> 
> 
> ## CG estimation
> # Estimate with only function values
> mlCg <- maxLik( loglik, start = 1, method = "CG" )
> print(summary( mlCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximization, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> # Estimate with analytic gradient
> mlgCg <- maxLik( loglik, gradlik, start = 1, method = "CG" )
> print(summary( mlgCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximization, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> # Estimate with analytic gradient and Hessian (not used for estimation)
> mlghCg <- maxLik( loglik, gradlik, hesslik, start = 1, method = "CG" )
> print(summary( mlghCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximization, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> 
> ## test for method "estfun"
> library( sandwich )
> try( estfun( mlSum ) )
Error in estfun.maxLik(mlSum) : 
  cannot return the gradients of the log-likelihood function evaluated at each observation: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> estfun( ml )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlg )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlBhhh )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlgBhhh )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlBfgs )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlgBfgs )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67935
[3,]  0.03857
[4,]  0.07130
[5,]  0.15905
> estfun( mlNm )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38679
[2,] -1.67939
[3,]  0.03853
[4,]  0.07126
[5,]  0.15901
> estfun( mlgNm )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38679
[2,] -1.67939
[3,]  0.03853
[4,]  0.07126
[5,]  0.15901
> estfun( mlSann )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67936
[3,]  0.03856
[4,]  0.07129
[5,]  0.15904
> estfun( mlgSann )[ 1:5, , drop = FALSE ]
         [,1]
[1,]  0.38682
[2,] -1.67936
[3,]  0.03856
[4,]  0.07129
[5,]  0.15904
> 
> 
> ## test for method "bread"
> try( bread( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> round( bread( ml ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlg ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlBhhh ), 2 )
     [,1]
[1,]  4.6
> round( bread( mlgBhhh ), 2 )
     [,1]
[1,]  4.6
> round( bread( mlBfgs ), 2 )
     [,1]
[1,] 4.47
> round( bread( mlgBfgs ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlNm ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlgNm ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlSann ), 2 )
     [,1]
[1,] 4.48
> round( bread( mlgSann ), 2 )
     [,1]
[1,] 4.48
> 
> 
> ## test for method "sandwich"
> try( sandwich( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> printSandwich <- function( x ) {
+    print( round( sandwich( x ), 4 ) )
+    print( round( sum( abs( sandwich( x ) - vcov( x ) ) ), 4 ) )
+ }
> printSandwich( ml )
       [,1]
[1,] 0.0436
[1] 0.0012
> printSandwich( mlg )
       [,1]
[1,] 0.0435
[1] 0.0012
> printSandwich( mlBhhh )
      [,1]
[1,] 0.046
[1] 0
> printSandwich( mlgBhhh )
      [,1]
[1,] 0.046
[1] 0
> printSandwich( mlBfgs )
       [,1]
[1,] 0.0435
[1] 0.0012
> printSandwich( mlgBfgs )
       [,1]
[1,] 0.0435
[1] 0.0012
> printSandwich( mlNm )
       [,1]
[1,] 0.0435
[1] 0.0012
> printSandwich( mlgNm )
       [,1]
[1,] 0.0436
[1] 0.0012
> printSandwich( mlSann )
       [,1]
[1,] 0.0436
[1] 0.0012
> printSandwich( mlgSann )
       [,1]
[1,] 0.0436
[1] 0.0012
> 
> proc.time()
   user  system elapsed 
  1.485   0.037   1.516 
