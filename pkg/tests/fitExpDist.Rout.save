
R version 2.14.1 (2011-12-22)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## load the maxLik package
> library( maxLik )
Loading required package: miscTools
> 
> ## fitting an exponential distribution by ML,
> ## e.g. estimation of an exponential duration model
> 
> # generate data
> options(digits=5)
>                            # less differences b/w different platforms
> set.seed( 4 )
> t <- rexp( 100, 2 )
> 
> # log-likelihood function, gradient, and Hessian
> loglik <- function(theta) log(theta) - theta*t
> loglikSum <- function(theta) sum( log(theta) - theta*t )
> gradlik <- function(theta) 1/theta - t
> gradlikSum <- function(theta) sum( 1/theta - t )
> hesslik <- function(theta) -100/theta^2
> 
> 
> ## NR estimation
> # Estimate with only function values
> ml <- maxLik( loglik, start = 1 )
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.1159 
> summary( ml )
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( ml )
[1] 100
> print.default( ml )
$maximum
[1] -25.054

$estimate
[1] 2.1159

$gradient
[1] -3.8042e-07

$hessian
        [,1]
[1,] -22.332

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$fixed
[1] FALSE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
             [,1]
  [1,]  0.3868207
  [2,] -1.6793514
  [3,]  0.0385682
  [4,]  0.0712975
  [5,]  0.1590468
  [6,]  0.1051919
  [7,]  0.2482151
  [8,]  0.4472711
  [9,]  0.2179460
 [10,]  0.0540462
 [11,] -0.8675279
 [12,]  0.3285826
 [13,]  0.2702262
 [14,]  0.2581125
 [15,]  0.3028197
 [16,] -0.0519880
 [17,]  0.4428435
 [18,]  0.4055088
 [19,] -0.4473662
 [20,] -0.0333854
 [21,]  0.3505646
 [22,] -0.1507886
 [23,] -2.2972631
 [24,]  0.3886909
 [25,] -0.4441226
 [26,]  0.4434079
 [27,]  0.2768728
 [28,] -0.1511725
 [29,]  0.2266922
 [30,]  0.1922164
 [31,] -0.2163525
 [32,] -0.4273121
 [33,] -0.4156724
 [34,]  0.2781988
 [35,] -0.6369704
 [36,]  0.3945168
 [37,]  0.3440609
 [38,] -0.6202597
 [39,]  0.4577671
 [40,]  0.1672037
 [41,]  0.3537757
 [42,] -0.0653407
 [43,]  0.1477482
 [44,]  0.2827205
 [45,] -0.0152426
 [46,]  0.0798823
 [47,]  0.2743718
 [48,]  0.4523042
 [49,] -1.1448885
 [50,]  0.4052806
 [51,] -0.2277299
 [52,]  0.4332522
 [53,]  0.0813733
 [54,] -0.0811259
 [55,] -0.7399386
 [56,]  0.2071833
 [57,]  0.1135231
 [58,]  0.1191927
 [59,]  0.3429898
 [60,]  0.0932402
 [61,]  0.4401751
 [62,] -0.0730233
 [63,] -0.5010368
 [64,]  0.0753789
 [65,] -0.1721999
 [66,]  0.0454466
 [67,] -0.0258034
 [68,]  0.1817069
 [69,]  0.4479886
 [70,] -0.1600981
 [71,]  0.4398219
 [72,]  0.2482874
 [73,]  0.4030983
 [74,] -0.1907326
 [75,] -0.4726510
 [76,] -0.0650579
 [77,] -0.4551502
 [78,]  0.1595065
 [79,]  0.3768186
 [80,]  0.1216058
 [81,]  0.3019208
 [82,] -0.0011572
 [83,]  0.4141184
 [84,]  0.4009943
 [85,]  0.3492885
 [86,] -0.9969851
 [87,]  0.3787406
 [88,]  0.3850313
 [89,] -0.3168358
 [90,]  0.1926212
 [91,]  0.3287184
 [92,] -0.0421730
 [93,]  0.0605840
 [94,] -0.6448725
 [95,] -0.6325595
 [96,] -0.3563268
 [97,] -0.3239788
 [98,]  0.2205288
 [99,] -0.8325962
[100,]  0.3611287

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> # log-likelihood value summed over all observations
> mlSum <- maxLik( loglikSum, start = 1 )
> all.equal( mlSum[], ml[-11] )
[1] "Component 3: Mean relative difference: 0.48766"   
[2] "Component 4: Mean relative difference: 0.00047702"
> 
> # Estimate with analytic gradient
> mlg <- maxLik( loglik, gradlik, start = 1 )
> nObs( mlg )
[1] 100
> all.equal( mlg, ml )
[1] "Component 3: Mean relative difference: 7.2779"    
[2] "Component 4: Mean relative difference: 0.00021071"
> # gradient summed over all observations
> mlgSum <- maxLik( loglikSum, gradlikSum, start = 1 )
> all.equal( mlgSum[], mlg[-11] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian
> mlgh <- maxLik( loglik, gradlik, hesslik, start = 1 )
> all.equal( mlgh, mlg )
[1] "Component 3: Mean relative difference: 3.1e-06"
> 
> 
> ## BHHH estimation
> # Estimate with only function values
> mlBhhh <- maxLik( loglik, start = 1, method = "BHHH" )
> print( mlBhhh )
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.1159 
> summary( mlBhhh )
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.215    9.86  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBhhh )
[1] 100
> all.equal( mlBhhh[ -c( 5, 6, 10 ) ], ml[ -c( 5, 6, 10 ) ] )
[1] "Component 2: Mean relative difference: 3.4951e-08"                             
[2] "Component 3: Mean relative difference: 1.2976"                                 
[3] "Component 4: Attributes: < Length mismatch: comparison on first 1 components >"
[4] "Component 4: Mean relative difference: 0.027776"                               
[5] "Component 8: Mean relative difference: 4.885e-08"                              
> 
> # Estimate with analytic gradient
> mlgBhhh <- maxLik( loglik, gradlik, start = 1, method = "BHHH" )
> nObs( mlgBhhh )
[1] 100
> all.equal( mlgBhhh, mlBhhh )
[1] "Component 3: Mean relative difference: 0.0069727"
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBhhh <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BHHH" )
> all.equal( mlghBhhh, mlgBhhh )
[1] TRUE
> 
> ## BFGS estimation
> # Estimate with only function values
> mlBfgs <- maxLik( loglik, start = 1, method = "BFGS" )
> print( mlBfgs )
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.1159 
> summary( mlBfgs )
--------------------------------------------
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBfgs )
[1] 100
> all.equal( mlBfgs[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 2.8629e-08"
[2] "Component 3: Mean relative difference: 1.3939"    
[3] "Component 4: Mean relative difference: 0.00063593"
[4] "Component 7: Mean relative difference: 3.9649e-08"
> # log-likelihood value summed over all observations
> mlSumBfgs <- maxLik( loglikSum, start = 1, method = "BFGS" )
> all.equal( mlSumBfgs[], mlBfgs[-12] )
[1] "Component 3: Mean relative difference: 0.000517"
> 
> # Estimate with analytic gradient
> mlgBfgs <- maxLik( loglik, gradlik, start = 1, method = "BFGS" )
> nObs( mlgBfgs )
[1] 100
> all.equal( mlgBfgs, mlBfgs )
[1] "Component 3: Mean relative difference: 0.0072872" 
[2] "Component 4: Mean relative difference: 0.00042545"
> # gradient summed over all observations
> mlgSumBfgs <- maxLik( loglikSum, gradlikSum, start = 1, method = "BFGS" )
> all.equal( mlgSumBfgs[], mlgBfgs[-12] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBfgs <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BFGS" )
> all.equal( mlghBfgs, mlgBfgs )
[1] TRUE
> 
> 
> ## NM estimation
> # Estimate with only function values
> mlNm <- maxLik( loglik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> print( mlNm )
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.116 
> summary( mlNm )
--------------------------------------------
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlNm )
[1] 100
> all.equal( mlNm[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 7.3495e-05"
[2] "Component 3: Mean relative difference: 0.99989"   
[3] "Component 4: Mean relative difference: 0.00047702"
[4] "Component 7: Mean relative difference: 0.0001023" 
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgNm <- maxLik( loglik, gradlik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> nObs( mlgNm )
[1] 100
> all.equal( mlgNm, mlNm )
[1] "Component 3: Mean relative difference: 1.8182e-07"
[2] "Component 4: Mean relative difference: 0.0004135" 
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghNm <- maxLik( loglik, gradlik, hesslik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly
> all.equal( mlghNm, mlgNm )
[1] TRUE
> 
> ## SANN estimation
> # Estimate with only function values
> mlSann <- maxLik( loglik, start = 1, method = "SANN" )
> print( mlSann )
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 (1 free parameter(s))
Estimate(s): 2.1159 
> summary( mlSann )
--------------------------------------------
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlSann )
[1] 100
> all.equal( mlSann[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 1.0122e-05"
[2] "Component 3: Mean relative difference: 0.99921"   
[3] "Component 7: Mean relative difference: 1.409e-05" 
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgSann <- maxLik( loglik, gradlik, start = 1, method = "SANN" )
> nObs( mlgSann )
[1] 100
> all.equal( mlgSann, mlSann )
[1] "Component 3: Mean relative difference: 6.7051e-06"
[2] "Component 4: Mean relative difference: 0.00019046"
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghSann <- maxLik( loglik, gradlik, hesslik, start = 1, method = "SANN" )
> all.equal( mlghSann, mlgSann )
[1] TRUE
> 
> 
> ## CG estimation
> # Estimate with only function values
> mlCg <- maxLik( loglik, start = 1, method = "CG" )
> print(summary( mlCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximisation, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> # Estimate with analytic gradient
> mlgCg <- maxLik( loglik, gradlik, start = 1, method = "CG" )
> print(summary( mlgCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximisation, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> # Estimate with analytic gradient and Hessian (not used for estimation)
> mlghCg <- maxLik( loglik, gradlik, hesslik, start = 1, method = "CG" )
> print(summary( mlghCg))
--------------------------------------------
Maximum Likelihood estimation
CG maximisation, 33 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.054 
1  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)    
[1,]    2.116      0.212      10  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> 
> 
> ## test for method "estfun"
> library( sandwich )
Loading required package: zoo

Attaching package: 'zoo'

The following object(s) are masked from 'package:base':

    as.Date

> try( estfun( mlSum ) )
Error in estfun.maxLik(mlSum) : 
  cannot return the gradients of the log-likelihood function evaluated at each observation: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> estfun( ml )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlg )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlBhhh )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlgBhhh )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlBfgs )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlgBfgs )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386821
[2,] -1.679351
[3,]  0.038568
[4,]  0.071298
[5,]  0.159047
> estfun( mlNm )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386786
[2,] -1.679386
[3,]  0.038533
[4,]  0.071263
[5,]  0.159012
> estfun( mlgNm )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386786
[2,] -1.679386
[3,]  0.038533
[4,]  0.071263
[5,]  0.159012
> estfun( mlSann )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386816
[2,] -1.679356
[3,]  0.038563
[4,]  0.071293
[5,]  0.159042
> estfun( mlgSann )[ 1:5, , drop = FALSE ]
          [,1]
[1,]  0.386816
[2,] -1.679356
[3,]  0.038563
[4,]  0.071293
[5,]  0.159042
> 
> 
> ## test for method "bread"
> try( bread( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> bread( ml )
       [,1]
[1,] 4.4778
> bread( mlg )
       [,1]
[1,] 4.4769
> bread( mlBhhh )
       [,1]
[1,] 4.6022
> bread( mlgBhhh )
       [,1]
[1,] 4.6022
> bread( mlBfgs )
      [,1]
[1,] 4.475
> bread( mlgBfgs )
       [,1]
[1,] 4.4769
> bread( mlNm )
       [,1]
[1,] 4.4757
> bread( mlgNm )
       [,1]
[1,] 4.4775
> bread( mlSann )
       [,1]
[1,] 4.4778
> bread( mlgSann )
      [,1]
[1,] 4.477
> 
> 
> ## test for method "sandwich"
> try( sandwich( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> printSandwich <- function( x ) {
+    print( sandwich( x ) )
+    print( all.equal( sandwich( x ), vcov( x ) ) )
+ }
> printSandwich( ml )
         [,1]
[1,] 0.043568
[1] "Mean relative difference: 0.027776"
> printSandwich( mlg )
        [,1]
[1,] 0.04355
[1] "Mean relative difference: 0.027993"
> printSandwich( mlBhhh )
         [,1]
[1,] 0.046022
[1] TRUE
> printSandwich( mlgBhhh )
         [,1]
[1,] 0.046022
[1] TRUE
> printSandwich( mlBfgs )
         [,1]
[1,] 0.043513
[1] "Mean relative difference: 0.02843"
> printSandwich( mlgBfgs )
        [,1]
[1,] 0.04355
[1] "Mean relative difference: 0.027993"
> printSandwich( mlNm )
         [,1]
[1,] 0.043526
[1] "Mean relative difference: 0.028267"
> printSandwich( mlgNm )
         [,1]
[1,] 0.043562
[1] "Mean relative difference: 0.027842"
> printSandwich( mlSann )
         [,1]
[1,] 0.043568
[1] "Mean relative difference: 0.027776"
> printSandwich( mlgSann )
         [,1]
[1,] 0.043551
[1] "Mean relative difference: 0.027972"
> 
