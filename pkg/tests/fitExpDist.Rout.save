
R version 2.11.1 (2010-05-31)
Copyright (C) 2010 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## load the maxLik package
> library( maxLik )
Loading required package: miscTools
> 
> ## fitting an exponential distribution by ML,
> ## e.g. estimation of an exponential duration model
> 
> # generate data
> set.seed( 4 )
> t <- rexp( 100, 2 )
> 
> # log-likelihood function, gradient, and Hessian
> loglik <- function(theta) log(theta) - theta*t
> loglikSum <- function(theta) sum( log(theta) - theta*t )
> gradlik <- function(theta) 1/theta - t
> gradlikSum <- function(theta) sum( 1/theta - t )
> hesslik <- function(theta) -100/theta^2
> 
> 
> ## NR estimation
> # Estimate with only function values
> ml <- maxLik( loglik, start = 1 )
> print( ml )
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( ml )
--------------------------------------------
Maximum Likelihood estimation
Newton-Raphson maximisation, 5 iterations
Return code 1: gradient close to zero
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21157  10.001 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( ml )
[1] 100
> print.default( ml )
$maximum
[1] -25.05386

$estimate
[1] 2.11586

$gradient
[1] 3.550493e-07

$hessian
          [,1]
[1,] -22.33946

$code
[1] 1

$message
[1] "gradient close to zero"

$last.step
NULL

$activePar
[1] TRUE

$iterations
[1] 5

$type
[1] "Newton-Raphson maximisation"

$gradientObs
               [,1]
  [1,]  0.386820740
  [2,] -1.679351421
  [3,]  0.038568231
  [4,]  0.071297539
  [5,]  0.159046830
  [6,]  0.105191912
  [7,]  0.248215152
  [8,]  0.447271104
  [9,]  0.217946018
 [10,]  0.054046213
 [11,] -0.867527898
 [12,]  0.328582594
 [13,]  0.270226234
 [14,]  0.258112510
 [15,]  0.302819658
 [16,] -0.051988000
 [17,]  0.442843510
 [18,]  0.405508832
 [19,] -0.447366181
 [20,] -0.033385377
 [21,]  0.350564558
 [22,] -0.150788615
 [23,] -2.297263118
 [24,]  0.388690940
 [25,] -0.444122549
 [26,]  0.443407905
 [27,]  0.276872854
 [28,] -0.151172542
 [29,]  0.226692194
 [30,]  0.192216411
 [31,] -0.216352483
 [32,] -0.427312129
 [33,] -0.415672371
 [34,]  0.278198846
 [35,] -0.636970368
 [36,]  0.394516804
 [37,]  0.344060912
 [38,] -0.620259675
 [39,]  0.457767068
 [40,]  0.167203757
 [41,]  0.353775677
 [42,] -0.065340666
 [43,]  0.147748192
 [44,]  0.282720516
 [45,] -0.015242575
 [46,]  0.079882304
 [47,]  0.274371832
 [48,]  0.452304212
 [49,] -1.144888502
 [50,]  0.405280565
 [51,] -0.227729882
 [52,]  0.433252234
 [53,]  0.081373336
 [54,] -0.081125844
 [55,] -0.739938580
 [56,]  0.207183336
 [57,]  0.113523154
 [58,]  0.119192738
 [59,]  0.342989802
 [60,]  0.093240225
 [61,]  0.440175145
 [62,] -0.073023340
 [63,] -0.501036829
 [64,]  0.075378942
 [65,] -0.172199929
 [66,]  0.045446566
 [67,] -0.025803401
 [68,]  0.181706957
 [69,]  0.447988624
 [70,] -0.160098111
 [71,]  0.439821889
 [72,]  0.248287382
 [73,]  0.403098281
 [74,] -0.190732560
 [75,] -0.472651001
 [76,] -0.065057863
 [77,] -0.455150226
 [78,]  0.159506499
 [79,]  0.376818622
 [80,]  0.121605794
 [81,]  0.301920778
 [82,] -0.001157193
 [83,]  0.414118371
 [84,]  0.400994288
 [85,]  0.349288516
 [86,] -0.996985135
 [87,]  0.378740651
 [88,]  0.385031349
 [89,] -0.316835763
 [90,]  0.192621247
 [91,]  0.328718431
 [92,] -0.042173025
 [93,]  0.060583996
 [94,] -0.644872461
 [95,] -0.632559538
 [96,] -0.356326811
 [97,] -0.323978772
 [98,]  0.220528787
 [99,] -0.832596222
[100,]  0.361128747

attr(,"class")
[1] "maxLik" "maxim"  "list"  
> # log-likelihood value summed over all observations
> mlSum <- maxLik( loglikSum, start = 1 )
> all.equal( mlSum[], ml[-11] )
[1] "Component 3: Mean relative difference: 0.000625"
> 
> # Estimate with analytic gradient
> mlg <- maxLik( loglik, gradlik, start = 1 )
> nObs( mlg )
[1] 100
> all.equal( mlg, ml )
[1] "Component 3: Mean relative difference: 4.859286"    
[2] "Component 4: Mean relative difference: 0.0001073857"
> # gradient summed over all observations
> mlgSum <- maxLik( loglikSum, gradlikSum, start = 1 )
> all.equal( mlgSum[], mlg[-11] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian
> mlgh <- maxLik( loglik, gradlik, hesslik, start = 1 )
> all.equal( mlgh, mlg )
[1] "Component 3: Mean relative difference: 3.100045e-06"
> 
> 
> ## BHHH estimation
> # Estimate with only function values
> mlBhhh <- maxLik( loglik, start = 1, method = "BHHH" )
> print( mlBhhh )
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBhhh )
--------------------------------------------
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21453  9.8629 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBhhh )
[1] 100
> all.equal( mlBhhh[ -c( 5, 6, 10 ) ], ml[ -c( 5, 6, 10 ) ] )
[1] "Component 2: Mean relative difference: 1.942932e-08"
[2] "Component 3: Mean relative difference: 0.7222391"   
[3] "Component 4: Mean relative difference: 0.02810344"  
[4] "Component 8: Mean relative difference: 2.718984e-08"
> 
> # Estimate with analytic gradient
> mlgBhhh <- maxLik( loglik, gradlik, start = 1, method = "BHHH" )
> nObs( mlgBhhh )
[1] 100
> all.equal( mlgBhhh, mlBhhh )
[1] "Component 3: Mean relative difference: 0.006972723"
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBhhh <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BHHH" )
> all.equal( mlghBhhh, mlgBhhh )
[1] TRUE
> 
> ## BFGS estimation
> # Estimate with only function values
> mlBfgs <- maxLik( loglik, start = 1, method = "BFGS" )
> print( mlBfgs )
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.11586 
> summary( mlBfgs )
--------------------------------------------
Maximum Likelihood estimation
BFGS maximisation, 14 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11586    0.21154  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlBfgs )
[1] 100
> all.equal( mlBfgs[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 3: Mean relative difference: 0.6323927"   
[2] "Component 4: Mean relative difference: 0.000317965" 
[3] "Component 7: Mean relative difference: 1.798869e-08"
> # log-likelihood value summed over all observations
> mlSumBfgs <- maxLik( loglikSum, start = 1, method = "BFGS" )
> all.equal( mlSumBfgs[], mlBfgs[-12] )
[1] "Component 3: Mean relative difference: 0.0005170037"
> 
> # Estimate with analytic gradient
> mlgBfgs <- maxLik( loglik, gradlik, start = 1, method = "BFGS" )
> nObs( mlgBfgs )
[1] 100
> all.equal( mlgBfgs, mlBfgs )
[1] "Component 3: Mean relative difference: 0.007287182" 
[2] "Component 4: Mean relative difference: 0.0004254472"
> # gradient summed over all observations
> mlgSumBfgs <- maxLik( loglikSum, gradlikSum, start = 1, method = "BFGS" )
> all.equal( mlgSumBfgs[], mlgBfgs[-12] )
[1] TRUE
> 
> # Estimate with analytic gradient and Hessian (unused during estimation)
> mlghBfgs <- maxLik( loglik, gradlik, hesslik, start = 1, method = "BFGS" )
> all.equal( mlghBfgs, mlgBfgs )
[1] TRUE
> 
> ## NM estimation
> # Estimate with only function values
> mlNm <- maxLik( loglik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> print( mlNm )
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.116016 
> summary( mlNm )
--------------------------------------------
Maximum Likelihood estimation
Nelder-Mead maximisation, 28 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11602    0.21156  10.002 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlNm )
[1] 100
> all.equal( mlNm[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 7.351024e-05"
[2] "Component 3: Mean relative difference: 1.000102"    
[3] "Component 4: Mean relative difference: 0.0001590078"
[4] "Component 7: Mean relative difference: 0.0001023246"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgNm <- maxLik( loglik, gradlik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> nObs( mlgNm )
[1] 100
> all.equal( mlgNm, mlNm )
[1] "Component 3: Mean relative difference: 1.818245e-07"
[2] "Component 4: Mean relative difference: 0.0004134995"
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghNm <- maxLik( loglik, gradlik, hesslik, start = 1, method = "NM" )
Warning message:
In optim(par = start[!fixed], fn = logLikFunc, control = control,  :
  one-diml optimization by Nelder-Mead is unreliable: use optimize
> all.equal( mlghNm, mlgNm )
[1] TRUE
> 
> ## SANN estimation
> # Estimate with only function values
> mlSann <- maxLik( loglik, start = 1, method = "SANN" )
> print( mlSann )
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 (1 free parameter(s))
Estimate(s): 2.115882 
> summary( mlSann )
--------------------------------------------
Maximum Likelihood estimation
SANN maximisation, 10000 iterations
Return code 0: successful convergence 
Log-Likelihood: -25.05386 
1  free parameters
Estimates:
     Estimate Std. error t value   Pr(> t)    
[1,]  2.11588    0.21161   9.999 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
--------------------------------------------
> nObs( mlSann )
[1] 100
> all.equal( mlSann[ -c( 5, 6, 9, 10, 11 ) ], ml[ -c( 5, 6, 9, 10 ) ] )
[1] "Component 2: Mean relative difference: 1.013788e-05"
[2] "Component 3: Mean relative difference: 1.000742"    
[3] "Component 4: Mean relative difference: 0.0003181674"
[4] "Component 7: Mean relative difference: 1.411139e-05"
> 
> # Estimate with analytic gradient (unused during estimation)
> mlgSann <- maxLik( loglik, gradlik, start = 1, method = "SANN" )
> nObs( mlgSann )
[1] 100
> all.equal( mlgSann, mlSann )
[1] "Component 3: Mean relative difference: 6.705094e-06"
[2] "Component 4: Mean relative difference: 0.0001904553"
> 
> # Estimate with analytic gradient and Hessian (both unused during estimation)
> mlghSann <- maxLik( loglik, gradlik, hesslik, start = 1, method = "SANN" )
> all.equal( mlghSann, mlgSann )
[1] TRUE
> 
> 
> ## test for method "estfun"
> library( sandwich )
Loading required package: zoo
> try( estfun( mlSum ) )
Error in estfun.maxLik(mlSum) : 
  cannot return the gradients of the log-likelihood function evaluated at each observation: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> estfun( ml )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682074
[2,] -1.67935142
[3,]  0.03856823
[4,]  0.07129754
[5,]  0.15904683
> estfun( mlg )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682074
[2,] -1.67935142
[3,]  0.03856823
[4,]  0.07129754
[5,]  0.15904683
> estfun( mlBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlgBhhh )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935141
[3,]  0.03856824
[4,]  0.07129755
[5,]  0.15904684
> estfun( mlBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlgBfgs )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38682075
[2,] -1.67935142
[3,]  0.03856824
[4,]  0.07129754
[5,]  0.15904684
> estfun( mlNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlgNm )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38678600
[2,] -1.67938616
[3,]  0.03853349
[4,]  0.07126280
[5,]  0.15901209
> estfun( mlSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> estfun( mlgSann )[ 1:5, , drop = FALSE ]
            [,1]
[1,]  0.38681595
[2,] -1.67935621
[3,]  0.03856344
[4,]  0.07129275
[5,]  0.15904204
> 
> 
> ## test for method "bread"
> try( bread( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> bread( ml )
         [,1]
[1,] 4.476383
> bread( mlg )
         [,1]
[1,] 4.476864
> bread( mlBhhh )
         [,1]
[1,] 4.602185
> bread( mlgBhhh )
         [,1]
[1,] 4.602185
> bread( mlBfgs )
        [,1]
[1,] 4.47496
> bread( mlgBfgs )
         [,1]
[1,] 4.476864
> bread( mlNm )
         [,1]
[1,] 4.475671
> bread( mlgNm )
         [,1]
[1,] 4.477522
> bread( mlSann )
         [,1]
[1,] 4.477807
> bread( mlgSann )
         [,1]
[1,] 4.476955
> 
> 
> ## test for method "sandwich"
> try( sandwich( mlSum ) )
Error in nObs.maxLik(x) : 
  cannot return the number of observations: please re-run 'maxLik' and provide a gradient function using argument 'grad' or (if no gradient function is specified) a log-likelihood function using argument 'logLik' that return the gradients or log-likelihood values, respectively, at each observation
> printSandwich <- function( x ) {
+    print( sandwich( x ) )
+    print( all.equal( sandwich( x ), vcov( x ) ) )
+ }
> printSandwich( ml )
          [,1]
[1,] 0.0435402
[1] "Mean relative difference: 0.02810344"
> printSandwich( mlg )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799305"
> printSandwich( mlBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlgBhhh )
           [,1]
[1,] 0.04602185
[1] TRUE
> printSandwich( mlBfgs )
           [,1]
[1,] 0.04351252
[1] "Mean relative difference: 0.02843045"
> printSandwich( mlgBfgs )
           [,1]
[1,] 0.04354955
[1] "Mean relative difference: 0.02799309"
> printSandwich( mlNm )
           [,1]
[1,] 0.04352636
[1] "Mean relative difference: 0.02826694"
> printSandwich( mlgNm )
           [,1]
[1,] 0.04356236
[1] "Mean relative difference: 0.02784193"
> printSandwich( mlSann )
           [,1]
[1,] 0.04356791
[1] "Mean relative difference: 0.02777644"
> printSandwich( mlgSann )
           [,1]
[1,] 0.04355132
[1] "Mean relative difference: 0.02797222"
> 
