\documentclass{article}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xspace}

\newcommand{\elemProd}{\ensuremath{\odot}}  % elementwise product of matrices
\newcommand*{\mat}[1]{\mathsf{#1}}
\newcommand{\maxlik}{\texttt{maxLik}\xspace}
\newcommand*{\transpose}{^{\mkern-1.5mu\mathsf{T}}}
%\newcommand{\transpose}{\intercal}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}

% \VignetteIndexEntry{SGA Example}

\begin{document}
<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60,
        try.outFile=stdout()  # make try to produce error messages
        )
foo <- packageDescription("maxLik")
@

\title{Stochastic Gradient Ascent in maxLik}
\author{Ott Toomet}
\maketitle

\section{\texttt{maxLik} and Stochastic Gradient Ascent}

\texttt{maxLik} is a package, primarily intended with Maximum
Likelihood and related estimations.  Besides of its name, it also
includes a number of tools geared for a typical Maximum Likelihood workflow.

However, as predictive modeling and complex (deep) models have gained
popularity in the recend decade, \texttt{maxLik} also includes a few
popular algorithms for stochastic gradient ascent, the mirror image
for the more widely known stochastic gradient descent.


\section{Stochastic Gradient Ascent}
\label{sec:stochastic-gradient-ascent}

In Machine Learning literature, it is more common to describe the
optimization problems as minimization and hence to talk about
gradient descent.  As \texttt{maxLik} is primarily focused on
\emph{maximizing} likelihood, it implements the maximization version
of the method, stochastic gradient ascent (SGA).

The basic method is very easy, essentially just slow climb in the
gradient's direction.  Given and objective function
$f(\vec{\theta})$, and the initial parameter vector
$\vec{\theta}_{0}$, the algorithm will compute the gradient
$\vec{g}(\vec{\theta}_{0}) = \nabla_{\vec{\theta}}
f(\vec{\theta})\big|_{\vec{\theta} = \vec{\theta}_{0}}$, and update
the parameter vector as $\vec{\theta}_{1} = \vec{\theta}_{0} + \rho
\vec{g}(\vec{\theta}_{0})$.  Here $\rho$, the \emph{learning rate}, is
a small positive constant to ensure we do not overshoot the optimum.
Depending on the task it is typically of order $0.1 \dots 0.001$.
In typical tasks, the objective function $f(\vec{\theta})$ depends on
data $\mat{X}$ in an additive form $f(\vec{\theta}; \mat{X}) =
\sum_{i} f(\vec{\theta}; \vec{x}_{i})$ where $i$ denotes
``observations'', typically rows of the design matrix $\mat{X}$ that
are independent of each other.

The above introduction did not specify how to compute the gradient
$\vec{g}(\vec{\theta}_{0})$ in terms of which data vectors $\vec{x}$
to include.  A natural approach is to include the complete data and compute
\begin{equation}
  \label{eq:full-batch-gradient}
  \vec{g}_{N}(\vec{\theta}_{0}) = 
  \frac{1}{N}\sum_{i=1}^{N}
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}.
\end{equation}
This approach is called ``full batch'' and it has a number of
advantages.  In particular, it is deterministic (given data
$\mat{X}$), and the sum in~\eqref{eq:full-batch-gradient} can be
easily parallelized in typical applications.  However, there are also
a number of reasons why full-batch approach may not be desirable
\citep[see][]{bottou2018SIAM}:
\begin{itemize}
\item In practical applications, there is often a lot of redundancy in
  different observations in data.  When always using all the
  observations for the update means spending a substantial effort on
  redundant calculations.
\item Full-batch gradient lacks the stochastic noise.  While
  advantageous in the latter steps of optimization, the noise helps
  the optimizer to avoid local maxima and overcome flat areas in the
  objective function early in the process.  
\item SGA achieves much more rapid initial convergence compared to the
  full batch method (although full-batch methods may achieve better
  final result).
\item The advantage of SGA grows in sample size $N$.  Cost of
  computing the full-batch gradient grows with the sample size but
  that of minibatch gradient does not grow.  
\item It is empirically known that large-batch optimization tend to
  find sharp optima \citep{keskar+2016ArXiv} that do not generalize well to validation
  data.  Small batch approach leads to better validation performance. 
\end{itemize}

In what is usually referred to as ``stochastic gradient ascent'' in
the literature, refers to the case where the gradient is computed on a
single observation:
\begin{equation}
  \label{eq:stochastic-gradient}
  \vec{g}_{1}(\vec{\theta}_{0}) = 
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}
\end{equation}
where $i$ is chosen randomly.  In applications, all the observations
are usually walked through in a random order, to ensure that each
observation is included once, and only once, in an \emph{epoch}, a
full walk-through of the data.  In between the full-batch and
stochastic gradient there is \emph{minibatch} gradient
\begin{equation}
  \label{eq:stochastic-gradient}
  \vec{g}_{m}(\vec{\theta}_{0}) = 
  \frac{1}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}
\end{equation}
where $\mathcal{B}$ is the batch, a set of observations' indices, that
are included in the gradient computation.  In applications, one has to
construct a series of minibatches that cover the complete data, and
walk through those sequentially in one epoch.


\section{SGA in \texttt{maxLik} package}
\label{sec:sga-in-maxlik}

\maxlik implements two different optimizers: \texttt{maxSGA} for
simple SGA (including momentum), and \texttt{maxAdam} for the Adaptive
Moments method \citep[see][p. 301]{goodfellow+2016DL}.  Both methods
mostly follow that of the package's main workhorse, \texttt{maxNR} \citep[see][]{henningsen+toomet2011},
but their API has some important differences due to the different
nature and usage of SGA.

\subsection{The objective function}

Unlike in \texttt{maxNR} and related functions, SGA does not directly need the
objective function values.  The function can still be provided (and
probably will in most cases), but one can run the optimizer with only
gradient.  If provided, the function can be used for printing the
value at each epoch for following the process, and for stopping
through
\emph{patience}: normally, the new iteration has better (higher)
value of the objective function.  However, in unfavorable situations
it is often not the case.  In such a case the algorithm continues
not more than \emph{patience} times before stopping (and returning
the best parameters, not necessarily the last parameters).  

If provided, the function should accept two (or more) arguments: the
first is the numeric parameter vector, and the second, called
\texttt{index}, is the list of indices in the current minibatch.

As the function is not needed by the optimizer itself, it is up to the
user to decide what it should do.  An obvious option is to compute the
objective function value on the same minibatch as used for the
gradient computation.  But one can also opt for something else, for
instance to compute the value on the validation data instead (and
ignore the provided \emph{index}).  The latter may be an useful option
if one wants to employ the patience-based stopping criteria.

\subsection{Gradient function}
\label{sec:gradient-function}

Gradient is the work-horse of the SGA methods.  Although \maxlik can
also compute numeric gradient using the finite difference method, this
is not advisable, and may be very slow in high-dimensional problems.
The gradient should be a $1\times K$ matrix in numerator layout,
i.e. each column corresponds to the corresponding component in the
parameter vector $\vec{\theta}$.



\section{Example usage cases}
\label{sec:example-usage-cases}

\subsection{Linear regression}
\label{sec:linear-regression}

We demonstrate linear regression (OLS) as the first example, mostly for
illustrative purposes, as OLS is an easy-to understand model.  We use
the Boston housing data, a popular dataset where one traditionally attempts to
predict the median house price across 500 neighborhoods, using a
number of variables, such as mean house size, age, and proximity to
Charles river.  All variables in the dataset are numeric, and there
are no missing values.  The data is provided in \emph{MASS} package.

First, we create the design matrix $\mat{X}$ and extract the house
price $y$:
<<>>=
i <- which(names(MASS::Boston) == "medv")
X <- as.matrix(MASS::Boston[,-i])
X <- cbind("const"=1, X)  # add constant
y <- MASS::Boston[,i]
eigenvals <- eigen(crossprod(X))$values
@
Although the model and data are simple, it is not an easy task for
stock gradient ascent.  The problem lies in different scaling of
variables, the means are
<<>>=
colMeans(X)
@
One can see that \emph{chas} has an average value
\Sexpr{round(mean(X[,"chas"]), 3)} while that of \emph{tax} is
\Sexpr{round(mean(X[,"tax"]), 3)}.  This leads to highly elliptical
parabola of with the ratio of largest and smallest eigenvalues of
$\mat{X}^{\transpose} \mat{X} =
\Sexpr{round(eigenvals[1]/eigenvals[14], -5)}$.  Solely gradient-based
methods as SGA
have trouble working in
resulting narrow valleys.

For reference, let's also compute the analytic solution to this
linear regression model (reminder: $\hat{\vec{\beta}} = (\mat{X}^{\transpose}\,\mat{X})^{-1}\,\mat{X}\,\vec{y}$):
<<>>=
betaX <- solve(crossprod(X)) %*% crossprod(X, y)
betaX <- drop(betaX)
betaX
@ 

Next, we provide the gradient function.  As a reminder, OLS gradient
in numerator layout can be expressed as
\begin{equation}
  \label{eq:ols-gradient}
  \vec{g}_{m}(\vec{\theta}) =
  -\frac{2}{|\mathcal{B}|}
  \sum_{i\in\mathcal{B}}
  \left(y_{i} - \vec{x}_{i}^{\transpose} \cdot
    \vec{\theta} \right) \vec{x}_{i}^{\transpose}
  =
  -\frac{2}{|\mathcal{B}|}
  \left(y_{\mathcal{B}} -
    \mat{X}_{\mathcal{B}} \cdot \vec{\theta} \right)^{\transpose}
  \mat{X}_{\mathcal{B}}
\end{equation}
where $y_{\mathcal{B}}$ and $\mat{X}_{\mathcal{B}}$ denote the
outcomes and rows of the design matrix that
corresponds to the minibatch $\mathcal{B}$.
We implement it as:
<<>>=
gradloss <- function(theta, index)  {
   e <- y[index] - X[index,,drop=FALSE] %*% theta
   g <- -2*t(e) %*% X[index,,drop=FALSE]
   -g/length(index)
}
@
The \texttt{gradloss} function has two arguments: \texttt{theta} is the
parameter vector, and \texttt{index} tells which observations belong
to the current minibatch.  The actual argument will be an integer vector, and hence
we can use \texttt{length(index)} to find the size of the minibatch.
Finally, we return the negative of~\eqref{eq:ols-gradient} as
\texttt{maxSGA} performs maximization, not minimization.

First, we demonstrate how the models works without the objective
function.  We have to supply the gradient function, initial parameter
values, and also \texttt{nObs}, number of observations to select the
batches from.  The latter is needed as the optimizer itself does not
have access to data but still has to partition it into batches.
Finally, we may also provide various control parameters, such as
number of iterations, stopping conditions, and batch size.  We start
with only specifying the iteration limit, the only stopping condition
we use here:
<<gradonly, quiet=FALSE>>=
library(maxLik)
set.seed(3)
start <- setNames(rnorm(ncol(X), sd=0.1), colnames(X))
                           # add names for better reference
res <- try(maxSGA(grad=gradloss,
           start=start,
           nObs=nrow(X),
           control=list(iterlim=1000)
           )
    )
@
This run was a failure.  We encountered a run-away growth of the
gradient and parameter values.  This is because the default learning
rate $\rho=0.1$ is too big for such strongly curved objective
function.  But before we repeat the exercise with a smaller learning
rate, let's try gradient clipping.  The clipping, performed with
\texttt{SG\_clip} control option, limits the squared $L_{2}$-norm of
the gradient with the given value while keeping it's direction:
<<>>=
res <- maxSGA(grad=gradloss,
              start=start,
              nObs=nrow(X),
              control=list(iterlim=1000,
                           SG_clip=1e4)  # allow ||g|| <= 100
              )
summary(res)
@
This time the gradient did not explode and we were able to get a
result.  But the estimates are rather far from the analytic solution
shown above, in particular the constant estimate
\Sexpr{round(coef(res)[1], 3)} is very different from the
corresponding analytic value \Sexpr{round(betaX[1], 3)}.  Let's
analyze what is happening inside the optimizer.  We can ask for both
the parameter values and the objective function value to be stored
for each epoch.  But before we can store the objective function (MSE)
value, we have to supply it.  We compute MSE on the same dataset as
<<>>=
loss <- function(theta, index) {
   e <- y[index] - X[index,] %*% theta
   -crossprod(e)/length(index)
}
@ 
Now we can store the values with the control options
\texttt{storeParameters} and \texttt{storeValues}.  The corresponding
numbers can be retrieved with \texttt{storedParameters} and
\texttt{storedValues} methods.  The former returns a $K\times R+1$
matrix, one row for each epoch and one column for each parameter
component, and the latter returns a numeric vector of length $R+1$
where $R$ is the number of epochs.  The first value in both cases is
the initial value, that's why we have $R+1$ values in total.  Let's
retrieve the values and plot both.  We also decrease the learning rate
to $0.001$ using the \texttt{SG\_learningRate} control:
\setkeys{Gin}{width=\textwidth, height=80mm}
<<fig=TRUE, height=4>>=
res <- maxSGA(loss, gradloss,
              start=start,
              nObs=nrow(X),
              control=list(iterlim=1580,
                           # will misbehave with larger numbers
                           SG_clip=1e4,  # allow ||g|| <= 100
                           SG_learningRate=0.001,
                           storeParameters=TRUE,
                           storeValues=TRUE
                           )  
              )
par <- storedParameters(res)
val <- storedValues(res)
par(mfrow=c(1,2))
plot(par[,1], par[,2], type="b", pch=".",
     xlab=names(start)[1], ylab=names(start)[2], main="Parameters")
iB <- c(40, nrow(par)/2, nrow(par))
iA <- iB - 10
arrows(par[iA,1], par[iA,2], par[iB,1], par[iB,2], length=0.1)
plot(seq(length=length(val))-1, -val, type="l",
     xlab="epoch", ylab="MSE", main="Loss",
     log="y")
@
We can see how the parameters (first and second component here) evolve
through the iterations while the loss is rapidly falling.  One can see
an initial jump where the loss is falling very fast, followed but
subsequent slow movement.  It is possible the initial jump is be limited by
gradient clipping.


\subsection{Training and Validation Sets}
\label{sec:training-validation}

However, as we did not specify the batch size, \texttt{maxSGA} will
automatically pick the full batch (equivalent to control option
\texttt{SG\_batchSize = NULL}).  So there was nothing stochastic in
what we did above.  Let us pick a small batch size, for instance
1, a single observation at time.  However, as smaller batch sizes
introduce more noise to the gradient, we also make the learning rate
accordingly smaller and choose \texttt{SG\_learningRate = 2e-4}.

But now the existing loss function, calculated just at the single
observation, carries little meaning.  Instead, we split the data into
training and validation sets, feed batches of training data to gradient but
calculate the loss on the complete validation set.  This can be
achieved with small modifications in the \texttt{gradloss} and
\texttt{loss} function.  But first, let's split data:
<<>>=
i <- sample(nrow(X), 0.8*nrow(X))  # training indices
Xt <- X[i,]
yt <- y[i]
Xv <- X[-i,]
yv <- y[-i]
@ 
Thereafter we modify \texttt{gradloss} to only use the batches of
training data while \texttt{loss} will use the validation data and
just ignore \texttt{index}:
<<>>=
gradloss <- function(theta, index)  {
   e <- yt[index] - Xt[index,,drop=FALSE] %*% theta
   g <- -2*t(e) %*% Xt[index,,drop=FALSE]
   -g/length(index)
}
loss <- function(theta, index) {
   e <- yv - Xv %*% theta
   -crossprod(e)/length(yv)
}
@

Another thing to note is the speed.  \texttt{maxLik} implements SGA in
a fairly complex loop that does printing, storing, stopping
conditions, and complex function calls.  Hence smaller batch size
means much more such auxiliary computations per epoch and the
algorithm slows considerably down.
How do the convergence properties
look now with the updated approach?
<<batch1, fig=TRUE, height=4>>=
res <- maxSGA(loss, gradloss,
              start=start,
              nObs=nrow(Xt),  # note: only training data now
              control=list(iterlim=1000,
                           SG_batchSize=1,
                           SG_learningRate=1e-5,
                           # will misbehave with larger numbers
                           SG_clip=1e4,  # allow ||g|| <= 100
                           storeParameters=TRUE,
                           storeValues=TRUE
                           )  
              )
par <- storedParameters(res)
val <- storedValues(res)
par(mfrow=c(1,2))
plot(par[,1], par[,2], type="b", pch=".",
     xlab=names(start)[1], ylab=names(start)[2], main="Parameters")
iB <- c(40, nrow(par)/2, nrow(par))
iA <- iB - 1
arrows(par[iA,1], par[iA,2], par[iB,1], par[iB,2], length=0.1)
plot(seq(length=length(val))-1, -val, type="l",
     xlab="epoch", ylab="MSE", main="Loss",
     log="y")
@
We can see the parameters evolving and loss decreasing over epochs.
The convergence seems to be smooth and not ruptured by gradient
clipping. 

Next, we try to improve the convergence by introducing momentum.  With
momentum $\mu$, the gradient update is
\begin{equation}
  \label{eq:gradient-update-momentum}
  \vec{\theta}_{t+1} =
  \vec{\theta}_{t} + \vec{v}_{t}
  \quad\text{where}\quad
  \vec{v}_{t} = \mu \vec{v}_{t-1} + \rho \vec{g}(\vec{\theta}).
\end{equation}
See \citet[p. 288]{goodfellow+2016DL}.  The algorithm takes the initial ``velocity'' $\vec{v}_{0} = \vec{0}$. 

Let's add momentum $\mu = 0.95$ to the gradient (and also decrease the
learning rate):

<<momentum, fig=TRUE, height=4>>=
res <- maxSGA(loss, gradloss,
              start=start,
              nObs=nrow(Xt),
              control=list(iterlim=1000,
                           SG_batchSize=1,
                           SG_learningRate=2e-7,
                           SG_clip=1e4,
                           SGA_momentum = 0.99,
                           storeParameters=TRUE,
                           storeValues=TRUE
                           )  
              )
par <- storedParameters(res)
val <- storedValues(res)
par(mfrow=c(1,2))
plot(par[,1], par[,2], type="b", pch=".",
     xlab=names(start)[1], ylab=names(start)[2], main="Parameters")
iB <- c(40, nrow(par)/2, nrow(par))
iA <- iB - 1
arrows(par[iA,1], par[iA,2], par[iB,1], par[iB,2], length=0.1)
plot(seq(length=length(val))-1, -val, type="l",
     xlab="epoch", ylab="MSE", main="Loss",
     log="y")
@
We achieved a lower loss but we are still far from the correct solution.  

As the next step, we use Adaptive Moment (Adam) optimizer
\citep[p. 301]{goodfellow+2016DL}.  Adam adapts the learning rate by
variance of the gradient--if gradient components are unstable, it
slows down, and if they are stable it speeds up.  The adaptation is
proportional to the moving average of the gradient divided by the
square root of the moving average of the gradient squared, all
operations done component-wise.  In this way a stable gradient
component where
moving average is similar to the gradient value will have higher
speed than a fluctuating gradient where the components frequently
shift the sign and the average is much smaller.  More specifically,
the algorithm is as follows:
\begin{enumerate}
\item Initialize first and second moment averages $\vec{s} = \vec{0}$
  and $\vec{r} = \vec{0}$.
\item Compute the gradient $\vec{g} = \vec{g}(\vec{\theta}_{t})$.
\item Update the moving average for the first moment: $\vec{s}_{t+1} =
  \mu_{1} \vec{s}_{t} + (1 - \mu_{1}) \vec{g}$.  $\mu_{1}$ is the
  decay parameter, the larger it is, the longer memory does the method
  have.  It can be adjusted with the control parameter
  \texttt{Adam\_momentum1} with the default value 0.9.
\item \dots and the second moment: $\vec{r}_{t+1} =
  \mu_{1} \vec{r}_{t} + (1 - \mu_{1}) \vec{g} \elemProd \vec{g}$
  where $\elemProd$ denotes element-wise multiplication.  The control
  parameter for the $\mu_{2}$ is \texttt{Adam\_momentum2} with the default value 0.999.
\item As the algorithm starts with the moving averages equal to zero,
  we also correct the resulting bias: $\hat{\vec{s}} = \vec{s}/(1 -
  \mu_{1}^{t})$ and $\hat{r} = \vec{s}/(1 - \mu_{1}^{t})$.
\item Finally, update the estimate: $\vec{\theta}_{t+1} =
  \vec{\theta}_{t} + \rho \hat{\vec{s}}/(\delta +
  \sqrt{\hat{\vec{r}}})$ where division and square root are done
  element-wise and $\delta=10^{-8}$ takes care of numerical stabilization.
\end{enumerate}

<<Adam, fig=TRUE, height=4>>=
res <- maxAdam(loss, gradloss,
              start=start,
              nObs=nrow(Xt),
              control=list(iterlim=1000,
                           SG_batchSize=1,
                           SG_learningRate=2e-7,
                           SG_clip=1e4,
                           storeParameters=TRUE,
                           storeValues=TRUE
                           )  
              )
par <- storedParameters(res)
val <- storedValues(res)
par(mfrow=c(1,2))
plot(par[,1], par[,2], type="b", pch=".",
     xlab=names(start)[1], ylab=names(start)[2], main="Parameters")
iB <- c(40, nrow(par)/2, nrow(par))
iA <- iB - 1
arrows(par[iA,1], par[iA,2], par[iB,1], par[iB,2], length=0.1)
plot(seq(length=length(val))-1, -val, type="l",
     xlab="epoch", ylab="MSE", main="Loss",
     log="y")
@
As visible from the figure, we achieved a lower loss (approximately
500) but run into stability problems at the end.

\subsection{Sequence of Batch Sizes }
\label{sec:sequence-batch-sizes}

The OLS' loss function is globally convex and hence there is no danger
to get stuck in a local optimum.  However, when the objective function
is more complex, the noise, generated by the stochastic sampling helps
the algorithm to leave local maxima.  A suggested strategy is to
increase the batch size over time to achieve good exploratory
properties early in the process and stable convergence later
\citep[see][for more information]{smith+2018arXiv}.  This approach is
in many ways similar to Simulated Annealing.

Here we introduce such an approach by using batch sizes $B=1$, $B=10$
and $B=100$ in succession.  We also introduce patience stopping
condition, if the objective function value is worse than the best
value so far more than \emph{patience} times, the algorithm stops.
Here we use patience value 5.  We also
store the loss values from all the batch sizes into a single vector \texttt{val}.

\setkeys{Gin}{width=\textwidth, height=110mm}
<<SANN, fig=TRUE, heigh=6, width=7>>=
val <- NULL
for(B in c(1,10,100)) {
   res <- maxAdam(loss, gradloss,
                  start=start,
                  nObs=nrow(Xt),
                  control=list(iterlim=3000,
                               SG_batchSize=1,
                               SG_learningRate=2e-7,
                               SG_clip=1e4,
                               SG_patience=5,  # worse value allowed only 5 times
                               storeValues=TRUE
                               )  
                  )
   cat("Batch size", B, ",", nIter(res), "epochs, function value", maxValue(res), "\n")
   val <- c(val, na.omit(storedValues(res)))
   start <- coef(res)
}
plot(seq(length=length(val))-1, -val, type="l",
     xlab="epoch", ylab="MSE", main="Loss",
     log="y")
summary(res)
@ 

One can see that both first loops ($B=1$ and $B=10$) run over all 3000
epochs, but the last one only iterated 7 times--apparently it almost
immediately overshot the maximum.  A solution might be to decrease the
learning rate.  But we stop here as we believe every \emph{R}-savy user can adapt the
method to their need.

As explained above, this dataset is not an easy task for methods that are
solely gradient-based.
However, our task here is to demonstrate the usage of
the package, not to solve a linear regression exercise.


\bibliographystyle{apecon}
\bibliography{sga}

\end{document}
