\documentclass{article}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xspace}

\newcommand*{\mat}[1]{\mathsf{#1}}
\newcommand{\maxlik}{\texttt{maxLik}\xspace}
\newcommand*{\transpose}{^{\mkern-1.5mu\mathsf{T}}}
%\newcommand{\transpose}{\intercal}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}

% \VignetteIndexEntry{SGA Example}

\begin{document}
<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60,
        try.outFile=stdout()  # make try to produce error messages
        )
foo <- packageDescription("maxLik")
@

\title{Stochastic Gradient Ascent in maxLik}
\author{Ott Toomet}
\maketitle

\section{\texttt{maxLik} and Stochastic Gradient Ascent}

\texttt{maxLik} is a package, primarily intended with Maximum
Likelihood and related estimations.  Besides of its name, it also
includes a number of tools geared for a typical Maximum Likelihood workflow.

However, as predictive modeling and complex (deep) models have gained
popularity in the recend decade, \texttt{maxLik} also includes a few
popular algorithms for stochastic gradient ascent, the mirror image
for the more widely known stochastic gradient descent.


\section{Stochastic Gradient Ascent}
\label{sec:stochastic-gradient-ascent}

In Machine Learning literature, it is more common to describe the
optimization problems as minimization and hence to talk about
gradient descent.  As \texttt{maxLik} is primarily focused on
\emph{maximizing} likelihood, it implements the maximization version
of the method, stochastic gradient ascent (SGA).

The basic method is very easy, essentially just slow climb in the
gradient's direction.  Given and objective function
$f(\vec{\theta})$, and the initial parameter vector
$\vec{\theta}_{0}$, the algorithm will compute the gradient
$\vec{g}(\vec{\theta}_{0}) = \nabla_{\vec{\theta}}
f(\vec{\theta})\big|_{\vec{\theta} = \vec{\theta}_{0}}$, and update
the parameter vector as $\vec{\theta}_{1} = \vec{\theta}_{0} + \rho
\vec{g}(\vec{\theta}_{0})$.  Here $\rho$, the \emph{learning rate}, is
a small positive constant to ensure we do not overshoot the optimum.
Depending on the task it is typically of order $0.1 \dots 0.001$.
In typical tasks, the objective function $f(\vec{\theta})$ depends on
data $\mat{X}$ in an additive form $f(\vec{\theta}; \mat{X}) =
\sum_{i} f(\vec{\theta}; \vec{x}_{i})$ where $i$ denotes
``observations'', typically rows of the design matrix $\mat{X}$ that
are independent of each other.

The above introduction did not specify how to compute the gradient
$\vec{g}(\vec{\theta}_{0})$ in terms of which data vectors $\vec{x}$
to include.  A natural approach is to include the complete data and compute
\begin{equation}
  \label{eq:full-batch-gradient}
  \vec{g}_{N}(\vec{\theta}_{0}) = 
  \frac{1}{N}\sum_{i=1}^{N}
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}.
\end{equation}
This approach is called ``full batch'' and it has a number of
advantages.  In particular, it is deterministic (given data
$\mat{X}$), and the sum in~\eqref{eq:full-batch-gradient} can be
easily parallelized in typical applications.  However, there are also
a number of reasons why full-batch approach may not be desirable
\citep[see][]{bottou2018SIAM}:
\begin{itemize}
\item In practical applications, there is often a lot of redundancy in
  different observations in data.  When always using all the
  observations for the update means spending a substantial effort on
  redundant calculations.
\item Full-batch gradient lacks the stochastic noise.  While
  advantageous in the latter steps of optimization, the noise helps
  the optimizer to avoid local maxima and overcome flat areas in the
  objective function early in the process.  
\item SGA achieves much more rapid initial convergence compared to the
  full batch method (although full-batch methods may achieve better
  final result).
\item The advantage of SGA grows in sample size $N$.  Cost of
  computing the full-batch gradient grows with the sample size but
  that of minibatch gradient does not grow.  
\item It is empirically known that large-batch optimization tend to
  find sharp optima \citep{keskar+2016ArXiv} that do not generalize well to validation
  data.  Small batch approach leads to better validation performance. 
\end{itemize}

In what is usually referred to as ``stochastic gradient ascent'' in
the literature, refers to the case where the gradient is computed on a
single observation:
\begin{equation}
  \label{eq:stochastic-gradient}
  \vec{g}_{1}(\vec{\theta}_{0}) = 
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}
\end{equation}
where $i$ is chosen randomly.  In applications, all the observations
are usually walked through in a random order, to ensure that each
observation is included once, and only once, in an \emph{epoch}, a
full walk-through of the data.  In between the full-batch and
stochastic gradient there is \emph{minibatch} gradient
\begin{equation}
  \label{eq:stochastic-gradient}
  \vec{g}_{m}(\vec{\theta}_{0}) = 
  \frac{1}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}
  \nabla_{\vec{\theta}}
  f(\vec{\theta}; \vec{x}_{i})\big|_{\vec{\theta} = \vec{\theta}_{0}}
\end{equation}
where $\mathcal{B}$ is the batch, a set of observations' indices, that
are included in the gradient computation.  In applications, one has to
construct a series of minibatches that cover the complete data, and
walk through those sequentially in one epoch.


\section{SGA in \texttt{maxLik} package}
\label{sec:sga-in-maxlik}

\maxlik implements two different optimizers: \texttt{maxSGA} for
simple SGA (including momentum), and \texttt{maxAdam} for the Adaptive
Moments method \citep[see][p. 301]{goodfellow+2016DL}.  Both methods
mostly follow that of the package's main workhorse, \texttt{maxNR} \citep[see][]{henningsen+toomet2011},
but their API has some important differences due to the different
nature and usage of SGA.

\subsection{The objective function}

Unlike in \texttt{maxNR} and related functions, SGA does not directly need the
objective function values.  The function can still be provided (and
probably will in most cases), but one can run the optimizer with only
gradient.  If provided, the function can be used for printing the
value at each epoch for following the process, and for stopping
through
\emph{patience}: normally, the new iteration has better (higher)
value of the objective function.  However, in unfavorable situations
it is often not the case.  In such a case the algorithm continues
not more than \emph{patience} times before stopping (and returning
the best parameters, not necessarily the last parameters).  

If provided, the function should accept two (or more) arguments: the
first is the numeric parameter vector, and the second, called
\texttt{index}, is the list of indices in the current minibatch.

As the function is not needed by the optimizer itself, it is up to the
user to decide what it should do.  An obvious option is to compute the
objective function value on the same minibatch as used for the
gradient computation.  But one can also opt for something else, for
instance to compute the value on the validation data instead (and
ignore the provided \emph{index}).  The latter may be an useful option
if one wants to employ the patience-based stopping criteria.

\subsection{Gradient function}
\label{sec:gradient-function}

Gradient is the work-horse of the SGA methods.  Although \maxlik can
also compute numeric gradient using the finite difference method, this
is not advisable, and may be very slow in high-dimensional problems.
The gradient should be a $1\times K$ matrix in numerator notation,
i.e. each column corresponds to the corresponding component in the
parameter vector $\vec{\theta}$.



\section{Example usage cases}
\label{sec:example-usage-cases}

\subsection{Linear regression}
\label{sec:linear-regression}

We demonstrate linear regression (OLS) as the first example, mostly for
illustrative purposes, as OLS is an easy-to understand model.  We use
the Boston housing data, a popular dataset where one traditionally attempts to
predict the median house price across 500 neighborhoods, using a
number of variables, such as mean house size, age, and proximity to
Charles river.  All variables in the dataset are numeric, and there
are no missing values.  The data is provided in \emph{MASS} package.

First, we create the design matrix $\mat{X}$ and extract the house
price $y$:
<<>>=
i <- which(names(MASS::Boston) == "medv")
X <- as.matrix(MASS::Boston[,-i])
X <- cbind(1, X)  # add constant
y <- MASS::Boston[,i]
eigenvals <- eigen(crossprod(X))$values
@
Although the model and data are simple, it is not an easy task for
stock gradient ascent.  The problem lies in different scaling of
variables, the means are
<<>>=
colMeans(X)
@
One can see that \emph{chas} has an average value
\Sexpr{round(mean(X[,"chas"]), 3)} while that of \emph{tax} is
\Sexpr{round(mean(X[,"tax"]), 3)}.  This leads to highly elliptical
parabola of with the ratio of largest and smallest eigenvalues of
$\mat{X}^{\transpose} \mat{X} =
\Sexpr{round(eigenvals[1]/eigenvals[14], -5)}$.  Solely gradient-based
methods as SGA
have trouble working in
resulting narrow valleys.

For reference, let's also compute the analytic solution to this
linear regression model (reminder: $\hat{\vec{\beta}} = (\mat{X}^{\transpose}\,\mat{X})^{-1}\,\mat{X}\,\vec{y}$):
<<>>=
betaX <- solve(crossprod(X)) %*% crossprod(X, y)
betaX <- drop(betaX)
betaX
@ 

Next, we provide the gradient function.  As a reminder, OLS gradient
in numerator layout can be expressed as
\begin{equation}
  \label{eq:ols-gradient}
  \vec{g}_{m}(\vec{\theta}) =
  -\frac{2}{|\mathcal{B}|}
  \sum_{i\in\mathcal{B}}
  \left(y_{i} - \vec{x}_{i}^{\transpose} \cdot
    \vec{\theta} \right) \vec{x}_{i}^{\transpose}
  =
  -\frac{2}{|\mathcal{B}|}
  \left(y_{\mathcal{B}} -
    \mat{X}_{\mathcal{B}} \cdot \vec{\theta} \right)^{\transpose}
  \mat{X}_{\mathcal{B}}
\end{equation}
where $y_{\mathcal{B}}$ and $\mat{X}_{\mathcal{B}}$ denote the
outcomes and rows of the design matrix that
corresponds to the minibatch $\mathcal{B}$.
We implement it as:
<<>>=
gradlik <- function(theta, index)  {
   e <- y[index] - X[index,,drop=FALSE] %*% theta
   g <- -2*t(e) %*% X[index,,drop=FALSE]
   -g/length(index)
}
@
The \texttt{gradlik} function has two arguments: \texttt{theta} is the
parameter vector, and \texttt{index} tells which observations belong
to the current minibatch.  The actual argument will be an integer vector, and hence
we can use \texttt{length(index)} to find the size of the minibatch.
Finally, we return the negative of~\eqref{eq:ols-gradient} as
\texttt{maxSGA} performs maximization, not minimization.

First, we demonstrate how the models works without the objective
function.  We have to supply the gradient function, initial parameter
values, and also \texttt{nObs}, number of observations to select the
batches from.  The latter is needed as the optimizer itself does not
have access to data but still has to partition it into batches.
Finally, we may also provide various control parameters, such as
number of iterations, stopping conditions, and batch size.  We start
with only specifying the iteration limit, the only stopping condition
we use here:
<<gradonly, quiet=FALSE>>=
library(maxLik)
start <- setNames(rnorm(ncol(X), sd=0.1), colnames(X))
                           # add names for better reference
res <- try(maxSGA(grad=gradlik,
           start=start,
           nObs=nrow(X),
           control=list(iterlim=1000)
           )
    )
@
This run was a failure.  We encountered a run-away growth of the
gradient and parameter values.  This is because the default learning
rate $\rho=0.1$ is too big for such strongly curved objective
function.  But before we repeat the exercise with a smaller learning
rate, let's try gradient clipping.  The clipping, performed with
\texttt{SG\_clip} control option, limits the squared $L_{2}$-norm of
the gradient with the given value while keeping it's direction:
<<>>=
res <- maxSGA(grad=gradlik,
              start=start,
              nObs=nrow(X),
              control=list(iterlim=1000,
                           SG_clip=1e6)  # allow ||g|| <= 1000
              )
summary(res)
@
This time the gradient did not explode and we were able to get a
result.  But the estimates are rather far from the analytic solution
shown above, in particular the constant estimate
\Sexpr{round(coef(res)[1], 3)} is very different from the
corresponding analytic value \Sexpr{round(betaX[1], 3)}.  Let's
analyze what is happening inside the optimizer.  We can ask for both
the parameter values to be stored
for each epoch:
<<fig=TRUE>>=
res <- maxSGA(grad=gradlik,
              start=start,
              nObs=nrow(X),
              control=list(iterlim=100,
                           SG_clip=1e6,  # allow ||g|| <= 1000
                           storeParameters=TRUE
                           )  
              )
par <- storedParameters(res)
plot(par[,1], par[,2], type="b")
@



Let's try
increasing the number of iterations first:
<<>>=
res <- maxSGA(grad=gradlik,
              start=start,
              nObs=nrow(X),
              control=list(iterlim=100000,
                           SG_clip=1e6)  # allow ||g|| <= 1000
              )
coef(res)
@
Now we moved closer but we are still far from the correct solution.  


<<eval=FALSE>>=
##
## demonstrate the usage of index, and using
## fn for computing the objective function on validation data.
## create a linear model where variables are very unequally scaled
##
## OLS loglik function: compute the function value on validation data only
loglik <- function(theta, index) {
   e <- yValid - XValid %*% theta
   -crossprod(e)/length(y)
}
N <- 1000
## two random variables: one with scale 1, the other with 100
X <- cbind(rnorm(N), rnorm(N, sd=100))
theta <- c(1, 1)  # true parameter values
y <- X %*% theta + rnorm(N, sd=0.2)
## training-validation split
iTrain <- sample(N, 0.8*N)
XTrain <- X[iTrain,,drop=FALSE]
XValid <- X[-iTrain,,drop=FALSE]
yTrain <- y[iTrain]
yValid <- y[-iTrain]
##
cat("Analytic training solution:\n")
print(solve(crossprod(XTrain)) %*% crossprod(XTrain, yTrain))
## do this without momentum: learning rate must stay small for the gradient not to explode
@

\bibliographystyle{apecon}
\bibliography{sga}

\end{document}
