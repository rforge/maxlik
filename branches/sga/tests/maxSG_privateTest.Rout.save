
R version 3.6.2 (2019-12-12) -- "Dark and Stormy Night"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ### tests for stochastic gradient ascent
> ### Test the following things:
> ###
> ### 1. basic SGA
> ### 2. SGA w/momentum
> ### 3. SGA full batch
> ### 4. SGA, no gradient supplied
> 
> library(maxLik)
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> library(testthat)
> 
> ## ---------- OLS 
> ## log-likelihood function(s):
> ## return log-likelihood on validation data
> loglik <- function(beta, index) {
+    e <- yValid - XValid %*% beta
+    -crossprod(e)/length(y)
+ }
> ## gradlik: work on training data
> gradlik <- function(beta, index) {
+    e <- yTrain[index] - XTrain[index,,drop=FALSE] %*% beta
+    l <- crossprod(e)
+    g <- t(-2*t(XTrain[index,,drop=FALSE]) %*% e)
+    -g/length(index)
+ }
> 
> ### create random data
> set.seed(1)
> N <- 1000
> x <- rnorm(N)
> X <- cbind(1, x)
> y <- 1 + x + rnorm(N)
> ## training-validation
> iTrain <- sample(N, 0.8*N)
> XTrain <- X[iTrain,,drop=FALSE]
> XValid <- X[-iTrain,,drop=FALSE]
> yTrain <- y[iTrain]
> yValid <- y[-iTrain]
> cat("Analytic solution (training data):\n")
Analytic solution (training data):
> start <- c(const=10, x=10)
> b0 <- drop(solve(crossprod(XTrain)) %*% crossprod(XTrain, yTrain))
> names(b0) <- names(start)
> tol <- 1e-1  # coefficient tolerance
> 
> ## ---------- 1. working example
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SGA_batchSize=100, SGA_learningRate=0.1,
+                          storeValues=TRUE),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
>                            # SGA usually ends with gradient not equal to 0 so we don't test that
> 
> ## ---------- 2. SGA with momentum
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SGA_batchSize=100, SGA_learningRate=0.1, SGA_momentum=0.9,
+                          storeValues=TRUE),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- 3. full batch
> res <- maxSGA(loglik, gradlik, start=start,
+             control=list(printLevel=0, iterlim=200,
+                          SGA_batchSize=NULL, SGA_learningRate=0.1,
+                          storeValues=TRUE),
+             nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> ## ---------- 4. no gradient
> res <- maxSGA(loglik, start=start,
+               control=list(iterlim=1000, SGA_learningRate=0.02), nObs=length(yTrain))
> expect_equal(coef(res), b0, tolerance=tol)
> 
> proc.time()
   user  system elapsed 
  1.138   0.585   0.858 
