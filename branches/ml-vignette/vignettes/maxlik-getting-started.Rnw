\documentclass{article}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xspace}

\newcommand{\elemProd}{\ensuremath{\odot}}  % elementwise product of matrices
\newcommand*{\mat}[1]{\mathsf{#1}}
\newcommand{\likelihood}{\mathcal{L}}% likelihood
\newcommand{\loglik}{\ell}% log likelihood
\newcommand{\maxlik}{\texttt{maxLik}\xspace}
\newcommand*{\transpose}{^{\mkern-1.5mu\mathsf{T}}}
%\newcommand{\transpose}{\intercal}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}

% \VignetteIndexEntry{SGA introduction: the basic usage of maxSGA}

\begin{document}
<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60,
        try.outFile=stdout()  # make try to produce error messages
        )
foo <- packageDescription("maxLik")
library(ggplot2)
library(magrittr)
@

\title{Getting started with maximum likelihood and \texttt{maxLik}}
\author{Ott Toomet}
\maketitle


\section{Introduction}

This vignette is intended for those who are rather new for both
maximum likelihood (ML) estimation and the \texttt{maxLik} package.
The potential target group contains advanced undergraduate students in
technical fields, such as statistics or economics, graduate students
in social sciences or engineering who have to devise their own
estimators, and research stuff and practitioners who are just moving
into ML tasks.  If you are familiar enough with the concept of ML, you
may
skip the theoretical introduction and go straight to the
Section~\ref{sec:maxlik-usage} for \texttt{maxLik} usage.


\section{What is maximum likelihood estimation?}
\label{sec:what-is-ml}

Maximum Likelihood (ML) is in essence of maximizing \emph{likelihood}
over the parameters of interest.  This section explains what is
likelihood and how is it related to probability.  We start by
introducing two important concepts for probability distributions,
probability mass function and probability density function.
Thereafter we explain how one transforms probabilities to likelihood,
and what are the parameters there.  Finally, we also incorporate data
as most common ML applications are based on data.


\subsection{Probability mass function and probability density
  function}
\label{sec:pmf-pdf}

We start with a very simple example.  Imagine you are tossing a fair
coin.  What are the possible outcomes and what are the related
probabilities?  Obviously, in case of a coin there are only two
outcomes, heads $H$ and tails $T$.  If the coin is fair, both of these
will have probability exactly 0.5.  We can denote tails with 0, heads
with 1, and make a plot of the result
(Figure~\ref{fig:fair-coin-pmf}).

\begin{figure}[ht]
  \begin{center}
<<fig=TRUE, echo=FALSE, height=3.2>>=
N <- 1
p <- 0.5
data.frame(n=0:N, p=dbinom(0:N, N, p)) %>%
   ggplot(aes(n, p)) +
   geom_segment(aes(xend=n, yend=0), size=1) +
   geom_point(size=2) +
   labs(x = "Number of heads", y = "Probability") +
   scale_x_continuous(breaks=0:1, labels=0:1,
                      limits=c(-0.2, 1.2))
@   
\end{center}
\caption{Probability mass function for a fair coin toss}
\label{fig:fair-coin-pmf}
\end{figure}

The plot shows the probabilities for all possible events for our coin
toss example.  This is called \emph{probability mass function} (pmf).
Pmf just tells you the probability for every possible event.  Our coin toss
has only two possible events, tails (0) and heads (1), and hence its pmf
only has two points of support.  Such process is called \emph{Bernoulli
  process}.  More specifically, this is \emph{Bernoulli(0.5)} process
as for the fair coin the probability of ``success'' (i.e. heads) is
0.5.  If the coin is not fair, we denote the corresponding process
Bernoulli($p$), where $p$ is the probability to get heads.

Now lets toss the coin two times.  What is the probability that we end
up with one heads and one tails?  First, as the coin tosses are
independent,\footnote{Events are independent when outcome of one event
  does not influence the outcome of the other event.  Here the result
  of the second toss is not affected by the first toss.}
we can just multiply the probabilities: $0.5$ for one heads and $0.5$
for one tails, $0.25$ when multiplied.  However, this is not the whole
story--we have two possibilities to get one heads and one tails,
either $H$, $T$ or $T$, $H$.  Both of these events are equally likely,
so the final answer will be 0.5.

But now imagine we do not know if the coin is fair.  Maybe it is not a
coin but and object of complex shape instead.  How can we tell if it
is fair or not?  We can repeat the exercise what we did with two
coins.  But as we don't know the true probability of heads, lets
denote it with $p$.  The probability of tails will be $1-p$
accordingly, and the probability of tossing the coin two times will be
$2 p (1-p)$, $p$ for one heads, $1-p$ for one tails, and ``2'' takes
into account the fact that we can get one tails, one heads in two
possible orders.

This probability is essentially the likelihood.  We usually denote the
likelihood with $\likelihood(p)$, stressing that it depends on the
model parameter $p$.  Hence in this example we have
\begin{equation}
  \label{eq:2-coin-likelihood}
  \likelihood(p) = 2 \, p \, (1-p).
\end{equation}
Let's repeat here what did we do above:
\begin{enumerate}
\item First, we described the coin toss result as Bernoulli($p$)
  process.  In order to apply maximum likelihood method, we always
  need a probability model to describe our data.  $p$, the probability
  of heads, is the model parameter we typically don't know.  Bernoulli
  process has only a single parameter, but more complex processes can
  have many more.
\item Thereafter we computed the probability to observe the data--one
  heads, one tails.
  This is only possible if we have a suitable
  probability model.  As the model contains (usually unknown)
  parameters, the probability will also contain parameters.
\item And finally we just called this probability
  \emph{likelihood} $\likelihood(p)$ to
  stressed that it depends on the model parameter.
\end{enumerate}

What is left is to use this likelihood function to \emph{estimate} the
parameters.  These are normally unknown, and we want to use
data to find out the best possible set of parameters.  \emph{Maximum
  likelihood} (ML) method finds such combination of parameters' values that
maximizes the likelihood function.  It can be shown that such value
has number of desirable properties, in particular it will become
increasingly similar to the ``true value'' on increasingly large
dataset, given that our probability model is correct (it is a
consistent estimator).  These desirable
properties, and relative simplicity of the method, have made ML one of
the most widely used statistical estimators.

Let us now generalize the example we did above, and convert it into a
form suitable for ML estimation.  Assume we toss a coin of unknown
``fairness'' where we just denote the probability to receive heads
with $p$.  Further, assume that out of $N$ trials, $N_{H}$ trials
turned out heads and $N_{T}$ trials turned tails.  The probability of
this occuring is
\begin{equation}
  \label{eq:general-cointoss-probability}
  \binom{N}{N_{H}} \, p^{N_{H}} \, (1 - p)^{N_{T}} =
  \likelihood(p; N_{H}, N_{T})
\end{equation}
The binomial coefficient $\displaystyle\binom{N}{N_{H}} = \displaystyle\frac{N!}{N_{H}!(N -
  N_{H})!}$ takes into account that there are many ways to how heads
and tail can turn up while still resulting in $N_{H}$ heads and
$H_{T}$ tails, in the example above there were just two possible
combinations.  The probability depends on both the parameter $p$ and
data--the corresponding counts $N_{H}$ and $N_{T}$.  We stress this in
notation by writing $p$ in the first position followed by data. 

Technically, it is easier to work with log-likelihood instead of
likelihood (as log is monotonic function, maximum of likelihood and
maximum of log-likelihood occur at the same parameter value).
We denote log-likelihood by $\loglik$ and write
\begin{equation}
  \label{eq:general-cointoss-loglik}
  \loglik(p; N_{H}, N_{T}) =
  \log\likelihood(p; N_{H}, N_{T}) =
  \log \binom{N}{N_{H}} +
  N_{H} \log p + N_{T} \log (1 - p).
\end{equation}
ML method means finding value of $p$ that maximizes this expression.
Fortunately, the binomial coefficient $\binom{N}{N_{H}}$ does not
depend on the parameter $p$.  Intuitively, $p$ determines the
probability of various combinations of heads and tails, but \emph{what
  kind of combinations are possible} does not depend on $p$.  Hence we
can ignore the first term on the right hand side
of~\eqref{eq:general-cointoss-loglik} when maximizing the
log-likelihood.  Such approach is very common in practice, many terms
that are invariant with respect to parameters are often ignored.
Hence, with certain abuse of notation we can rewrite the
log-likelihood as
\begin{equation}
  \label{eq:general-cointoss-partial-loglik}
  \loglik(p; N_{H}, N_{T}) =
  N_{H} \log p + N_{T} \log (1 - p).
\end{equation}
It is easy to check that the solution, the value of $p$ that maximizes
log-likelihood~\eqref{eq:general-cointoss-partial-loglik} is
\begin{equation}
  \label{eq:general-cointoss-solution}
  p^{*} =
  \frac{N_{H}}{N}.
\end{equation}
This should be surprise to no-one--the intuitive 
``fairness'' of the coin is just the average percentage of heads we
get.

Next, we look at an example with continuous outcomes--linear
regression.


\subsection{Continuous variables: probability density and likelihood}
\label{sec:continuous-outcomes}








Use all examples with data



Continuous data: normal distribution, cats' body weight using
boot::catsM data.  pdf is not probability.  Stress it contains two parameters



\subsection{From probability to likelihood}
\label{sec:from-probability-likelihood}

Kind-of-the same thing: devise likelihood for the previous examples

Vector arguments: put the $\mu$, $\sigma$ into the same vector


\subsection{Non-linear optimization}
\label{sec:non-linear-optimization}

Why do we need it, and why we do the stuff in vector form

What are the issues in high dimensions

Why gradients are needed



\section{How to use maxLik}
\label{sec:maxlik-usage}


\subsection{Basic usage}
\label{sec:basic-usage}

maxLik function

summary, coef, stdEr

vector arguments

select optimizers

BHHH

providing analytic derivatives

\subsection{Advanced usage}
\label{sec:advanced-usage}

condiNumber, compareDerivatives

fixed coefficients

mention SGA

\bibliographystyle{apecon}
\bibliography{sga}

\end{document}
