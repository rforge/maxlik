\documentclass{article}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage[inline]{enumitem}
\usepackage[T1]{fontenc}
\usepackage[bookmarks=TRUE,
            colorlinks,
            pdfpagemode=none,
            pdfstartview=FitH,
            citecolor=black,
            filecolor=black,
            linkcolor=blue,
            urlcolor=black,
            ]{hyperref}
\usepackage{graphicx}
\usepackage{icomma}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}  % for extended pderiv arguments
\usepackage{natbib}
\usepackage{xargs}  % for extended pderiv arguments
\usepackage{xspace}

\newcommand{\COii}{\ensuremath{\mathit{CO}_{2}}\xspace}
\DeclareMathOperator*{\E}{\mathbbm{E}}% expectation
\newcommand*{\mat}[1]{\mathsf{#1}}
\newcommand{\likelihood}{\mathcal{L}}% likelihood
\newcommand{\loglik}{\ell}% log likelihood
\newcommand{\maxlik}{\texttt{maxLik}\xspace}
\newcommand{\me}{\mathrm{e}} % Konstant e=2,71828
\newcommandx{\pderiv}[3][1={}, 2={}]{\frac{\partial^{#2}{#1}}{\mathmbox{\partial{#3}}^{#2}}}
% #1: function to differentiate (optional, empty = write after the formula)
% #2: the order of differentiation (optional, empty=1)
% #3: the variable to differentiate wrt (mandatory)
\newcommand{\R}{\texttt{R}\xspace}
\newcommand*{\transpose}{^{\mkern-1.5mu\mathsf{T}}}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}
% \VignetteIndexEntry{Maximum likelihood estimation with maxLik}

\title{Maximum Likelihood Estimation with \emph{maxLik}}
\author{Ott Toomet}

\begin{document}
\maketitle

<<echo=FALSE>>=
library(maxLik)
set.seed(6)
@ 

\section{Introduction}
\label{sec:introduction}

This vignette is intended for users who are well familiar with
concepts of likelihood, log-likelihood, and related methods, such as
information equality and BHHH approximation, and with \R language.
The vignette 
focuses on \maxlik usage and does not explain the underlying
mathematical concepts.  If you need a refresher, consult the
accompanied vignette ``Getting started with maximum likelihood and
\maxlik''. 
Potential target group
includes researchers, graduate students, and industry practitioners
who want to apply their own custom maximum likelihood estimators.

The next section introduces the basic usage, including the \maxlik
function, the main entry point for the package, gradients, selecting
different optimizers, and controlling the optimization behavior.
These are topics that are hard to avoid when working with applied ML
estimation.  Section~\ref{sec:advanced-usage}
contains a selection of more niche topics, including
arguments to the log-likelihood function, other types of optimization,
testing condition numbers and constrained optimization.


\section{Basic usage}
\label{sec:basic-usage}

\subsection{The maxLik function}
\label{sec:maxlik-function}

The main entry point to \maxlik functionality is the function of
the same name, \verb|maxLik|.  It is a wrapper around the underlying
optimization algorithms that also ensures that the returned object is
of the right class, so that one
can use the convenience functions, such as \verb|summary| or
\verb|logLik| with it.

The basic usage of the function is very simple: one has just to submit
the likelihood function (argument \verb|logLik|) and start value
(argument \verb|start|).  Let us demonstrate the basic usage
by estimating the normal distribution parameters.  We create 100 standard normals,
and estimate the best fit mean and standard deviation.  For a refresher,
the normal probability density function is
\begin{equation}
  \label{eq:normal-pdf}
  f(x; \mu, \sigma) =
  \frac{1}{\sqrt{2\pi}}
  \frac{1}{\sigma}
  \,
  \me^{
    -\frac{1}{2}
    \frac{(x - \mu)^{2}}{\sigma^{2}}
  }.
\end{equation}
and hence the log-likelihood contribution of $x$ is
\begin{equation}
  \label{eq:normal-loglik}
  \loglik(\mu, \sigma; x)
  =
  - \log{\sqrt{2\pi}}
  - \log \sigma
  - \frac{1}{2} \frac{(x - \mu)^{2}}{\sigma^{2}}.
\end{equation}
Instead of explicitly coding our own version, we can instead rely on
\R function \verb|dnorm| (see Section~\ref{sec:different-optimizers}
for a version that does not rely on \verb|dnorm|):
<<>>=
x <- rnorm(100)  # data.  true mu = 0, sigma = 1
loglik <- function(theta) {
   mu <- theta[1]
   sigma <- theta[2]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
m <- maxLik(loglik, start=c(mu=1, sigma=2))
                           # give start value somewhat off
summary(m)
@ 
The algorithm converged in 7 iterations and one can check that the
results are equal to the sample mean and
variance.\footnote{Note that \R function \texttt{var} returns the
  unbiased
  estimator by using denominator
  $n-1$, the ML estimator is biased with denominator $n$.
}

This example demonstrates a number of key features of \verb|maxLik|:
\begin{itemize}
\item The first argument of \verb|logLik| is the parameter vector.  In
  this example we define it as $\vec{\theta} = (\mu, \sigma)$, and the
  first lines of \verb|logLik| are used to extract these values from
  the vector.
\item The \verb|logLik| function returns a single number, sum of
  individual log-likelihood contributions of individual $x$
  components.  (It may also return the components individually, see
  the explanation of the BHHH method
  in Section~\ref{sec:different-optimizers} below.)
\item Vector of start values must be of correct length.  If the
  components are named, those names are also displayed for \verb|summary|
  (and for \verb|coef| and \verb|stdEr|, see below).
\item \verb|summary| method displays a handy summary of the results,
  including the convergence message, the estimated values, and
  statistical significance test results.
\end{itemize}
As we did not specify the optimizer, \verb|maxLik| picked
Newton-Raphson by default, and computed the necessary gradient and
Hessian matrix numerically.

Besides summary, \verb|maxLik| also contains a number of utility
functions to simplify handling of estimated models:
\begin{itemize}
\item \verb|coef| extracts the model coefficients:
<<>>=
coef(m)
@   
\item \verb|stdEr| returns the standard errors (by inverting the
  corresponding Hessian):
<<>>=
stdEr(m)
@   
\item Other functions include \verb|logLik| to return the
  log-likelihood value, \verb|returnCode| and \verb|returnMessage| to
  return the convergence code and message respectively, and \verb|AIC|
  to return Akaike's information criterion.  See the respective
  documentation for more information.
\item One can also query the number of observations with \verb|nObs|,
  but this requires likelihood values to be supplied by observation (see
  the BHHH method in Section~\ref{sec:different-optimizers} below).
\end{itemize}


\subsection{Supplying gradients}
\label{sec:supplying-gradients}

The simple example above worked fast and well.  In particular, the
numeric gradient \verb|maxLik| had to compute implicitly
did not pose any problems.  But users are strongly
advised to supply analytic gradient, or even better, both gradient
and Hessian.  More complex problems
may be intractably slow, completely fail to converge, or converge to a
sub-optimal solution if numeric gradients are too noisy.  Needless to
say, unreliable Hessian also leads to unreliable inference.  Here we
demonstrate how to supply gradient to the \verb|maxLik| function.

It is easy to see from~\eqref{eq:normal-loglik} that the gradient
components are
\begin{equation}
  \label{eq:loglik-gradient}
  \begin{split}
    \pderiv{\mu}\loglik(\mu, \sigma; x) &=
    \frac{x - \mu}{\sigma^{2}}
    \\
    \pderiv{\sigma}\loglik(\mu, \sigma; x) &=
    -\frac{1}{\sigma} + \frac{(x - \mu)^{2}}{\sigma^{3}}.
  \end{split}
\end{equation}
Hence we can program the gradient function as
<<>>=
gradlik <- function(theta) {
   mu <- theta[1]
   sigma <- theta[2]
   N <- length(x)  # number of observations
   gradient <- numeric(2)  # create an empty gradient vector
   gradient[1] <- sum((x - mu)/sigma^2)
   gradient[2] <- -N/sigma + sum((x - mu)^2/sigma^3)
   gradient
}
@ 
Note that we have written $-N/\sigma$ as the first term for the second
component because the
term $-1/\sigma$ must be summed over all observations (despite it being
invariant).  We also return gradient as a length-2 vector, not a
matrix.  The example with BHHH below demonstrates gradient in a matrix form.

Now we can supply this function to \verb|maxLik| as
<<>>=
m <- maxLik(loglik, gradlik, start=c(mu=1, sigma=2))
summary(m)
@ 
While these results are identical to the ones above with numeric
gradients, the algorithm is now much more robust, and much faster in
case of large number of parameters.

While this is perhaps the most straightforward way to compute
gradients, \maxlik also supports gradients (and Hessian) supplied as
log-likelihood function attribute.  This is motivated by the fact that
computing gradient often involves a number of similar computations as
computing log-likelihood, and one may want to re-use the auxiliary results.  We
demonstrate this on the same example, but this time we do not rely on
the \verb|dnorm| function in order to show how certain results can be
reused:
<<>>=
loglikA <- function(theta) {
   ## log-likelihood with gradent attribute
   mu <- theta[1]
   sigma <- theta[2]
   N <- length(x)  # number of observations
   sse <- sum((x - mu)^2)  # we re-use this value for the gradient
   ## compute log likelihood
   ll <- -N*log(sqrt(2*pi)) - N*log(sigma) - 0.5*sse/sigma^2
   ## compute gradient
   gradient <- numeric(2)
   gradient[1] <- sum(x - mu)/sigma^2
   gradient[2] <- -N/sigma + sse/sigma^3
   attr(ll, "gradient") <- gradient
                           # add gradient as attribute 'gradient'
   ll
}
m <- maxLik(loglikA, gradlik, start=c(mu=1, sigma=2))
summary(m)
@
The log-likelihood with attribute \verb|loglikA| computes
log-likelihood as above, but also computes its gradient, and adds it
as attribute ``gradient'' to the log-likelihood.  This gives a
potential efficiency gain as sum of squared errors, \verb|sse|, is
re-used.  \maxlik checks the presence of the attribute, and if it is
there, it uses the provided gradient.  Obviously, in real applications
the efficiency gain will depend on the amount of computations re-used,
and the number of likelihood calls versus gradient calls.

While analytic gradients are always helpful and often necessary, they
may be hard to derive and code.  In order to help to derive and debug the
analytic gradient, another provided function,
\verb|compareDerivatives|, takes the log-likelihood function, analytic
gradent, and compares the numeric and analytic gradient.  As an
example, we compare the log-likelihood and gradient functions we coded above:
<<>>=
compareDerivatives(loglik, gradlik, t0=c(1,2))
                           # 't0' is the parameter value
@
The function prints the analytic gradient, numeric gradient, their
relative difference, and the largest relative difference value (in absolute
value).  The latter is handy in case of large gradient vectors where
it may be hard to spot a lonely component that is off.
In case of reasonably smooth functions, expect
the relative difference to be smaller than $10^{-7}$.  In this simple
case it is of order $10^{-9}$.

Note that the gradient of log-likelihood, a scalar function, with
respect to length-2 parameter vector is
$1\times2$ matrix.  \maxlik uses numerator layout.

\verb|compareDerivatives| supports vector functions, so one can use
the same to test analytic Hessian by calling it with \verb|gradlik| as
the first argument and the analytic hessian as the second argument. 


\subsection{Different optimizers}
\label{sec:different-optimizers}

By default, \maxlik uses Newton-Raphson optimizer but one can easily
swap the optimizer by \verb|method| argument.  The supported
optimizers include ``NR'' for the default Newton-Raphson, ``BFGS'' for
gradient-only Broyden-Fletcher-Goldfarb-Shannon, ``BHHH'' for the
information-equality based Berndt-Hall-Hall-Hausman, and ``NM'' for
gradient-less Nelder-Mead.  Different optimizers have their different
strengths and weaknesses.  They may also be based on very different
approaches and certain concepts, such as \emph{iteration}, may mean
quite different things for different methods.

For instance, although Newton-Raphson is a simple, fast and intuitive
method that approximates the function with a parabola, it needs to
know the Hessian matrix (the second derivatives).  This is usually
even harder to program, and even slower and more error-prone when
computed numerically.  Let us replace NR with gradient-only
BFGS method.  It is a quasi-Newton method that computes its own
internal approximation of the Hessian while relying only on gradients:
<<>>=
m <- maxLik(loglik, gradlik, start=c(mu=1, sigma=2),
            method="BFGS")
summary(m)
@ 
One can see that the results were identical, but while NR converged in
7 iterations, it took 20 iterations for BFGS.  This is a manifestation
of the fact that providing more information (in this case gradient)
helps to find the solution.
In a similar fashion, one can simply drop in most other provided optimizers.

One method that is very popular for ML estimation is
BHHH.  We discuss it here at length because that method requires both
log-likelihood and gradient function to return a somewhat different
value.  The essence of BHHH is information equality, the fact that in
case of log-likelihood function $\loglik(\theta)$, the expected value
of Hessian at the true parameter value $\vec{\theta}_{0}$ can be
expressed through the expected value of the outer product of the
gradient: 
\begin{equation}
  \label{eq:information-equality}
  \E
  \left[
    \frac{\partial^2 l(\vec{\theta})}
    {\partial\vec{\theta}\, \partial\vec{\theta}'}
  \right]_{\vec{\theta} = \vec{\theta}_0}
  =
  - \E
  \left[
    \left.
      \frac{\partial l(\vec{\theta})}
      {\partial\vec{\theta}'}
    \right|_{\vec{\theta} = \vec{\theta}_0}
    \cdot
    \left.
      \frac{\partial l(\vec{\theta})}
      {\partial\vec{\theta}}
    \right|_{\vec{\theta} = \vec{\theta}_0}
  \right].
\end{equation}
Hence we can approximate Hessian by the average outer product of the
gradient.  Obviously, this is only an approximation, and it is less
correct when we are far from the true value $\vec{\theta}_{0}$.  Note
also that information equality relies on the assumption that the
observations are independent, and may not work with non-independent
data, such as time series.

However, in order to compute the average outer product, we need to
compute gradient \emph{by observation}.  Hence it is not enough to
just return a single gradient vector, we have to compute a matrix
where rows correspond to individual data points and columns to the
parameter components.  The following example demonstrates the usage:
<<>>=
gradlikB <- function(theta) {
   ## BHHH-compatible gradient
   mu <- theta[1]
   sigma <- theta[2]
   N <- length(x)  # number of observations
   gradient <- matrix(0, N, 2)  # gradient is matrix:
                           # N datapoints (rows), 2 components
   gradient[, 1] <- (x - mu)/sigma^2
                           # first column: derivative wrt mu
   gradient[, 2] <- -1/sigma + (x - mu)^2/sigma^3
                           # second column: derivative wrt sigma
   gradient
}
m <- maxLik(loglik, gradlikB, start=c(mu=1, sigma=2),
            method="BHHH")
summary(m)
@ 
The code of the new gradient function is similar to the previous one,
but we do not sum over the individual values.  Instead, we fill the
rows of the $N\times2$ gradient matrix with the values
observation-wise.

In case we do not have time and energy to code the analytic gradient, we can
let \maxlik figure out a numeric one for BHHH too.
In this case we have to supply the
log-likelihood by observation.  This essentially means we remove
summing from the original likelihood function:
<<>>=
loglikB <- function(theta) {
   mu <- theta[1]
   sigma <- theta[2]
   -log(sqrt(2*pi)) - log(sigma) - 0.5*(x - mu)^2/sigma^2
                           # no summing here
                           # also no 'N*' terms as we work by individual observations
}
m <- maxLik(loglikB, start=c(mu=1, sigma=2),
            method="BHHH")
summary(m)
@

Besides of relying on information equality, BHHH is
essentially the same algorithm as NR.  As the Hessian is just
approximated, its is converging at a slower pace than NR with analytic
Hessian.  But when relying on numeric derivatives, BHHH may be more
reliable. 

For convenience, the other methods also support
observation-wise gradients and log-likelihood values, those numbers
are just summed internally.  So one can just code the problem in an 
BHHH-compatible manner and use it for all supported optimizers.

\maxlik package also supports stochastic gradient ascent optimizer.
However, as that method is rarely ever used for ML estimation, it
cannot be supplied through the ``method'' argument.  Consult
the separate vignette ``Stochastic gradient ascent in \maxlik''.


\subsection{Control options}
\label{sec:control-options}

\maxlik supports a number of control options, most of which can be
supplied through \verb|control=list(...)| method.  Some of the most
important options include 
\verb|printLevel| to control debugging information, \verb|iterLim| to
control the maximum number of iterations, and various
\verb|tol|-parameters to control the convergence tolerances.  For
instance, we can limit the iterations to two, while also printing out
the parameter estimates at each step.  We use the previous
example with BHHH optimizer:
<<>>=
m <- maxLik(loglikB, start=c(mu=1, sigma=2),
            method="BHHH",
            control=list(printLevel=3, iterlim=2))
summary(m)
@
The first option, \verb|printLevel=3|, prints out parameters, gradient
a few other pieces of information at every step.  Larger levels output
more information, printlevel 1 only
prints the first and last parameter values.
The output from \maxlik-implemented
optimizers is fairly consistent, but methods that are call optimizers in
other packages, such as BFGS, may output debugging
information in a quite different way.  The second option,
\verb|iterLim=2| stops the algorithm after two iterations.  It returns
with
code 4:
iteration limit exceeded. 

Other sets of handy options are the convergence tolerances.  There are
three convergence tolerances:
\begin{description}
\item[tol] This measures
  the absolute convergence tolerance.  Stop if
  successive function evaluations
  differ by less than \emph{tol} (default $10^{-8}$).  
\item[reltol] This is somewhat similar to \emph{tol}, but relative to
  the function value.  Stop if successive function evaluations differ by less than
  $\mathit{reltol}\cdot (\loglik(\vec{\theta}) + \mathit{reltol})$
  (default \verb|sqrt(.Machine[["double.eps"]])|).
\item[gradtol] stop if the (Euclidean) norm of the gradient is smaller
  than this value (default $10^{-6}$).
\end{description}
The default values are typically good enough, but in certain cases one
may want to switch off certain tolerance-based criteria.  For
instance, in case our function values are very small, we may want to
rely only on the relative tolerance.  A simple way to achieve this is
to set both \emph{tol} and \emph{gradtol} to negative values.  In that
case these two conditions are never satisfied and the algorithm stops
only when the relative convergence criterion is fulfilled.  For
instance, in the previous case we get:
<<>>=
m <- maxLik(loglikB, start=c(mu=1, sigma=2),
            method="BHHH",
            control=list(tol=-1, gradtol=-1))
summary(m)
@ 

Note that stochastic gradient ascent
relies on different convergence criteria.  See the dedicated vignette
``Stochastic Gradient Ascent in \maxlik''.


\section{Advanced usage}
\label{sec:advanced-usage}

This section describes more advanced and less frequently used aspects
of \maxlik.

\subsection{Additional arguments to the log-likelihood function}
\label{sec:additional-arguments-loglik}

The only rule about the arguments for the log-likelihood is that its
first argument must be the parameter vector.  But one can have more
arguments and pass those through \maxlik as additional arguments to
the \verb|maxLik| function.  For instance, let's change the
log-likelihood function in a way that it expects data $\vec{x}$ to be
passed as an argument.  Now we just have to add \verb|x| as an
additional argument:
<<>>=
loglik <- function(theta, x) {
   mu <- theta[1]
   sigma <- theta[2]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
m <- maxLik(loglik, start=c(mu=1, sigma=2), x=x)
summary(m)
@
This approach works with most of the argument names, except with those
that are the same as \verb|maxLik|'s arguments.  In the latter case it
prints an informative error message.


\subsection{Maximizing other functions}
\label{sec:maximizing-other-functions}

The \verb|maxLik| function is basically just a wrapper to a number of
maximization algorithms, together with likelihood-related goodies like
standard errors.  However, from time-to-time we need to optimize other
functions where inverting the Hessian to compute standard errors is
not applicable.  In such cases one can call the included optimizers
directly, using the form \verb|maxXXX| where \verb|XXX| stands for the
name of the method, e.g. \verb|maxNR| for Newton-Rapshon
(\verb|method="NR"|) and \verb|maxBFGS| for BFGS.  There is also
\verb|maxBHHH| although the information equality--based BHHH is not
correct if we do not work with likelihood function.  Let us optimize
the 2-dimensional bell curve,
\begin{equation}
  \label{eq:2d-bell-curve}
  f(x, y) = \me^{-x^{2} - y^{2}}:
\end{equation}
<<>>=
f <- function(theta) {
   x <- theta[1]
   y <- theta[2]
   exp(-x^2 - y^2)
                           # optimum at (0, 0)
}
m <- maxBFGS(f, start=c(1,1))
                           # give start value a bit off
summary(m)
@
Note that the summary output is slightly different: if reports the
parameter and gradient value, appropriate for a task that is not
likelihood optimization.  Note also that BFGS, based on the
\verb|stats::optim| does not report the convergence results in a
similar way as BHHH and NR, the algorithms provided by the \maxlik
package. 

\subsection{Testing condition numbers}
\label{sec:testing-condition-numbers}

Non-linear optimizers perform best in regions that are roughly round.
In an area that is fairly flat, they performance will degrade and they
may fail to converge completely.  The flatness may occur for different
reasons, below we demonstrate a simple linear regression example with
very different scale of the variables.  We create three variables,
$x_{1}$, $x_{2}$ and $x_{3}$ at very different scale, and set up a
linear regression task
\begin{equation}
  \label{eq:different-scale-linear-regression}
  y_{i} = x_{1} + x_{2} + x_{3} + \epsilon_{i}.
\end{equation}
Hence the correct parameter values are $\beta_{1} = \beta_{2} =
\beta_{3} = 1$.
<<echo=FALSE>>=
set.seed(4)
@ 
<<>>=
## create 3 variables with very different scale
x1 <- rnorm(100)
x2 <- rnorm(100, sd=1e4)
x3 <- rnorm(100, sd=1e7)
## note: correct coefficients are 1, 1, 1
y <- x1 + x2 + x3 + rnorm(100)
TSS <- function(theta) {
   ## total sum of squares
   yhat <- theta[1]*x1 + theta[2]*x2 + theta[3]*x3
   -sum((yhat - y)^2)
   # note: '-' as we are maximizing
}
m <- maxNR(TSS, start=c(x1=0, x2=0, x3=0))
                           # give start values a bit off
summary(m)
@
As one can see, the optimizer gets stuck and returns completely wrong
values.  The reason here is that the loss function (TSS) is of
extremely elongated shape, and the parabolic approximation of
Newton-Raphson optimizer fails to represent it correctly.
In this particular case one can easily solve the problem by
re-scaling the variables, introducing analytic derivatives, or
choosing a more robust optimizer.
But
this is not always possible, in particular complex
function in high dimensions may contain flat regions of unknown shape
where the algorithm cannot easily proceed.

In such conditions one may want to test the
condition number of the Hessian matrix.  \verb|condinumber| runs such
test variable-by-variable:
<<>>=
condiNumber(hessian(m))
@ 
Condition number of the first column of the Hessian ($1\times1$ matrix) is always
one.  When we the second column corresponding to $x_{2}$, the
condition number is still small.  However, bringing in the last column
makes the Hessian not well conditioned.  This hints that the problem
is related to $x_{3}$, or more specifically, $x_{3}$ does not play
well along with $x_{1}$ and $x_{2}$.  Such knowledge occasionally
helps to debug and address the convergence problems.

What helps here to locate the problem is to see how the condition
number increases when we keep adding columns.  This helps to see which
variable does not play together with others in a nice manner.  If we
only observe the condition number of the complete Hessian, we do not
gain such insight.


\subsection{Fixed parameters and constrained optimization}
\label{sec:fixed-parameters}

\maxlik supports three types of constrains.  The first one is the
simplest: just keeping values of certain parameters fixed.  The other
two, general linear equality and inequality constraints are somewhat
more complex.

Occasionally we want to treat one of the model parameters as
constant.  This can be achieved in a very simple manner, just through
the argument \verb|fixed|.  It must be an index vector,
such as \verb|c(2,4)|, \verb|c(FALSE, TRUE, FALSE, TRUE)|, or
\verb|c("beta2", "beta4")| if \verb|start| has names.  We demonstrate
this with the original example where the task is to estimate the
parameters for normal distribution.  However, this time we fix $\sigma
= 1$:
<<>>=
x <- rnorm(100)
loglik <- function(theta) {
   mu <- theta[1]
   sigma <- theta[2]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
m <- maxLik(loglik, start=c(mu=1, sigma=1),
            fixed="sigma")
                           # fix the component names 'sigma'
summary(m)
@
The result has $\sigma$ equal to exactly $1$, it's standard error $0$,
and $t$ value undefined.
The fixed components are ignored when computing gradients and Hessian,
so the standard errors are correct.

Next, we demonstrate equality constraints.  We take the
two-dimensional we used in
Section~\ref{sec:maximizing-other-functions} and add constraints $x +
y = 1$.  The constraint must be described in matrix form
$\mat{A}\,\vec{\theta} + \vec{b} = 0$ where $\vec{\theta}$ is the
parameter vector and $\mat{A}$ and $vec{b}$ describe the constraints.
In this case we can write
\begin{equation}
  \label{eq:equality-constraints}
  \begin{pmatrix}
    1 & 1
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    x \\ y
  \end{pmatrix}
  +
  \begin{pmatrix}
    -1
  \end{pmatrix}
  = 0,
\end{equation}
i.e. $\mat{A} = (1 1)$ and $\vec{b} = -1$.  These values must be
supplied to the optimizer argument \verb|constraints|.  This is a list
with components names \verb|eqA| and \verb|eqB| for $\mat{A}$ and
$\vec{b}$ accordingly.
We choose not
to demonstrate
this with a likelihood example as no corrections to the Hessian matrix
is done and hence the standard errors are incorrect.  But for ordinary
optimization, one can write:
<<>>=
f <- function(theta) {
   x <- theta[1]
   y <- theta[2]
   exp(-x^2 - y^2)
                           # optimum at (0, 0)
}
A <- matrix(c(1, 1), ncol=2)
B <- -1
m <- maxNR(f, start=c(1,1),
           constraints=list(eqA=A, eqB=B))
summary(m)
@
The problem is solved using sequential unconstrained maximization
technique (SUMT).  The idea of it to add a small penalty for the
constraint violation, and to slowly increase the penalty until
violations are prohibitively expensive.  As the example indicates, the
solution is extremely close to the constraint line.

Finally the inequality constraints.  The usage of inequality
constraints is fairly similar to the usage of equality constraints.
We have to code the inequalities as $\mat{A}\,\vec{\theta} + \vec{b} >
0$ where the matrices $\mat{A}$ and $vec{b}$ are defined as above.
Let us optimize the function over the region $x + y > 1$.  In matrix
form this will be
\begin{equation}
  \label{eq:inequality-constraints-1}
  \begin{pmatrix}
    1 & 1
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    x \\ y
  \end{pmatrix}
  +
  \begin{pmatrix}
    -1
  \end{pmatrix}
  > 0.
\end{equation}
Supplying the constraints is otherwise similar to the equality
constraints, just the constraints-list components must be called
\verb|ineqA| and \verb|ineqB|.  As \verb|maxNR| does not support
inequality constraints, we use \verb|maxBFGS| instead.
The corresponding code is
<<>>=
A <- matrix(c(1, 1), ncol=2)
B <- -1
m <- maxBFGS(f, start=c(1,1),
             constraints=list(ineqA=A, ineqB=B))
summary(m)
@
Not surprisingly, the result is exactly the same as in case of
equality constraints, in this case the optimum is found at the
boundary, the same line what we specified using equality constraints.

One can supply more than one set of constraints, in that case these all
must be satisfied at the same time.  For instance, let's add another
condition, $x - y > 1$.  Both constraints together in the matrix form
will be
\begin{equation}
  \label{eq:inequality-constraints-2}
  \begin{pmatrix}
    1 & 1\\
    1 & -1
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    x \\ y
  \end{pmatrix}
  +
  \begin{pmatrix}
    -1 \\ -1
  \end{pmatrix}
  > 0.
\end{equation}
We also have to ensure the initial value satisfies the constraint, so
we choose $\vec{\theta}_{0} = (2, 0)$.  The code will be accordingly:
<<>>=
A <- matrix(c(1, 1, 1, -1), ncol=2)
B <- c(-1, -1)
m <- maxBFGS(f, start=c(2, 0),
             constraints=list(ineqA=A, ineqB=B))
summary(m)
@
The solution is $(1, 0)$ the closest point to the origin where both
constraints are satisfied.


\section{Tips and tricks}
\label{sec:suggestions}

analytic gradients

speed of convergence


\subsection{TODO}

example with Hessian

% \bibliographystyle{apecon}
% \bibliography{maxlik}

\end{document}
