\documentclass[smallextended,natbib]{svjour3}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{bm,xspace}
\smartqed

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\loglik}{{l}}
\newcommand{\maxlik}{\pkg{maxLik}\xspace}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\R}{\textsl{R}\xspace}
\setlength{\emergencystretch}{3em}

\journalname{Computational Statistics}

\begin{document}
<<echo=FALSE, results=hide>>=
options(width=70)
set.seed( 123 )
@
@ 
\title{maxLik: A Package for Maximum Likelihood Estimation in \textsf{R}}

\titlerunning{maxLik: Maximum Likelihood Estimation}

\author{Arne Henningsen \and Ott Toomet}

\institute{
   Arne Henningsen \at
   Institute of Food and Resource Economics, University of Copenhagen\\
   Rolighedsvej 25, 1958 Frederiksberg C, Denmark\\
   Tel.: +45-353-32274\\
   \email{arne@foi.dk}
\and
   Ott Toomet \at
   Department of Economics, University of Tartu (Estonia)\\
   Department of Economics, Aarhus School of Business,
      University of Aarhus (Denmark)
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
  This paper describes the package \pkg{maxLik} for \textsl{R}
  statistical environment.  The package is essentially a unified
  wrapper interface to various optimization routines with interface,
  offering easy access to likelihood-specific features like standard
  errors, or information equality (BHHH method).  More advanced
  features of the opimization algorithms, such as forcing the value of
  a particular parameter to be fixed, is also supported.
  
  \keywords{Maximum Likelihood \and Optimisation}
\end{abstract}


\section{Introduction}

The Maximum Likelihood (ML) method is one of the most important
techniques in statistics and econometrics.
Most statistical and econometric software packages include
ready-made routines for maximum likelihood estimations
of many standard models such as logit, probit, sample-selection,
count-data, or survival models.
However, if practitioners and researchers want to estimate
non-standard models or develop new models,
they have to implement the routines
for the maximum likelihood estimations themselves.
In this case, the \pkg{maxLik} package~\citep{r-maxlik-0.5}
for the statistical software environment \textsf{R} \citep{r-project09}
might be very helpful.
Furthermore, developers that implement routines for maximum likelihood
estimations of specific models
can reduce their effort
by using the \pkg{maxLik} package for the actual parameter estimation.
For instance, the packages
\pkg{LambertW} \citep{r-lambertw-0.1.6},
\pkg{mlogit} \citep{r-mlogit-0.1},
\pkg{sampleSelection} \citep{toomet08}, and
\pkg{truncreg} \citep{r-truncreg-0.1}
use the \pkg{maxLik} package
for their maximum likelihood estimations.

The \pkg{maxLik} package is available from
CRAN (\url{http://cran.r-project.org/package=maxLik}),
R-Forge (\url{http://r-forge.r-project.org/projects/maxlik/}), and its
homepage (\url{http://www.maxLik.org/}).

The paper proceeds as follows: in the next section we explain the
implementation of the package.  Section~\ref{sec:usage} decribes the
basic and more advanced usage of the pacakge, and
Section~\ref{sec:summary} concludes.

% .... exists in GAUSS

\section{Implementation}
\label{sec:implementation}

\maxlik is designed to provide a single, unified interface to
different optimization routines, and to treat the results in a way,
suitable for maximum likelihood (ML) estimation.  The package
implements a flexible multi-purpose Newton-Raphson type optimization
routine \code{maxNR}.  This is used as a basis of \code{maxBHHH}, a
Bernt-Hall-Hall-Hausman type optimizer, popular for ML problems.  In
addition, various methods from the \code{optim} function (from package
\pkg{stats}) is wrapped in a unified way (functions \code{maxBFGS},
\code{maxNM}, and \code{maxSANN}).

The package is designed in two layers.  The first (innermost) is the
optimization (maximization) layer: all the maximization routines are
designed to have a unified and intuitive interface which allows the
user to switch easily between them.  All the main parameters have
identical names and similar order (however, method-specific parameters
may vary).  These functions can be used for different types of
optimization tasks, both related and not related to the likelihood.
They return an S3 object of class \code{maxim} including both
estimated parameters and various diagnostics information.

The second layer is the likelihood maximization layer.  It's main
purpose is to treat the estimates in ML-specific way (for instance,
computing the variance-covariance matrix based on the estimated
Hessian).  The \code{maxBHHH} function belong to this layer as well,
being essentially a call for \code{maxNR} using information equality
as the approximate Hessian matrix.  A new class \code{maxLik} is added
to the maximization object for automatic selection of the ML-related
methods.

The maximization layer supports linear equality and inequality
constraints.  The equality constraints are estimated using sequential
unconstrained maximization technique (SUMT), implemented in the
package.  The inequality constraints are delegated to
\code{constrOptim} in the package \pkg{stats}.  The likelihood layer
is aware of the constraints and is able to select a suitable optimization method,
however, no attempt is made to correct the resulting
variance-covariance matrix (just a warning is printed).

The \pkg{maxLik} package is implemented using S3 classes.
Corresponding methods can handle the likelihood-specific properties
of the estimate including the fact
that the inverse of the negative Hessian is the variance-covariance matrix
of the estimated parameters.
The most important methods for objects of class \code{"maxLik"} are:
\code{summary} for returning (and printing) summary results,
\code{coef} for extracting the estimated parameters,
\code{vcov} for calculating the variance covariance matrix
of the estimated parameters,
\code{logLik} for extracting the log-likelihood value, and
\code{AIC} for calculating the Akaike information criterion.


\section{Using the \pkg{maxLik} package}
\label{sec:usage}

\subsection{Basic usage}
\label{sec:basic_usage}

As the other packages in \R, the
\pkg{maxLik} package must be installed and loaded before it can be used.
The following command loads the \pkg{maxLik} package:
<<>>=
library( maxLik )
@
The most important user interface of the \pkg{maxLik} package
is a function with the (same) name \code{maxLik}.  It is mostly a wrapper
for different optimization routines with a few additional features,
useful for ML estimations.
This function has two mandatory arguments
<<eval=FALSE>>=
maxLik( logLik, start )
@
The first argument (\code{logLik}) must be a function
that calculates the log-likelihood value as a function of the
parameter (usually parameter vector).
The second argument (\code{start}) must be a vector of starting values.

We demonstrate the usage of the \pkg{maxLik} package by a simple example:
we fit a normal distribution by maximum likelihood.
First, we generate a vector ($x$) of 100 draws
from a normal distribution with a mean of $\mu = 1$
and a standard deviation of~$\sigma = 2$:
<<>>=
x <- rnorm( 100, mean = 1, sd = 2 )
@
%
The probability density function of a standard normal distribution
for a value $x_i$ is
\begin{equation}
P(x_i) =
\frac{1}{\sigma \sqrt{2 \pi}}
\exp \left( - \frac{( x_i - \mu )^2}{2 \sigma^2} \right).
\end{equation}
Hence, the likelihood function for $\mu$, $\sigma$,
and the values in vector $x = ( x_1, \ldots, x_N )$ is
\begin{equation}
L( x; \mu, \sigma ) = \prod_{i=1}^N
\frac{1}{\sigma \sqrt{2 \pi}}
\exp \left( - \frac{( x_i - \mu )^2}{2 \sigma^2} \right)
\end{equation}
and its logarithm (i.e.\ the log-likelihood function) is
\begin{equation}
\log( L( x, \mu, \sigma ) ) =
- \frac{1}{2} N \log ( 2 \pi ) - N \log ( \sigma )
- \frac{1}{2} \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^2}.
\end{equation}

Given the log-likelihood function above,
we create an \textsf{R} function
that calculates the log-likelihood value.
Its first argument must be the vector of the parameters to be estimated
and it must return the log-likelihood value.%
\footnote{
Alternatively, it could return a numeric vector
where each element is the log-likelihood value
corresponding to an (independent) individual observation (see below).
}
The easiest way to implement this log-likelihood function
is to use the capabilities of the function
\code{dnorm}: 
<<>>=
logLikFun <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
@
For the actual estimation we
set the first argument (\code{logLik}) equal to the log-likelihood function
that we have defined above (\code{logLikFun})
and we use the parameters of a standard normal distribution
($\mu = 0$, $\sigma = 1$) as starting values (argument \code{start}).
Assigning names to the vector of starting values is not required
but has the advantage
that the returned estimates have also names,
which improves the readability of the results.
<<>>=
mle <- maxLik( logLik = logLikFun, start = c( mu = 0,  sigma = 1 ) )
summary( mle )
@
As expected, the estimated parameters are equal to the mean
and the standard deviation (without correction for degrees of freedom)
of the values in vector~$x$\footnote{%
The function \code{all.equal} considers two elements as equal
if either the mean absolute difference or the mean relative difference
is smaller than the tolerance
(defaults to \code{.Machine\$double.eps\^{ }0.5},
usually around \Sexpr{round(.Machine[["double.eps"]]^0.5,9)}).
}.
<<>>=
all.equal( coef( mle ), c( mean( x ),
   sqrt( sum( ( x - mean( x ) )^2 ) / 100 ) ),
   check.attributes = FALSE )
@

If no analytical gradients are provided by the user,
numerical gradients and numerical Hessians are calculated
by the functions \code{numericGradient} and \code{numericNHessian},
which are also included in the \pkg{maxLik} package.
While the maximisation of the likelihood function of this simple model
works well with numerical gradients and Hessians, 
this may not be the case for more complicated models.  Numerical
derivatives may be costly to compute, even more, they may turn out to
be noisy and unreliable, and in this way
result the estimation either to slow down considerably or even fail to
converge.  In these cases the user is recommended either to provide
analytic derivatives, or to switch to a more robust estimation method,
such as Nelder-Mead, which is not based on gradient.

The gradient of the log-likelihood function of the log-likelihood
function is
\begin{align}
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \mu } & =
   \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^2}\\
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \sigma } & =
   - \frac{ N }{ \sigma }
   + \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^3}.
\end{align}
This can be calculated in \textsf{R} by the following function:
<<>>=
logLikGrad <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikGradValues <- numeric( 2 )
   logLikGradValues[ 1 ] <- sum( ( x - mu ) / sigma^2 )
   logLikGradValues[ 2 ] <- - N / sigma + sum( ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
@
Now we call the \code{maxLik} function and use argument \code{grad}
to specify the function that calculates the gradients:
<<>>=
mleGrad <- maxLik( logLik = logLikFun, grad = logLikGrad,
   start = c( mu = 0, sigma = 1 ) )
summary( mleGrad )
all.equal( logLik( mleGrad ), logLik( mle ) )
all.equal( coef( mleGrad ), coef( mle ) )
all.equal( vcov( mleGrad ), vcov( mle ) )
@
Providing analytical gradients has no (relevant)%
effect on the estimates
but their covariance matrix and standard errors are slightly different.

The analytic Hessian of the log-likelihood function can be provided by
the argument \code{hess}.
If the user provides a function to calculate the gradients
but does not use argument \code{hess},
the Hessians are calculated numerically by the function
\code{numericHessian}.\footnote{Only Newton-Raphson 
  algorithm uses Hessian directly for obtaining the estimates.  The
  other algorithms ignore it for estimation but use for computing the
  final variance-covariance matrix.}
The elements of the Hessian matrix of the log-likelihood function
of the normal distribution are
\begin{align}
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \mu )^2 } & =
   - \frac{ N }{\sigma^2}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ \partial \mu \; \partial \sigma } & =
   - 2 \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^3}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \sigma )^2 } & =
   \frac{ N }{ \sigma^2 }
   - 3 \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^4}.
\end{align}
They can be calculated in \textsf{R} using the following function:
<<>>=
logLikHess <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikHessValues <- matrix( 0, nrow = 2, ncol = 2 )
   logLikHessValues[ 1, 1 ] <- - N / sigma^2
   logLikHessValues[ 1, 2 ] <-  - 2 * sum( ( x - mu ) / sigma^3 )
   logLikHessValues[ 2, 1 ] <- logLikHessValues[ 1, 2 ]
   logLikHessValues[ 2, 2 ] <- N / sigma^2 - 3 * sum( ( x - mu )^2 / sigma^4 )
   return( logLikHessValues )
}
@
Now we call the \code{maxLik} function with argument \code{hess}
set to this function:
<<>>=
mleHess <- maxLik( logLik = logLikFun, grad = logLikGrad,
   hess = logLikHess, start = c( mu = 0, sigma = 1 ) )
summary( mleHess )
all.equal( list( logLik( mleHess ), coef( mleHess ), vcov( mleHess ) ),
   list( logLik( mleGrad ), coef( mleGrad ), vcov( mleGrad ) ) )
@
Providing an analytical Hessian has no (relevant) effect
on the outcome of the ML estimation in our simple example.  However,
exactly as in the case of gradient, numerical Hessian may turn out to
be slow and unreliable.


\subsection{Optimisation Methods}

The \code{maxLik} function allows to select between five actual underlying
optimization algorithm by argument \code{method}.
It defaults to
\code{"NR"} for the Newton-Raphson algorithm.  The other options are
\code{"BHHH"} for Berndt-Hall-Hall-Hausman \citep{berndt74},
\code{"BFGS"} for Broyden-Fletcher-Goldfarb-Shanno
\citep{broyden70,fletcher70,goldfarb70,shanno70},
\code{"NM"} for Nelder-Mead \citep{nelder65}, and
\code{"SANN"} for simulated-annealing \citep{belisle92}.
The Newton-Raphson algorithm uses (numerical or analytical)
gradients and Hessians;
the BHHH and BFGS algorithms use only (numerical or analytical) gradients;
the NM and SANN algorithms use neither gradients nor Hessians
but only function values.
The gradients and Hessians provided by the user through
the arguments \code{grad} and \code{hess} are always accepted in order
to make the changing the algorithms easier, but they are ignored
if the selected method does not require this information,
e.g.\ argument \code{hess} for the Berndt-Hall-Hall-Hausman method.
In this way the user can easily change the optimisation method
without changing the arguments.

In general, it is advisable to use all the available information,
e.g. to use \code{"NR"} method if analytic Hessian is available, one
of the gradient-based methods (either \code{"BHHH"} or \code{"BFGS"})
if analytic gradient is available, and to resort to the value-only
methods only if gradients are not provided.  

\subsubsection{Berndt-Hall-Hall-Hausman (BHHH)}
The BHHH method approximates the Hessian by the negative of the
expected outer product of the gradient (score) vector.  It
the log-likelihood function (argument \code{logLik})
to return a vector of log-likelihood values,
where each element corresponds to an individual
observation.\footnote{In case of a supplied gradient function returns
  gradients by individual observations, the likelihood function may
  return a single log-likelihood value.}.
This
vector is used for calculating the gradient matrix which corresponds
to the gradients for every individual observation, later used for
outer product approximation.

We modify our example above accordingly: instead of returning a single
summary value of log-likelihood, we return the values by individual
observations by simply removing the \code{sum} operator:
<<>>=
logLikFunInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   dnorm(x, mean=mu, sd=sigma, log=TRUE)
}
mleBHHH <- maxLik( logLik = logLikFunInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
summary( mleBHHH )
all.equal( logLik( mleBHHH ), logLik( mle ) )
all.equal( coef( mleBHHH ), coef( mle ) )
all.equal( vcov( mleBHHH ), vcov( mle ) )
@
While the estimated parameters and the corresponding log-likelihood value
are virtually identical to the previous estimates,
the covariance matrix of the estimated parameters is slightly
different.  This is because the outer product approximation may differ
from the derivative-based hessian on finite samples
\citep{calzolari+fiorentini1993}. 

If the user chooses to provide analytical gradients,
the function that calculates the gradients (argument \code{grad})
must return a numeric matrix,
where each column corresponds to an estimated parameter
and each row corresponds to an individual observation.  Note that in
this case, the log-likelihood function itself does not have to return
a vector of log-likelihood values by observations, as the gradient by
observation is supplied by the \code{grad} function.  Note that the
following example uses a single summary log-likelihood from the
Newton-Raphson example.
<<>>=
logLikGradInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   logLikGradValues <- cbind( ( x - mu ) / sigma^2,
      - 1 / sigma + ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
mleGradBHHH <- maxLik( logLik = logLikFun, grad = logLikGradInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
all.equal( list( logLik( mleBHHH ), coef( mleBHHH ), vcov( mleBHHH ) ),
   list( logLik( mleGradBHHH ), coef( mleGradBHHH ), vcov( mleGradBHHH ) ) )
@
Estimates based on numerical gradients and analytical gradients are
virtually identical.


\subsubsection{Nelder-Mead (NM) and other methods}
The other maximization methods: Nelder-Mead,
Broyden-Fletcher-Goldfarb-Shannon, and Simulated Annealing, are
implemented by a call to the the \code{optim} function in package
\pkg{stats}.  To give the reader a taste, we give an example using
gradientless Nelder-Mead method:
<<>>=
mleNM <- maxLik( logLik = logLikFun,
   start = c( mu = 0, sigma = 1 ), method = "NM" )
summary( mleNM )
logLik( mleNM ) - logLik( mleGrad )
all.equal( coef( mleNM ), coef( mleGrad ) )
all.equal( vcov( mleNM ), vcov( mleGrad ) )
@
The estimates and the covariance matrix
obtained from the Nelder-Mead algorithm slightly differ
from previous results using other algorithms
and the fit (log-likelihood value) of the model
is slightly worse (smaller) than for the previous models.

Note that although \code{summary} reports the number of iterations for
all the methods, the meaning of ``iteration'' may be completely
different for different optimization techniques.


\subsection{More advanced usage}
\label{sec:advanced_usage}

Optimization methods may support various additional features, such as
the temperature-related parameters for \code{maxSANN}.  Those can be
consulted in the documentation of the corresponding function.  Below, we
demonstrate how it is possible to fix certain parameters, we would
like to handle as constants in the optimization process.  This feature
is implemented in \code{maxNR} (and hence supported by \code{maxBHHH}
as well), which is a part of \pkg{maxLik}.

Let us return to our original task of estimating the parameters of a
normal sample.  However, assume we know that the true value of $\sigma
= 2$.  Instead of writing a new likelihood function, we may use the
existing one while specifying that $\sigma$ is fixed at 2.  This is
done via argument \code{activePar} of \code{maxNR}.  It is a logical
vector of length of the parameter vector which specifies which
components are allowed to change (are ``active'').  So, as $\sigma$
was the second parameter, we should call:
<<code:activePar>>=
summary(maxLik(logLikFun, start=c(mu=0, sigma=2), 
               activePar=c(TRUE, FALSE)))
@ 
As we can see, the $\sigma$ is indeed exactly 2 while it's standard
error and $t-$value do not carry any useful information.  Note also
that the estimate of $\mu$ is unchanged (indeed, MLE estimate of it is
still the sample average) while the estimated standard error is
different.  Obviously, log-likelihood is lower in the constrained
space although the reader may verify that allowing $\sigma$ to be free
is an insignificant improvement according to the likelihood ratio
test.

Next, we demonstrate, how it is possible to fix a parameter
automatically during the computations.  This may be useful when
estimating a large
number of similar models where parameters occasionally converge toward
the boundary of the parameter space.  Most popular optimization
algorithms do not work well at such circumstances.  We demonstrate
this feature
by estimating a mixture model of two normal distributions on a sample,
drawn from a single normal distribution.  Note that the following
example is highly dependent on the initialization of random number
generator and initial values for the estimation.  This happens often
with mixture models.

First, we generate a mixture of normals:
<<code:generate_mixture>>=
xMix <- c(rnorm(500), rnorm(500, mean=1))
@ 
Hence \code{xMix} is a 50\%-50\% mixture of two normal distributions: the
first one has mean equal to 0 and the second has mean 1 (standard
deviations are fixed to 1 for both distributions).  The log-likelihood
of a mixture is simply
\begin{equation}
  \loglik = \sum_{i=1}^{N} 
  \log( \varrho \phi (x_{i} - \mu_{1}) +
  (1 - \varrho) \phi(x_{i} - \mu_{2})),
\end{equation}
where $\varrho$ is the percentage of the first component in the
mixture and $\phi(\cdot)$ is the standard normal density function.  We
implement this in \textsl{R}:
<<code:logLikMix>>=
logLikMix <- function(param) {
   rho <- param[1]
   if(rho < 0 || rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
}
@ 
Note that the function includes checking for feasible parameter
values.  If $\varrho \not\in [0,1]$, it returns \code{NA}.  This
signals to the \code{maxNR} that the attempted parameter value was out
of range, and forces it to find a new one (closer to the previous
value).  This is a way of implementing box constraints in the
log-likelihood function.  The results look following:
<<code:mix1>>=
summary(m1 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
The estimates replicate the true parameters within the confidence
intervals, however compared to the examples in Section~\ref{sec:basic_usage}, the
standard errors are rather large (note that the sample here includes
1000 observations instead of mere 100 above).  This is a common
outcome while estimating mixture models.

Let us now
replace the mixture by a pure normal sample
<<code:generate_pure_normal>>=
xMix <- rnorm(1000)
@ 
and estimate it using the same log-likelihood implementation:
<<code:mix2>>=
summary(m2 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
vcov(m2)
@ 
Although the estimates seem to be close to the correct point in the
parameter space: mixture of 100\% normal with mean 0 and
0\% with mean 1, the Hessian matrix is singular and hence standard
errors are missing.  This is because both components of the mixture
converge to the same value and hence $\varrho$ is not identified.  
Hence we have no way establishing whether
the common mean of the sample is, in fact, significally different from
0.
If the estimation is done by hand, it would be easy
to fix $\varrho$ as in the example above.  However, this may
not be a suitable approach if we automatically run a large
number of similar computations on different samples.  In that case the
user may want to consider signalling the \code{maxNR} routine that the
parameters should be treated as fixed.
We may rewrite the log-likelihood function as follows:
<<code:logLikMix2>>=
freePar <- rep(TRUE, 3)
logLikMix1 <- function(param) {
   rho <- param[1]
   if(rho < 0 | rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   constPar <- NULL
   if(freePar[1] & (abs(mu1 - mu2) < 1e-2)) {
      rho <- 1
      constPar <- c(1, 3)
      newVal <- c(1, 0)
      fp <- freePar
      fp[constPar] <- FALSE
      assign("freePar", fp, inherits=TRUE)
   }
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
   if(!is.null(constPar)) {
      attr(ll, "constPar") <- constPar
      attr(ll, "newVal") <- list(index=constPar, val=newVal)
   }
   ll
}
@ 
If changing the fixed parameters run-time, we have to keep track of
the process.  This is why we introduce \code{freePar} \emph{outside}
the function itself -- it must retain it's value over successive calls
to the function.  Next novelty is related to checking the proximity to
the parameter space boundary: \code{if(freePar[1] \& (rho < 0.01))}.
Hence, if we haven't fixed the first parameter ($\varrho$) yet, and if
it is too close to 0, we set a new value 0 to it, and mark the first
parameter in the parameter vector to be fixed.  Note that because
$\mu_{1}$ is undefined as $\varrho = 0$, we also have to fix that
parameter.  Accordingly, we write \code{constPar <- c(1, 2)} for
fixing both of these parameters and \code{newVal <- c(1, 0)} for
assigning new values.  Later we also set according attributes to
log-likelihood.  Attribute \code{newVal} must contain two components
-- indices and values.  This is because in general we may want to fix
a parameter which value is not changed, or change a value of a
parameter which is not fixed.

Now the estimation results look like:
<<code:mix3>>=
summary(m <- maxLik(logLikMix1, start=c(rho=0.5, mu1=0, mu2=0)))
@ 
with parameters \code{rho} and \code{mu2} fixed.  Now the Hessian at the
maximum, based on the free parameters (i.e. \code{mu1}) is negative
definite. 


\section{Summary and Outlook}
\label{sec:summary}

The \pkg{maxLik} package provides a convenient interface
for maximum likelyhood estimations
--- both for end users and package developers.
Alternatively, the \code{mle} function
\citep[package \pkg{stats4},][]{r-project09}
and general-purpose solvers can be used for maximum likelyhood estimations
in \textsf{R}.
However, the \pkg{maxLik} package has three important features
that are not available in at least some of the alternatives:
First, the covariance matrix of the estimates can be calculated automatically.
Second, the user can easily switch between different optimisation algorithms.
Third, the package provides the Berndt-Hall-Hall-Hausman (BHHH) algorithm,
which is very popular for maximum likelihood estimations.

In the future,
we plan to add support for further optimisation algorithms,
e.g.\ function \code{nlm},
the "L-BFGS-B" algorithm in function \code{optim}
   that allows for box constraints,
function \code{nlminb}
   that uses PORT routines \citep{gay90}
   and also allows for box constraints,
function \code{constrOptim}
   that allows for linear inequality constraints, or
function \code{ucminf} of the \pkg{ucminf} package \citep{r-ucminf-1.0}.

We hope that these improvements make the \pkg{maxLik} package
even more attractive for users and package writers.

%\begin{acknowledgements}
%\end{acknowledgements}

\bibliographystyle{spbasic}
% \bibliography{agrarpol}
\bibliography{references}

TODO:

* Describe that numeric gradients are computed by us, not by optim

* Merge maxBFGS, maxNM, maxSANN (if have time)

* add activePar for optim-based methods

* fixed parameters ``are treated as known''

* metion that constraints API is experimental

\end{document}



