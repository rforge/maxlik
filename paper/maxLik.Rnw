\documentclass[smallextended,natbib]{svjour3}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{bm,xspace}
\smartqed

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\loglik}{{l}}
\newcommand{\maxLik}{\pkg{maxLik}\xspace}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\R}{\proglang{R}\xspace}
\setlength{\emergencystretch}{3em}

\journalname{Computational Statistics}

\begin{document}
<<echo=FALSE, results=hide>>=
options(width=70)
@
\title{maxLik: A Package for Maximum Likelihood Estimation in \R}

\titlerunning{maxLik: Maximum Likelihood Estimation}

\author{Arne Henningsen \and Ott Toomet}

\institute{
   Arne Henningsen \at
   Institute of Food and Resource Economics, University of Copenhagen\\
   Rolighedsvej 25, 1958 Frederiksberg C, Denmark\\
   Tel.: +45-353-32274\\
   \email{arne@foi.dk}
\and
   Ott Toomet \at
   Department of Economics, Aarhus School of Business,
      University of Aarhus\\
   Hermodsvej 22, 8230 Åbyhøj, Denmark\\
   Department of Economics, University of Tartu\\
   Narva 4, Tartu 51009, Estonia\\
   \email{ottt@asb.dk}
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
  This paper describes the package \pkg{maxLik} for the
  statistical environment \R.  The package is essentially a unified
  wrapper interface to various optimization routines,
  offering easy access to likelihood-specific features like standard
  errors or information equality (BHHH method).  More advanced
  features of the optimization algorithms, such as forcing the value of
  a particular parameter to be fixed, are also supported.
  
  \keywords{Maximum Likelihood \and Optimisation}
% classifications
% 1.01080: Econometrics
% 1.02570: Maximum Likelihood
% 1.03030: Nonlinear Optimization
% 1.03130: Numerical Optimization
% 1.04290: Software: R
% 1.04440: S Programming Language
% 1.04490: Statistical Computing
% 1.04580: Statistical Software
\end{abstract}


\section{Introduction}

The Maximum Likelihood (ML) method is one of the most important
techniques in statistics and econometrics.
Most statistical and econometric software packages include
ready-made routines for maximum likelihood estimations
of many standard models such as logit, probit, sample-selection,
count-data, or survival models.
However, if practitioners and researchers want to estimate
non-standard models or develop new estimators,
they have to implement the routines
for the maximum likelihood estimations themselves.  Several popular
statistical packages include frameworks for simplifying the
estimation, allowing the user to easily choose between a number of
optimization algorithms, different ways of calculating
variance-covariance matrices, and easy reporting of the results.  The examples
include the \code{ml} command in \proglang{stata} and
the \code{maxlik} library for \proglang{GAUSS}.

The free software environment for statistical computing and graphics
\R \citep{r-project09} has included built-in optimization algorithms
since its early days.  The first general-purpose ML framework,
function \code{mle} in the built-in package \pkg{stats4}, was added in 2003, and
an extension, \code{mle2} in package \pkg{bbmle} \citep{r-bbmle-0.9.3}, in 2007.
However, both
of these packages are based on a general-purpose optimizer
\code{optim} which does not include an option to use the Newton-Raphson
algorithm.  In particular, its variant,
the Berndt-Hall-Hall-Hausman algorithm
\citep{berndt74}, is very popular for ML problems.  The \R package
\pkg{maxLik} \citep{r-maxlik-0.7} is intended to fill this gap.  The
package can be used both by end-users, developing their own
statistical methods, and by package developers, implementing ML
estimators for specific models.  For instance, the packages
\pkg{LambertW} \citep{r-lambertw-0.1.6}, \pkg{mlogit}
\citep{r-mlogit-0.1}, \pkg{sampleSelection} \citep{toomet08}, and
\pkg{truncreg} \citep{r-truncreg-0.1} use the \pkg{maxLik} package for
their maximum likelihood estimations.

The \pkg{maxLik} package (currently version~0.7) is available from
CRAN (\url{http://cran.r-project.org/package=maxLik}),
R-Forge (\url{http://r-forge.r-project.org/projects/maxlik/}), and its
homepage (\url{http://www.maxLik.org/}).  
This paper focuses on the maximum likelihood related
usage of the package; the other features (including finite-difference
derivatives and optimization) are only briefly mentioned.

The paper proceeds as follows: in the next section we explain the
implementation of the package.  Section~\ref{sec:usage} describes the
usage of the package, including the basic and more advanced features, and
Section~\ref{sec:summary} concludes.

% .... exists in GAUSS

\section{Implementation}
\label{sec:implementation}

The \R package \maxLik is designed to provide a single, unified interface for
different optimization routines, and to treat the results in a way
suitable for maximum likelihood (ML) estimation.  The package
implements a flexible multi-purpose Newton-Raphson type optimization
routine in function \code{maxNRCompute}.
This internal function is not intended to be called by users
but function \code{maxNR} provides a convenient user-interface
and calls \code{maxNRCompute} for the actual optimization.
This Newton-Raphson type algorithm is also used as the basis
of function \code{maxBHHH},
which implements a Berndt-Hall-Hall-Hausman type algorithm \citep{berndt74}
that is popular for ML problems.
In addition, the Broyden-Fletcher-Goldfarb-Shanno algorithm
\citep{broyden70,fletcher70,goldfarb70,shanno70},
the Nelder-Mead routine \citep{nelder65}, and
a simulated annealing method \citep{belisle92}
are available in a unified way
in functions \code{maxBFGS}, \code{maxNM}, and \code{maxSANN}, respectively.
These three functions are predominantly wrapper functions
around the internal function \code{maxOptim},
which calls function \code{optim} (from the built-in package \pkg{stats}) 
for the actual optimisation.

The \maxLik package is designed in two layers.  The first (innermost) is the
optimization (maximization) layer: all the maximization routines are
designed to have a unified and intuitive interface which allows the
user to switch easily between them.  All the main arguments have
identical names and similar order; only method-specific parameters
may vary.  These functions can be used for different types of
optimization tasks, both related and not related to the likelihood.
They return an S3 object of class \code{maxim} including both
estimated parameters and various diagnostics information.

The second layer is the likelihood maximization layer.
The most important tool of this layer is the function \code{maxLik}.
Its main
purpose is to treat the inputs and maximization results in a ML-specific way (for instance,
computing the variance-covariance matrix based on the estimated
Hessian).  The \mbox{\code{maxBHHH}} function belongs to this layer as well,
being essentially a call for \code{maxNR} using information equality
as the way to approximate the Hessian matrix.  A new class \code{maxLik} is added
to the returned maximization object for automatic selection of the ML-related
methods.

The maximization layer supports linear equality and inequality
constraints.  The equality constraints are estimated using the sequential
unconstrained maximization technique (SUMT), which is also implemented in the
\maxLik package.  This is achieved by adding a (initially tiny) penalty term,
related to violation of the constraints, to the objective function.
Thereafter the problem is repeatedly solved while the penalty is
increased 
for every new repetition.  The inequality constraints are delegated to
\code{constrOptim} in the package \pkg{stats}.  The \code{maxLik}
function is aware of the constraints and is able to select a suitable
optimization method, however, no attempt is made to correct the
resulting variance-covariance matrix (just a warning is printed).  As
the constrained optimization should still be considered as
experimental, we refer the reader to the documentation of the package
for examples.

The \pkg{maxLik} package is implemented using S3 classes.
Corresponding methods can handle the likelihood-specific properties
of the estimate including the fact
that the inverse of the negative Hessian is the variance-covariance matrix
of the estimated parameters.
The most important methods for objects of class \code{"maxLik"} are:
\code{summary} for returning (and printing) summary results,
\code{coef} for extracting the estimated parameters,
\code{vcov} for calculating the variance covariance matrix
of the estimated parameters, \code{stdEr} for calculation standard
errors of the estimates (to our knowledge, this is the only ML-related
package which provides this convenience function),
\code{logLik} for extracting the log-likelihood value, and
\code{AIC} for calculating the Akaike information criterion.


\section{Using the \pkg{maxLik} package}
\label{sec:usage}

\subsection{Basic usage}
\label{sec:basic_usage}

As the other packages in \R, the
\pkg{maxLik} package must be installed and loaded before it can be used.
The following command loads the \pkg{maxLik} package:
<<>>=
library( "maxLik" )
@
The most important user interface of the \pkg{maxLik} package
is a function with the (same) name \code{maxLik}.
As explained above, \code{maxLik} is mostly a wrapper
for different optimization routines with a few additional features,
useful for ML estimations.
This function has two mandatory arguments
<<eval=FALSE>>=
maxLik( logLik, start )
@
The first argument (\code{logLik}) must be a function
that calculates the log-likelihood value as a function of the
parameter (usually parameter vector).
The second argument (\code{start}) must be a vector of starting values.

We demonstrate the usage of the \pkg{maxLik} package by a simple
example: we estimate the parameters of normal distribution based on a
random sample.
First, we generate a vector ($x$) of $N = 100$ draws
from a normal distribution with a mean of $\mu = 1$
and a standard deviation of~$\sigma = 2$:
<<>>=
set.seed( 123 )
x <- rnorm( 100, mean = 1, sd = 2 )
@
%
The logarithm of the probability density of the sample (i.e.\ the
log-likelihood function) is
\begin{equation}
\log( L( x; \mu, \sigma ) ) =
- \frac{1}{2} N \log ( 2 \pi ) - N \log ( \sigma )
- \frac{1}{2} \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^2}.
\end{equation}

Given the log-likelihood function above,
we create an \R function
that calculates the log-likelihood value.
Its first argument must be the vector of the parameters to be estimated
and it must return the log-likelihood value.%
\footnote{
Alternatively, it could return a numeric vector
where each element is the log-likelihood value
corresponding to an (independent) individual observation (see below).
}
The easiest way to implement this log-likelihood function
is to use the capabilities of the function
\code{dnorm}: 
<<>>=
logLikFun <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
@
For the actual estimation we
set the first argument (\code{logLik}) equal to the log-likelihood function
that we have defined above (\code{logLikFun})
and we use the parameters of a standard normal distribution
($\mu = 0$, $\sigma = 1$) as starting values (argument \code{start}).
Assigning names to the vector of starting values is not required
but has the advantage
that the returned estimates have also names,
which improves the readability of the results.\footnote{Alternatively,
if all the components of the parameter vector have standardized names,
one may prefer using \code{with}-construct for evaluating the
likelihood expression.  We are grateful to a referee for this suggestion.}
<<>>=
mle <- maxLik( logLik = logLikFun, start = c( mu = 0,  sigma = 1 ) )
summary( mle )
@
For convenience, the estimated parameters can be accessed by the
\code{coef} method and standard errors by the \code{stdEr} method:
<<code:coef-std>>=
coef(mle)
stdEr(mle)
@ 
As expected, the estimated parameters are equal to the mean
and the standard deviation (without correction for degrees of freedom)
of the values in vector~$x$.\footnote{%
The function \code{all.equal} considers two elements as equal
if either the mean absolute difference or the mean relative difference
is smaller than the tolerance
(defaults to \code{\mbox{.Machine\$double.eps\^{ }0.5}},
usually around $1.5 \cdot 10^{-8}$).
}
<<>>=
all.equal( coef( mle ), c( mean( x ),
   sqrt( sum( ( x - mean( x ) )^2 ) / 100 ) ),
   check.attributes = FALSE )
@

If no analytical gradient is provided by the user,
finite-difference gradient and Hessian are calculated
by the functions \code{numericGradient} and \code{numericNHessian},
which are also included in the \pkg{maxLik} package.
While the maximisation of the likelihood function of this simple model
works well with finite-difference gradients and Hessians, 
this may not be the case for more complex models.  Numerical
derivatives may be costly to compute, and, even more, they may turn out to
be noisy and unreliable. In this way
numerical derivatives might either slow down the estimation or even impede the
convergence. In these cases, the user is recommended to either provide
analytical derivatives or switch to a more robust estimation method,
such as Nelder-Mead or SANN, which is not based on gradients.

The gradients of the log-likelihood function with respect to the two
parameters are
\begin{align}
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \mu } & =
   \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^2}\\
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \sigma } & =
   - \frac{ N }{ \sigma }
   + \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^3}.
\end{align}
This can be calculated in \R by the following function:
<<>>=
logLikGrad <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikGradValues <- numeric( 2 )
   logLikGradValues[ 1 ] <- sum( ( x - mu ) / sigma^2 )
   logLikGradValues[ 2 ] <- - N / sigma + sum( ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
@
Now we call the \code{maxLik} function and use argument \code{grad}
to specify the function that calculates the gradients:
<<>>=
mleGrad <- maxLik( logLik = logLikFun, grad = logLikGrad,
   start = c( mu = 0, sigma = 1 ) )
all.equal( logLik( mleGrad ), logLik( mle ) )
all.equal( coef( mleGrad ), coef( mle ) )
all.equal( stdEr( mleGrad ), stdEr( mle ) )
@
Providing analytical gradients has no (relevant)
effect on the estimates
but their standard errors are slightly different.

The analytic Hessian of the log-likelihood function can be provided by
the argument \code{hess}.
If the user provides a function to calculate the gradients
but does not use argument \code{hess},
the Hessians are calculated by the function
\code{numericHessian} using finite-difference approach.
The elements of the Hessian matrix of the log-likelihood function
for the normal distribution are
\begin{align}
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \mu )^2 } & =
   - \frac{ N }{\sigma^2}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ \partial \mu \; \partial \sigma } & =
   - 2 \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^3}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \sigma )^2 } & =
   \frac{ N }{ \sigma^2 }
   - 3 \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^4}.
\end{align}
They can be calculated in \R using the following function:
<<>>=
logLikHess <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikHessValues <- matrix( 0, nrow = 2, ncol = 2 )
   logLikHessValues[ 1, 1 ] <- - N / sigma^2
   logLikHessValues[ 1, 2 ] <-  - 2 * sum( ( x - mu ) / sigma^3 )
   logLikHessValues[ 2, 1 ] <- logLikHessValues[ 1, 2 ]
   logLikHessValues[ 2, 2 ] <- N / sigma^2 - 3 * sum( ( x - mu )^2 / sigma^4 )
   return( logLikHessValues )
}
@
Now we call the \code{maxLik} function with argument \code{hess}
set to this function:
<<>>=
mleHess <- maxLik( logLik = logLikFun, grad = logLikGrad,
   hess = logLikHess, start = c( mu = 0, sigma = 1 ) )
all.equal( list( logLik( mleHess ), coef( mleHess ), vcov( mleHess ) ),
   list( logLik( mleGrad ), coef( mleGrad ), vcov( mleGrad ) ) )
@
Providing an analytical Hessian has no (relevant) effect
on the outcome of the ML estimation in our simple example.  However,
as in the case of finite-difference gradients, finite-difference Hessians may turn out to
be slow and unreliable.


\subsection{Optimisation Methods}

The \code{maxLik} function allows the user to select between five
optimization algorithms by argument \code{method}.
It defaults to
\code{"NR"} for the Newton-Raphson algorithm.  The other options are
\code{"BHHH"} for Berndt-Hall-Hall-Hausman \citep{berndt74},
\code{"BFGS"} for Broyden-Fletcher-Goldfarb-Shanno
\citep{broyden70,fletcher70,goldfarb70,shanno70},
\code{"NM"} for Nelder-Mead \citep{nelder65}, and
\code{"SANN"} for simulated annealing \citep{belisle92}.
The Newton-Raphson algorithm uses (finite-difference or analytical)
gradients and Hessians;
the BHHH and BFGS algorithms use only (finite-difference or analytical) gradients;
the NM and SANN algorithms use neither gradients nor Hessians
but only function values.
The gradients and Hessians provided by the user through
the arguments \code{grad} and \code{hess} are always accepted.
In this way the user can easily switch the optimisation method
without changing the arguments.
If arguments \code{grad} or \code{hess} are provided
but the selected method does not require this information---%
for instance for the
Nelder-Mead method---%
they are ignored during the optimization.
However, even if the optimization
method itself does not make use of the Hessian, this information is
used for computing the (final) variance-covariance matrix
of the parameters (except for the \code{"BHHH"} method.)

In general, it is advisable to use all the available information,
e.g.\ to use the \code{"NR"} method if both analytical gradients and Hessians are available, one
of the gradient-based methods (either \code{"BHHH"} or \code{"BFGS"})
if analytical gradients but no Hessians are available, and to resort to the value-only
methods only if gradients are not provided.  

\subsubsection{Berndt-Hall-Hall-Hausman (BHHH)}
The idea of the BHHH method is based on information equality, replacing
the Hessian by the negative of the
expectation of outer product of the gradient vector.  Note
that this is only valid while maximizing log-likelihood and
hence this method is usually not included in general-purpose optimizers.  In
practice, the expectation is approximated as the average over
outer product of gradients of individual (independent) observations.
Hence, in order to use BHHH method, one has to provide gradient
vectors by individual observations.  This can be achieved either by
providing a corresponding gradient function (see below) or by providing a
vector of individual observation-specific likelihood values by the
log-likelihood function itself (if no analytical gradients are provided).
In the latter case, finite-difference gradients are used to calculate the Hessian.

We modify our example above accordingly: instead of returning a single
summary value of log-likelihood, we return the values by individual
observations by simply removing the \code{sum} operator:
<<>>=
logLikFunInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   dnorm(x, mean=mu, sd=sigma, log=TRUE)
}
mleBHHH <- maxLik( logLik = logLikFunInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
summary( mleBHHH )
all.equal( logLik( mleBHHH ), logLik( mle ) )
all.equal( coef( mleBHHH ), coef( mle ) )
all.equal( vcov( mleBHHH ), vcov( mle ) )
@
While the estimated parameters and the corresponding log-likelihood value
are virtually identical to the previous estimates,
the covariance matrix of the estimated parameters is slightly
different.  This is because the outer product approximation may differ
from the derivative-based Hessian on finite samples
\citep{calzolari+fiorentini1993}. 

If the user chooses to provide analytical gradients,
the function that calculates the gradients (argument \code{grad})
must return a numeric matrix,
where each column represents the gradient with respect to the corresponding
element of the parameter vector
and each row corresponds to an individual observation.  Note that in
this case, the log-likelihood function itself does not have to return
a vector of log-likelihood values by observations, as the gradient by
observation is supplied by the \code{grad} function.  In the
following example, we define a function that calculates the gradient matrix
and we estimate the model by BHHH method
using this gradient matrix and the single summed log-likelihood from the
Newton-Raphson example.
<<>>=
logLikGradInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   logLikGradValues <- cbind( ( x - mu ) / sigma^2,
      - 1 / sigma + ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
mleGradBHHH <- maxLik( logLik = logLikFun, grad = logLikGradInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
all.equal( list( logLik( mleBHHH ), coef( mleBHHH ), vcov( mleBHHH ) ),
   list( logLik( mleGradBHHH ), coef( mleGradBHHH ), vcov( mleGradBHHH ) ) )
@
Estimates based on finite-difference gradients and analytical gradients are
virtually identical in our simple example.


\subsubsection{Nelder-Mead (NM) and other methods}

The other maximization methods: Nelder-Mead,
Broyden-Fletcher-Goldfarb-Shanno, and Simulated Annealing, are
implemented by a call to the the \code{optim} function in package
\pkg{stats}.  In order to retain compatibility with the BHHH method,
all these methods accept the log-likelihood function returning a
vector of individual likelihoods (these are summed internally).
A function to compute a gradient matrix with gradients of individual observations, is accepted as well.
If the user does
not provide gradients, the gradients are computed by finite-difference
approach.

We give an example using the
gradient-free Nelder-Mead method:
<<>>=
mleNM <- maxLik( logLik = logLikFun,
   start = c( mu = 0, sigma = 1 ), method = "NM" )
summary( mleNM )
logLik( mleNM ) - logLik( mleGrad )
all.equal( coef( mleNM ), coef( mleGrad ) )
all.equal( vcov( mleNM ), vcov( mleGrad ) )
@
The estimates and the covariance matrix
obtained from the Nelder-Mead algorithm slightly differ
from previous results using other algorithms
and the fit (log-likelihood value) of the model
is slightly worse (smaller) than for the previous models.

Note that although the \code{summary} method reports the number of iterations for
all the methods, the meaning of ``iteration'' may be completely
different for different optimization techniques.


\subsection{More advanced usage}
\label{sec:advanced_usage}

The \code{maxLik} function supports a variety of other arguments, most of
which are passed to the selected optimizer.  Among the most important
ones is \code{print.level} which controls the output of debugging
information (0 produces no debugging output, larger numbers produce
more output). 
Optimization methods may also support various additional features, such as
the temperature-related parameters for \code{maxSANN}.  Those will not
be discussed here, the interested reader is
referred to the documentation of the corresponding optimizer.

\subsubsection{Fixed parameter values}
\label{sec:fixed_parameter_values}

Below, we
demonstrate how it is possible to keep certain parameters fixed
as constants in the optimization process.  This feature
is implemented in all optimization methods supported by \code{maxLik}.

Let us return to our original task of estimating the parameters of a
normal sample.  However, assume we know that the true value of $\sigma
= 2$.  Instead of writing a new likelihood function, we may use the
existing one while specifying that $\sigma$ is kept fixed at 2.  This is
done via argument \code{fixed} of \code{maxLik}.%
\footnote{%
In earlier version of the \pkg{maxLik} package ($\leq 0.6$),
parameters could be fixed only in functions \code{maxNR} and \code{maxBHHH}
by using argument \code{activePar}.
This argument was a logical vector indicating,
which parameters should \emph{not} be fixed.
Since version~0.7 of the \pkg{maxLik} package,
it is recommended to fix parameters by argument \code{fixed},
because this has several advantages,
e.g.\ it is easier to fix just a few out of many parameters
and this works also with the BFGS, NM, and SANN method.
However, \code{activePar} of function \code{maxNR}
is kept for backward compatibility.
}
This argument allows for specifying the fixed parameters
in three different ways:
First, it can be a logical vector of length equal to that
of the parameter vector,
which specifies which components are not allowed to change,
i.e.\ stay fixed at their starting values.
Second, argument \code{fixed} can be an index vectors
that indicates the positions of the fixed parameters.
Third, this argument can be a vector of character strings
indicating the names of the fixed parameters,
where the parameter names are taken from argument \code{start}.
So, as $\sigma$
was the second parameter, we may call:
<<code:fixed>>=
summary(maxLik(logLikFun, start=c(mu=0, sigma=2), fixed=2))
@ 
As we can see, the $\sigma$ is indeed exactly 2.  Its standard error
is set to zero while the $t$-value is not defined.  Note also
that the estimate of $\mu$ is unchanged (indeed, ML estimate of it is
still the sample average) while the estimated standard error is
different.  Obviously, log-likelihood is lower in the constrained
space, although the reader may verify that allowing $\sigma$ to vary freely
is an insignificant improvement according to the likelihood ratio
test.

\subsubsection{Automatic transformation of parameters to fixed constants}

Next, we demonstrate, how it is possible to turn a parameter
automatically to a fixed constant during the computations when using the \code{maxNR} optimizer.
This may be useful when
estimating a large
number of similar models where parameters occasionally converge toward
the boundary of the parameter space or another problematic region.  
Most popular optimization
algorithms do not work well in such circumstances.  In some cases, a
solution is to replace the initial model with a simpler submodel, for
instance replacing a density mixture with a single density component.
Note that this problem cannot be easily
handled by constrained maximization
either, as the mixture parameters are not identified, if the weight
of one component goes to zero.
Below, we demonstrate
this problem
by estimating the parameters of a normal mixture on a sample,
drawn from a single normal distribution.  Note that this example
is highly dependent on the initialization of the random number
generator and the initial values for the estimation.  This happens often
with mixture models.

First, we demonstrate the outcome on a mixture of two distinct components.
We generate $N = 1000$ values from two different normal distributions:
<<code:generate_mixture>>=
xMix <- c(rnorm(500), rnorm(500, mean=1))
@ 
Variable \code{xMix} is a 50\%-50\% mixture of two normal distributions: the
first one has mean equal to 0 and the second has mean 1 (for
simplicity, we fix the standard
deviations to 1).  The log-likelihood
of a mixture is simply
\begin{equation}
  \loglik = \sum_{i=1}^{N} 
  \log( \varrho \phi (x_{i} - \mu_{1}) +
  (1 - \varrho) \phi(x_{i} - \mu_{2})),
\end{equation}
where $\varrho$ is the percentage of the first component in the
mixture and $\phi(\cdot)$ is the density function of the standard normal distribution.
We implement this in \R:
<<code:logLikMix>>=
logLikMix <- function(param) {
   rho <- param[1]
   if(rho < 0 || rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
}
@ 
Note that the function includes checking for feasible parameter
values.  If $\varrho \not\in [0,1]$, it returns \code{NA}.  This
signals to the optimizer that the attempted parameter value was out
of range, and forces it to find a new one (closer to the previous
value).  This is a way of implementing box constraints in the
log-likelihood function.  The results look like the following:
<<code:mix1>>=
summary(m1 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
The estimates replicate the true parameters within the confidence
intervals; however compared to the examples in Section~\ref{sec:basic_usage}, the
standard errors are rather large (note also that the sample here includes
1000 observations instead of mere 100 above).  This is a common
outcome while estimating mixture models.

Let us now
replace the mixture by a pure normal sample
<<code:generate_pure_normal>>=
xMix <- rnorm(1000)
@ 
and estimate it using the same log-likelihood implementation:
<<code:mix2>>=
summary(m2 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
Although the estimates seem to be close to the correct point in the
parameter space: mixture of 100\% normal with mean 0 and
0\% with mean 1, the Hessian matrix is singular and hence standard
errors are infinite.  This is because both components of the mixture
converge to the same value and hence $\varrho$ is not identified.  
Hence we have no way establishing whether
the common mean of the sample is, in fact, significantly different from
0.
If the estimation is done by hand, it would be easy
to treat $\varrho$ as fixed as in the example in Section~\ref{sec:basic_usage}.
However, this may not be a suitable approach if we want to run a large
number of similar computations on different samples.  In that case the
user may want to consider signalling the \code{maxNR} routine that the
parameters should be kept fixed.
We may rewrite the function for calculating the log-likelihood value as follows:
<<code:logLikMix2>>=
freePar <- rep(TRUE, 3)
logLikMix1 <- function(param) {
   rho <- param[1]
   if(rho < 0 | rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   constPar <- NULL
   if(freePar[1] & (abs(mu1 - mu2) < 1e-3)) {
      rho <- 1
      constPar <- c(1, 3)
      newVal <- c(1, 0)
      fp <- freePar
      fp[constPar] <- FALSE
      assign("freePar", fp, inherits=TRUE)
   }
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
   if(!is.null(constPar)) {
      attr(ll, "constPar") <- constPar
      attr(ll, "newVal") <- list(index=constPar, val=newVal)
   }
   ll
}
@ 
We have introduced three changes into the log-likelihood function.
\begin{itemize}
\item First, while changing the fixed parameters at run-time, we have to
  keep track of the process.  This is why we introduce \code{freePar}
  \emph{outside} the function itself, as it has to retain its value over
  successive calls to the function.
\item Next novelty is related to checking the proximity to the region
  of trouble: \code{if(freePar[1] \& (abs(mu1 - mu2) < 1e-3))}.
  Hence, if we haven't set the first parameter ($\varrho$) to a
  constant yet (this
  is what \code{freePar[1]} keeps track of), and the estimated means
  of the components are close to each other, we set $\varrho$ to 1.
  This means we assume the mixture contains only component 1.  Note
  that because $\mu_{2}$ is undefined as $\varrho = 1$, we also have
  to keep that parameter fixed.  We mark both of these parameters in the
  parameter vector to as fixed (\code{constPar <- c(1, 3)}), and
  provide the new values for them (\code{newVal <- c(1, 0)}).
\item As the last step, we inform \code{maxNR} algorithm of our
  decision by setting respective attributes to log-likelihood.  Two
  attributes are used: \code{constPar} informs the algorithm that
  corresponding parameters in the parameter vector must be treated as
  constants from now on; and \code{newVal} (which contains two components -- indices and
  values) informs which parameters have new values.  It
  is possible to set parameters to constants without changing the values by
  setting the \code{constPar} attribute only.
\end{itemize}

Now the estimation results look like:
<<code:mix3>>=
summary(m <- maxLik(logLikMix1, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
With parameters \code{rho} and \code{mu2} treated as constants, the resulting
one-component model has small standard errors.


\section{Summary and Outlook}
\label{sec:summary}

The \pkg{maxLik} package fills an existing gap in the \R statistical environment
and provides a convenient interface
for maximum likelihood estimations
--- both for end users and package developers.
Although \R has included general-purpose optimizers and more specific Maximum
Likelihood tools for a long time,
the \pkg{maxLik} package has three important features
that are not available in at least some of the alternatives:
First, the package provides the Berndt-Hall-Hall-Hausman (BHHH) 
algorithm,
a popular optimization method which is available only for
likelihood-type problems.  Second, the covariance matrix of the
estimates can be calculated automatically.
Third, the user can easily switch between different optimization
algorithms.

In the future,
we plan to add support for further optimisation algorithms,
e.g.\ function \code{nlm} of the built-in \pkg{stats} package
   that uses a Newton-type algorithm,
the "L-BFGS-B" algorithm in function \code{optim}
   that allows for box constraints,
function \code{nlminb} of the \pkg{stats} package
   that uses PORT routines \citep{gay90}
   and also allows for box constraints,
function \code{ucminf} of the \pkg{ucminf} package
   \citep{r-ucminf-1.0}
   that uses an improved quasi-Newton type algorithm,
and function \code{DEoptim} of the \pkg{DEoptim} package
   \citep{r-DEoptim-2.0-4}
   that performs evolutionary global optimization via the differential
   evolution algorithm.
% mention ROI here? ....
Another future extension includes a more comprehensive handling of constrained
maximum likelihood problems.

We hope that these improvements will make the \pkg{maxLik} package
even more attractive for users and package writers.

\begin{acknowledgements}
The authors are grateful to two anonymous referees
and to the participants of the sixth international workshop
on Directions in Statistical Computing (DSC)
in Copenhagen, Denmark, 13$-$14 July 2009
for giving them valuable comments and suggestions
regarding the \maxLik package and this paper.
Arne Henningsen is grateful to the German Research Foundation
(Deutsche Forschungsgemeinschaft, DFG)
for financially supporting this research.
Of course, all errors are the sole responsibility of the authors.
\end{acknowledgements}

\bibliographystyle{spbasic}
\bibliography{references}
% \bibliography{agrarpol}

% TODO:

% * Merge maxBFGS, maxNM, maxSANN (if have time)

% * add activePar for optim-based methods

\end{document}
