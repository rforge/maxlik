\documentclass[smallextended,natbib]{svjour3}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{bm,xspace}
\smartqed

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\loglik}{{l}}
\newcommand{\maxlik}{\pkg{maxLik}\xspace}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\R}{\textsl{R}\xspace}
\setlength{\emergencystretch}{3em}

\journalname{Computational Statistics}

\begin{document}
<<echo=FALSE, results=hide>>=
options(width=70)
@
\title{maxLik: A Package for Maximum Likelihood Estimation in \textsf{R}}

\titlerunning{maxLik: Maximum Likelihood Estimation}

\author{Arne Henningsen \and Ott Toomet}

\institute{
   Arne Henningsen \at
   Institute of Food and Resource Economics, University of Copenhagen\\
   Rolighedsvej 25, 1958 Frederiksberg C, Denmark\\
   Tel.: +45-353-32274\\
   \email{arne@foi.dk}
\and
   Ott Toomet \at
   Department of Economics, University of Tartu (Estonia)\\
   Department of Economics, Aarhus School of Business,
      University of Aarhus (Denmark)
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
  This paper describes the package \pkg{maxLik} for the
  statistical environment \R.  The package is essentially a unified
  wrapper interface to various optimization routines,
  offering easy access to likelihood-specific features like standard
  errors, or information equality (BHHH method).  More advanced
  features of the optimization algorithms, such as forcing the value of
  a particular parameter to be fixed, is also supported.
  
  \keywords{Maximum Likelihood \and Optimisation}
% classifications
% 1.01080: Econometrics
% 1.02570: Maximum Likelihood
% 1.03030: Nonlinear Optimization
% 1.03130: Numerical Optimization
% 1.04290: Software: R
% 1.04440: S Programming Language
% 1.04490: Statistical Computing
% 1.04580: Statistical Software
\end{abstract}


\section{Introduction}

The Maximum Likelihood (ML) method is one of the most important
techniques in statistics and econometrics.
Most statistical and econometric software packages include
ready-made routines for maximum likelihood estimations
of many standard models such as logit, probit, sample-selection,
count-data, or survival models.
However, if practitioners and researchers want to estimate
non-standard models or develop new estimators,
they have to implement the routines
for the maximum likelihood estimations themselves.  Several popular
statistical packages include framework for simplifying the
estimation, allowing the user to easily choose between a number of
optimization algorithms, different ways of calculating
variance-covariance matrices, and easily reporting the results.  The examples
include the \code{ml} command in \textsf{stata} and
the \code{maxlik} library for \textsf{GAUSS}.

\R \citep{r-project09} has included built-in optimization algorithms
since its early days.  The first general-purpose ML framework
(function \code{mle} in package \pkg{stats4}) was added in 2003, and
an extension (\code{mle2} in package {bbmle}) in 2007.  However, both
of these packages are based on a general-purpose optimizer
\code{optim}, which does not include an option to use Newton-Raphson,
and in particular, its variant Berndt-Hall-Hall-Hausman optimizer
\citep{berndt74}, very popular for ML problems.  The package
\pkg{maxLik} \citep{r-maxlik-0.5} is intended to fill this gap.  The
package can be used both by end-users, developing their own
statistical methods, and by package developers, implementing ML
estimators for specific models.  For instance, the packages
\pkg{LambertW} \citep{r-lambertw-0.1.6}, \pkg{mlogit}
\citep{r-mlogit-0.1}, \pkg{sampleSelection} \citep{toomet08}, and
\pkg{truncreg} \citep{r-truncreg-0.1} use the \pkg{maxLik} package for
their maximum likelihood estimations.

The \pkg{maxLik} package is available from
CRAN (\url{http://cran.r-project.org/package=maxLik}),
R-Forge (\url{http://r-forge.r-project.org/projects/maxlik/}), and its
homepage (\url{http://www.maxLik.org/}).  
This paper focuses on the maximum likelihood related
usage of the package; the other features (including finite-difference
derivatives and optimization) are only briefly mentioned.

The paper proceeds as follows: in the next section we explain the
implementation of the package.  Section~\ref{sec:usage} describes the
usage of the package, including the basic and more advanced features, and
Section~\ref{sec:summary} concludes.

% .... exists in GAUSS

\section{Implementation}
\label{sec:implementation}

\maxlik is designed to provide a single, unified interface for
different optimization routines, and to treat the results in a way,
suitable for maximum likelihood (ML) estimation.  The package
implements a flexible multi-purpose Newton-Raphson type optimization
routine \code{maxNR}.  This is used as a basis of \code{maxBHHH}, a
Berndt-Hall-Hall-Hausman type optimizer \citep{berndt74}, popular for ML problems.  In
addition, various methods from the \code{optim} function (from package
\pkg{stats}) are included by the package in a unified way (functions \code{maxBFGS},
\code{maxNM}, and \code{maxSANN}).

The package is designed in two layers.  The first (innermost) is the
optimization (maximization) layer: all the maximization routines are
designed to have a unified and intuitive interface which allows the
user to switch easily between them.  All the main arguments have
identical names and similar order; only method-specific parameters
may vary.  These functions can be used for different types of
optimization tasks, both related and not related to the likelihood.
They return an S3 object of class \code{maxim} including both
estimated parameters and various diagnostics information.

The second layer is the likelihood maximization layer.
The most important tool of this layer is the function \code{maxLik}.
Its main
purpose is to treat the inputs and maximization results in a ML-specific way (for instance,
computing the variance-covariance matrix based on the estimated
Hessian).  The \mbox{\code{maxBHHH}} function belongs to this layer as well,
being essentially a call for \code{maxNR} using information equality
as the way to approximate the Hessian matrix.  A new class \code{maxLik} is added
to the returned maximization object for automatic selection of the ML-related
methods.

The maximization layer supports linear equality and inequality
constraints.  The equality constraints are estimated using sequential
unconstrained maximization technique (SUMT), implemented in the
package.  This is achieved by adding a (initially tiny) penalty term,
related to violation of the constraints, to the target function.
Thereafter on can solve the problem repeatedly with increasing penalty
for every new repetition.  The inequality constraints are delegated to
\code{constrOptim} in the package \pkg{stats}.  The \code{maxLik}
function is aware of the constraints and is able to select a suitable
optimization method, however, no attempt is made to correct the
resulting variance-covariance matrix (just a warning is printed).  As
the constrained optimization should still be considered as
experimental, we refer the reader to the documentation of the package
for examples.

The \pkg{maxLik} package is implemented using S3 classes.
Corresponding methods can handle the likelihood-specific properties
of the estimate including the fact
that the inverse of the negative Hessian is the variance-covariance matrix
of the estimated parameters.
The most important methods for objects of class \code{"maxLik"} are:
\code{summary} for returning (and printing) summary results,
\code{coef} for extracting the estimated parameters,
\code{vcov} for calculating the variance covariance matrix
of the estimated parameters, \code{stdEr} for calculation standard
errors of the estimates (to our knowledge, this is the only ML-related
package which provides this convenience function),
\code{logLik} for extracting the log-likelihood value, and
\code{AIC} for calculating the Akaike information criterion.


\section{Using the \pkg{maxLik} package}
\label{sec:usage}

\subsection{Basic usage}
\label{sec:basic_usage}

As the other packages in \R, the
\pkg{maxLik} package must be installed and loaded before it can be used.
The following command loads the \pkg{maxLik} package:
<<>>=
library( maxLik )
@
The most important user interface of the \pkg{maxLik} package
is a function with the (same) name \code{maxLik}.  It is mostly a wrapper
for different optimization routines with a few additional features,
useful for ML estimations.
This function has two mandatory arguments
<<eval=FALSE>>=
maxLik( logLik, start )
@
The first argument (\code{logLik}) must be a function
that calculates the log-likelihood value as a function of the
parameter (usually parameter vector).
The second argument (\code{start}) must be a vector of starting values.

We demonstrate the usage of the \pkg{maxLik} package by a simple example:
we fit a normal distribution by maximum likelihood.
First, we generate a vector ($x$) of 100 draws
from a normal distribution with a mean of $\mu = 1$
and a standard deviation of~$\sigma = 2$:
<<>>=
set.seed( 123 )
x <- rnorm( 100, mean = 1, sd = 2 )
@
%
The logarithm of the probability density of the sample (i.e.\ the
log-likelihood function) is
\begin{equation}
\log( L( x; \mu, \sigma ) ) =
- \frac{1}{2} N \log ( 2 \pi ) - N \log ( \sigma )
- \frac{1}{2} \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^2}.
\end{equation}

Given the log-likelihood function above,
we create an \textsf{R} function
that calculates the log-likelihood value.
Its first argument must be the vector of the parameters to be estimated
and it must return the log-likelihood value.%
\footnote{
Alternatively, it could return a numeric vector
where each element is the log-likelihood value
corresponding to an (independent) individual observation (see below).
}
The easiest way to implement this log-likelihood function
is to use the capabilities of the function
\code{dnorm}: 
<<>>=
logLikFun <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))
}
@
For the actual estimation we
set the first argument (\code{logLik}) equal to the log-likelihood function
that we have defined above (\code{logLikFun})
and we use the parameters of a standard normal distribution
($\mu = 0$, $\sigma = 1$) as starting values (argument \code{start}).
Assigning names to the vector of starting values is not required
but has the advantage
that the returned estimates have also names,
which improves the readability of the results.\footnote{Alternatively,
if all the components of the parameter vector have standardized names,
one may prefer using \code{with}-construct for evaluating the
likelihood expression.  We are grateful to a referee for this suggestion.}
<<>>=
mle <- maxLik( logLik = logLikFun, start = c( mu = 0,  sigma = 1 ) )
summary( mle )
@
For convenience, the estimated parameters can be accessed by the
\code{coef} method and standard errors by the \code{stdEr} method:
<<code:coef-std>>=
coef(mle)
stdEr(mle)
@ 
As expected, the estimated parameters are equal to the mean
and the standard deviation (without correction for degrees of freedom)
of the values in vector~$x$.\footnote{%
The function \code{all.equal} considers two elements as equal
if either the mean absolute difference or the mean relative difference
is smaller than the tolerance
(defaults to \code{\mbox{.Machine\$double.eps\^{ }0.5}},
usually around $1.5 \cdot 10^{-8}$).
}
<<>>=
all.equal( coef( mle ), c( mean( x ),
   sqrt( sum( ( x - mean( x ) )^2 ) / 100 ) ),
   check.attributes = FALSE )
@

If no analytical gradient is provided by the user,
finite-difference gradient and Hessian are calculated
by the functions \code{numericGradient} and \code{numericNHessian},
which are also included in the \pkg{maxLik} package.
While the maximisation of the likelihood function of this simple model
works well with finite-difference gradients and Hessians, 
this may not be the case for more complex models.  Numerical
derivatives may be costly to compute, and, even more, they may turn out to
be noisy and unreliable. In this way
numerical derivatives might either slow down the estimation or even impede the
convergence. In these cases, the user is recommended to either provide
analytical derivatives or switch to a more robust estimation method,
such as Nelder-Mead, which is not based on gradients.

The gradients of the log-likelihood function with respect to the two
parameters are
\begin{align}
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \mu } & =
   \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^2}\\
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \sigma } & =
   - \frac{ N }{ \sigma }
   + \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^3}.
\end{align}
This can be calculated in \textsf{R} by the following function:
<<>>=
logLikGrad <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikGradValues <- numeric( 2 )
   logLikGradValues[ 1 ] <- sum( ( x - mu ) / sigma^2 )
   logLikGradValues[ 2 ] <- - N / sigma + sum( ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
@
Now we call the \code{maxLik} function and use argument \code{grad}
to specify the function that calculates the gradients:
<<>>=
mleGrad <- maxLik( logLik = logLikFun, grad = logLikGrad,
   start = c( mu = 0, sigma = 1 ) )
summary( mleGrad )
all.equal( logLik( mleGrad ), logLik( mle ) )
all.equal( coef( mleGrad ), coef( mle ) )
all.equal( vcov( mleGrad ), vcov( mle ) )
@
Providing analytical gradients has no (relevant)
effect on the estimates
but their covariance matrix and standard errors are slightly different.

The analytic Hessian of the log-likelihood function can be provided by
the argument \code{hess}.
If the user provides a function to calculate the gradients
but does not use argument \code{hess},
the Hessians are calculated by the function
\code{numericHessian} using finite-difference approach.\footnote{Only the Newton-Raphson 
  algorithm uses the Hessian directly for obtaining the estimates.  The
  other algorithms ignore it during the estimation but use it for computing the
  final variance-covariance matrix.}
The elements of the Hessian matrix of the log-likelihood function
for the normal distribution are
\begin{align}
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \mu )^2 } & =
   - \frac{ N }{\sigma^2}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ \partial \mu \; \partial \sigma } & =
   - 2 \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^3}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \sigma )^2 } & =
   \frac{ N }{ \sigma^2 }
   - 3 \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^4}.
\end{align}
They can be calculated in \textsf{R} using the following function:
<<>>=
logLikHess <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikHessValues <- matrix( 0, nrow = 2, ncol = 2 )
   logLikHessValues[ 1, 1 ] <- - N / sigma^2
   logLikHessValues[ 1, 2 ] <-  - 2 * sum( ( x - mu ) / sigma^3 )
   logLikHessValues[ 2, 1 ] <- logLikHessValues[ 1, 2 ]
   logLikHessValues[ 2, 2 ] <- N / sigma^2 - 3 * sum( ( x - mu )^2 / sigma^4 )
   return( logLikHessValues )
}
@
Now we call the \code{maxLik} function with argument \code{hess}
set to this function:
<<>>=
mleHess <- maxLik( logLik = logLikFun, grad = logLikGrad,
   hess = logLikHess, start = c( mu = 0, sigma = 1 ) )
summary( mleHess )
all.equal( list( logLik( mleHess ), coef( mleHess ), vcov( mleHess ) ),
   list( logLik( mleGrad ), coef( mleGrad ), vcov( mleGrad ) ) )
@
Providing an analytical Hessian has no (relevant) effect
on the outcome of the ML estimation in our simple example.  However,
as in the case of finite-difference gradients, finite-difference Hessians may turn out to
be slow and unreliable.


\subsection{Optimisation Methods}

The \code{maxLik} function allows the user to select between five
optimization algorithms by argument \code{method}.
It defaults to
\code{"NR"} for the Newton-Raphson algorithm.  The other options are
\code{"BHHH"} for Berndt-Hall-Hall-Hausman \citep{berndt74},
\code{"BFGS"} for Broyden-Fletcher-Goldfarb-Shanno
\citep{broyden70,fletcher70,goldfarb70,shanno70},
\code{"NM"} for Nelder-Mead \citep{nelder65}, and
\code{"SANN"} for simulated-annealing \citep{belisle92}.
The Newton-Raphson algorithm uses (finite-difference or analytical)
gradients and Hessians;
the BHHH and BFGS algorithms use only (finite-difference or analytical) gradients;
the NM and SANN algorithms use neither gradients nor Hessians
but only function values.
The gradients and Hessians provided by the user through
the arguments \code{grad} and \code{hess} are always accepted in order
to make the switching the algorithms easier, but they are ignored
if the selected method does not require this information,
for instance, the argument \code{hess} for the
Berndt-Hall-Hall-Hausman method.\footnote{Even if the optimization
  method itself does not make use of the Hessian, this information is
  used (except for \code{"BHHH"}) for computing the final
  variance-covariance matrix.}
In this way the user can easily change the optimisation method
without changing the arguments.

In general, it is advisable to use all the available information,
e.g.\ to use the \code{"NR"} method if both analytical gradients and Hessians are available, one
of the gradient-based methods (either \code{"BHHH"} or \code{"BFGS"})
if analytical gradients but no Hessians are available, and to resort to the value-only
methods only if gradients are not provided.  

\subsubsection{Berndt-Hall-Hall-Hausman (BHHH)}
The BHHH method uses the fact, that in case of independent
observations, the Hessian can be approximeted by the negative of the 
expectation of the outer product of the gradient (score) vector.  In
order to calculate the expectation, the algorithm needs gradient
vectors by individual observations.  This can be achieved either by
providing a corresponding gradient function (see below) or by providing a
vector of individual observation-specific likelihood values by the
log-likelihood function itself (if no analytical gradients are provided).
In the latter case, finite-difference gradients are used to calculate the Hessian.

We modify our example above accordingly: instead of returning a single
summary value of log-likelihood, we return the values by individual
observations by simply removing the \code{sum} operator:
<<>>=
logLikFunInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   dnorm(x, mean=mu, sd=sigma, log=TRUE)
}
mleBHHH <- maxLik( logLik = logLikFunInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
summary( mleBHHH )
all.equal( logLik( mleBHHH ), logLik( mle ) )
all.equal( coef( mleBHHH ), coef( mle ) )
all.equal( vcov( mleBHHH ), vcov( mle ) )
@
While the estimated parameters and the corresponding log-likelihood value
are virtually identical to the previous estimates,
the covariance matrix of the estimated parameters is slightly
different.  This is because the outer product approximation may differ
from the derivative-based Hessian on finite samples
\citep{calzolari+fiorentini1993}. 

If the user chooses to provide analytical gradients,
the function that calculates the gradients (argument \code{grad})
must return a numeric matrix,
where each column represents the gradient with respect to the corresponding
element of the parameter vector
and each row corresponds to an individual observation.  Note that in
this case, the log-likelihood function itself does not have to return
a vector of log-likelihood values by observations, as the gradient by
observation is supplied by the \code{grad} function.  In the
following example, we define a function that calculates the gradient matrix
and we estimate the model by BHHH method
using this gradient matrix and the single summed log-likelihood from the
Newton-Raphson example.
<<>>=
logLikGradInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   logLikGradValues <- cbind( ( x - mu ) / sigma^2,
      - 1 / sigma + ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
mleGradBHHH <- maxLik( logLik = logLikFun, grad = logLikGradInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
all.equal( list( logLik( mleBHHH ), coef( mleBHHH ), vcov( mleBHHH ) ),
   list( logLik( mleGradBHHH ), coef( mleGradBHHH ), vcov( mleGradBHHH ) ) )
@
Estimates based on finite-difference gradients and analytical gradients are
virtually identical.


\subsubsection{Nelder-Mead (NM) and other methods}

The other maximization methods: Nelder-Mead,
Broyden-Fletcher-Goldfarb-Shanno, and Simulated Annealing, are
implemented by a call to the the \code{optim} function in package
\pkg{stats}.  In order to retain compatibility with the BHHH method,
all these methods accept the log-likelihood function returning a
vector of individual likelihoods (these are summed internally).
A function to compute a gradient matrix with gradients of individual observations, is accepted as well.
If the user does
not provide gradients, the gradients are computed by finite-difference
approach.

To give the reader a taste, we give an example using the
gradientless Nelder-Mead method:
<<>>=
mleNM <- maxLik( logLik = logLikFun,
   start = c( mu = 0, sigma = 1 ), method = "NM" )
summary( mleNM )
logLik( mleNM ) - logLik( mleGrad )
all.equal( coef( mleNM ), coef( mleGrad ) )
all.equal( vcov( mleNM ), vcov( mleGrad ) )
@
The estimates and the covariance matrix
obtained from the Nelder-Mead algorithm slightly differ
from previous results using other algorithms
and the fit (log-likelihood value) of the model
is slightly worse (smaller) than for the previous models.

Note that although the \code{summary} method reports the number of iterations for
all the methods, the meaning of ``iteration'' may be completely
different for different optimization techniques.


\subsection{More advanced usage}
\label{sec:advanced_usage}

The \code{maxLik} function supports a variety of other arguments, most of
which are passed to the selected optimizer.  Among the most important
ones is \code{print.level} which controls the output of debugging
information (0 produces no debugging output, larger numbers produce
more output). 
Optimization methods may also support various additional features, such as
the temperature-related parameters for \code{maxSANN}.  Those will not
be discussed here, the interested reader is
referred to the documentation of the corresponding optimizer.

\subsubsection{Fixed parameter values}
\label{sec:fixed_parameter_values}

Below, we
demonstrate how it is possible to fix certain parameters we would
like to handle as constants in the optimization process.  This feature
is implemented in \code{maxNR} (and hence supported by \code{maxBHHH}
as well), which is a part of \pkg{maxLik}.

Let us return to our original task of estimating the parameters of a
normal sample.  However, assume we know that the true value of $\sigma
= 2$.  Instead of writing a new likelihood function, we may use the
existing one while specifying that $\sigma$ is fixed at 2.  This is
done via argument \code{activePar} of \code{maxNR}.  It is a logical
vector of length equal to that of the parameter vector which specifies which
components are allowed to change (are ``active'').  The other,
``fixed'' parameters are treated as known constants, always
retaining their initial value.  Certain parameters can be kept fixed
by either specifying the free ones (by argument \code{activePar}), or
fixed ones (argument \code{fixedPar}).  Both must be index vector, if
both are specified, the latter are used (and a warning issued).
So, as $\sigma$
was the second parameter, we may call:
<<code:activePar>>=
summary(maxLik(logLikFun, start=c(mu=0, sigma=2), 
               activePar=c(TRUE, FALSE)))
@ 
As we can see, the $\sigma$ is indeed exactly 2.  Its standard error
is set to zero while the $t$-value is not defined.  Note also
that the estimate of $\mu$ is unchanged (indeed, ML estimate of it is
still the sample average) while the estimated standard error is
different.  Obviously, log-likelihood is lower in the constrained
space, although the reader may verify that allowing $\sigma$ to be free
is an insignificant improvement according to the likelihood ratio
test.

\subsubsection{Automatic fixing of parameter values}

Next, we demonstrate, how it is possible to fix a parameter
automatically during the computations when using \code{maxNR} optimizer.
This may be useful when
estimating a large
number of similar models where parameters occasionally converge toward
the boundary of the parameter space or another problematic region.  Most popular optimization
algorithms do not work well in such circumstances.  We demonstrate
this feature
by estimating the parameters of a mixture model of two normal distributions on a sample,
drawn from a single normal distribution.  Note that this example
is highly dependent on the initialization of random number
generator and initial values for the estimation.  This happens often
with mixture models.

First, we demonstrate the outcome on a mixture of two distinct components.
We generate a mixture of normals:
<<code:generate_mixture>>=
xMix <- c(rnorm(500), rnorm(500, mean=1))
@ 
Hence \code{xMix} is a 50\%-50\% mixture of two normal distributions: the
first one has mean equal to 0 and the second has mean 1 (for
simplicity, we fix the standard
deviations to 1).  The log-likelihood
of a mixture is simply
\begin{equation}
  \loglik = \sum_{i=1}^{N} 
  \log( \varrho \phi (x_{i} - \mu_{1}) +
  (1 - \varrho) \phi(x_{i} - \mu_{2})),
\end{equation}
where $\varrho$ is the percentage of the first component in the
mixture and $\phi(\cdot)$ is the standard normal density function.  We
implement this in \textsl{R}:
<<code:logLikMix>>=
logLikMix <- function(param) {
   rho <- param[1]
   if(rho < 0 || rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
}
@ 
Note that the function includes checking for feasible parameter
values.  If $\varrho \not\in [0,1]$, it returns \code{NA}.  This
signals to the optimizer that the attempted parameter value was out
of range, and forces it to find a new one (closer to the previous
value).  This is a way of implementing box constraints in the
log-likelihood function.  The results look like the following:
<<code:mix1>>=
summary(m1 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
The estimates replicate the true parameters within the confidence
intervals; however compared to the examples in Section~\ref{sec:basic_usage}, the
standard errors are rather large (note that the sample here includes
1000 observations instead of mere 100 above).  This is a common
outcome while estimating mixture models.

Let us now
replace the mixture by a pure normal sample
<<code:generate_pure_normal>>=
xMix <- rnorm(1000)
@ 
and estimate it using the same log-likelihood implementation:
<<code:mix2>>=
summary(m2 <- maxLik(logLikMix, start=c(rho=0.5, mu1=0, mu2=0.01)))
@ 
Although the estimates seem to be close to the correct point in the
parameter space: mixture of 100\% normal with mean 0 and
0\% with mean 1, the Hessian matrix is singular and hence standard
errors are infinite.  This is because both components of the mixture
converge to the same value and hence $\varrho$ is not identified.  
Hence we have no way establishing whether
the common mean of the sample is, in fact, significantly different from
0.
If the estimation is done by hand, it would be easy
to fix $\varrho$ as in the example above.  However, this may
not be a suitable approach if we want to run a large
number of similar computations on different samples.  In that case the
user may want to consider signalling the \code{maxNR} routine that the
parameters should be treated as fixed.
We may rewrite the log-likelihood function as follows:
<<code:logLikMix2>>=
freePar <- rep(TRUE, 3)
logLikMix1 <- function(param) {
   rho <- param[1]
   if(rho < 0 | rho > 1)
       return(NA)
   mu1 <- param[2]
   mu2 <- param[3]
   constPar <- NULL
   if(freePar[1] & (abs(mu1 - mu2) < 1e-3)) {
      rho <- 1
      constPar <- c(1, 3)
      newVal <- c(1, 0)
      fp <- freePar
      fp[constPar] <- FALSE
      assign("freePar", fp, inherits=TRUE)
   }
   ll <- log(rho*dnorm(xMix - mu1) + (1 - rho)*dnorm(xMix - mu2))
   if(!is.null(constPar)) {
      attr(ll, "constPar") <- constPar
      attr(ll, "newVal") <- list(index=constPar, val=newVal)
   }
   ll
}
@ 
We have introduced three changes into the log-likelihood function.
\begin{itemize}
\item First, while changing the fixed parameters at run-time, we have to
  keep track of the process.  This is why we introduce \code{freePar}
  \emph{outside} the function itself, as it has to retain its value over
  successive calls to the function.
\item Next novelty is related to checking the proximity to the region
  of trouble: \code{if(freePar[1] \& (abs(mu1 - mu2) < 1e-3))}.
  Hence, if we haven't fixed the first parameter ($\varrho$) yet (this
  is what \code{freePar[1]} keeps track of), and the estimated means
  of the components are close to each other, we set $\varrho$ to 1.
  This means we assume the mixture contains only component 1.  Note
  that because $\mu_{2}$ is undefined as $\varrho = 1$, we also have
  to fix that parameter.  We mark both of these parameters in the
  parameter vector to be fixed (\code{constPar <- c(1, 3)}), and
  provide the new values for them (\code{newVal <- c(1, 0)}).
\item As the last step, we inform \code{maxNR} algorithm of our
  decision by setting respective attributes to log-likelihood.  Two
  attributes are used: \code{constPar} informs the algorithm that
  corresponding parameters in the parameter vector must be treated as
  fixed from now on; and \code{newVal} (which contains two components -- indices and
  values) informs which parameters have new values.  It
  is possible to fix parameters without changing the values by
  setting the \code{constPar} attribute only.
\end{itemize}

Now the estimation results look like:
<<code:mix3>>=
summary(m <- maxLik(logLikMix1, start=c(rho=0.5, mu1=0.1, mu2=0)))
@ 
With parameters \code{rho} and \code{mu2} fixed, the resulting
one-component model has small standard errors.


\section{Summary and Outlook}
\label{sec:summary}

The \pkg{maxLik} package fills an existing gap in the \R statistical environment
and provides a convenient interface
for maximum likelihood estimations
--- both for end users and package developers.
Although \R has included general-purpose optimizers and more specific Maximum
Likelihood tools for a long time,
the \pkg{maxLik} package has three important features
that are not available in at least some of the alternatives:
First, the covariance matrix of the estimates can be calculated automatically.
Second, the user can easily switch between different optimization
algorithms.
Third, the package provides the Berndt-Hall-Hall-Hausman (BHHH) algorithm,
a popular optimization method which is available only for
likelihood-type problems.

In the future,
we plan to add support for further optimisation algorithms,
e.g.\ function \code{nlm},
the "L-BFGS-B" algorithm in function \code{optim}
   that allows for box constraints,
function \code{nlminb}
   that uses PORT routines \citep{gay90}
   and also allows for box constraints,
or
function \code{ucminf} of the \pkg{ucminf} package
\citep{r-ucminf-1.0}.  
% mention ROI here? ....
Another future extension includes comprehensive handling of constrained
maximum likelihood problems.

We hope that these improvements will make the \pkg{maxLik} package
even more attractive for users and package writers.

%\begin{acknowledgements}
%\end{acknowledgements}

\bibliographystyle{spbasic}
\bibliography{references}

% TODO:

% * Merge maxBFGS, maxNM, maxSANN (if have time)

% * add activePar for optim-based methods

\end{document}
