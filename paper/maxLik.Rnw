\documentclass[smallextended,natbib]{svjour3}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{bm}
\smartqed

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}
\setlength{\emergencystretch}{3em}

\journalname{Computational Statistics}

\begin{document}

\title{maxLik: A Package for Maximum Likelihood Estimation in \textsf{R}}

\titlerunning{maxLik: Maximum Likelihood Estimation}

\author{Arne Henningsen \and Ott Toomet}

\institute{
   Arne Henningsen \at
   Institute of Food and Resource Economics, University of Copenhagen\\
   Rolighedsvej 25, 1958 Frederiksberg C, Denmark\\
   Tel.: +45-353-32274\\
   \email{arne@foi.dk}
\and
   Ott Toomet \at
   Department of Economics, University of Tartu (Estonia)\\
   Department of Economics, Aarhus School of Business,
      University of Aarhus (Denmark)
}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}
Insert your abstract here.
\keywords{Maximum Likelihood \and Optimisation}
\end{abstract}


\section{Introduction}

The Maximum Likelihood (ML) method is one of the most important
techniques in statistics and econometrics.
Most statistical and econometric software packages include
ready-made routines for maximum likelihood estimations
of many standard models such as logit, probit, sample-selection,
count-data, or survival models.
However, if practitioners and researchers want to estimate
non-standard models or develop new models,
they have to implement the routines
for the maximum likelihood estimations themselves.
In this case, the \pkg{maxLik} package~\citep{r-maxlik-0.5}
for the statistical software environment \textsf{R} \citep{r-project09}
might be very helpful.
Furthermore, developers that implement routines for maximum likelihood
estimations of specific models
can reduce their effort
by using the \pkg{maxLik} package for the actual parameter estimation.
For instance, the packages
\pkg{LambertW} \citep{r-lambertw-0.1.6},
\pkg{mlogit} \citep{r-mlogit-0.1},
\pkg{sampleSelection} \citep{toomet08}, and
\pkg{truncreg} \citep{r-truncreg-0.1}
use the \pkg{maxLik} package
for their maximum likelihood estimations.

The \pkg{maxLik} package is available from
CRAN (\url{http://cran.r-project.org/package=maxLik}),
R-Forge (\url{http://r-forge.r-project.org/projects/maxlik/}), and its
homepage (\url{http://www.maxLik.org/}).


\section{Using the \pkg{maxLik} package}

The \pkg{maxLik} package must be installed and loaded before it can be used.
The following command loads the \pkg{maxLik} package:
<<>>=
library( maxLik )
@
The most important user interface of the \pkg{maxLik} package
is a function with the (same) name \code{maxLik}.
This function has two mandatory arguments
<<eval=FALSE>>=
maxLik( logLik, start )
@
The first argument (\code{logLik}) must be a function
that calculates the log-likelihood value.
The second argument (\code{start}) must be a vector of starting values.

We demonstrate the usage of the \pkg{maxLik} package by a simple example:
we fit a normal distribution by maximum likelihood.
First, we generate a vector ($x$) of 100 draws
from a normal distribution with a mean of $\mu = 1$
and a standard deviation of~$\sigma = 2$:
<<echo=FALSE,results=hide>>=
set.seed( 123 )
@
<<>>=
x <- rnorm( 100, mean = 1, sd = 2 )
@
%
The probability density function of a standard normal distribution
for a value $x_i$ is
\begin{equation}
P(x_i) =
\frac{1}{\sigma \sqrt{2 \pi}}
\exp \left( - \frac{( x_i - \mu )^2}{2 \sigma^2} \right).
\end{equation}
Hence, the likelihood function for $\mu$, $\sigma$,
and the values in vector $x = ( x_1, \ldots, x_N )$ is
\begin{equation}
L( x, \mu, \sigma ) = \prod_{i=1}^N
\frac{1}{\sigma \sqrt{2 \pi}}
\exp \left( - \frac{( x_i - \mu )^2}{2 \sigma^2} \right)
\end{equation}
and its logarithm (i.e.\ the log-likelihood function) is
\begin{equation}
\log( L( x, \mu, \sigma ) ) =
- \frac{1}{2} N \log ( 2 \pi ) - N \log ( \sigma )
- \frac{1}{2} \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^2}.
\end{equation}

Given the log-likelihood function above,
we create an \textsf{R} function
that calculates the log-likelihood value.
Its first argument must be the vector of the parameters to be estimated
and it must return the log-likelihood value.%
\footnote{
Alternatively, it could return a numeric vector
where each element is the log-likelihood value
corresponding to an individual observation (see below).
}
<<>>=
logLikFun <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikValue <- -0.5 * N * log( 2 * pi ) - N * log( sigma ) -
      0.5 * sum( ( x - mu )^2 / sigma^2 )
   return( logLikValue )
}
@
Now, we use the \code{maxLik} function
to find the values for the $\mu$ and $\beta$ parameters
that give the best fit, i.e.\ the largest value of the (log-)
likelihood function.
We set the first argument (\code{logLik}) equal to the log-likelihood function
that we have defined above (\code{logLikFun})
and we use the parameters of a standard normal distribution
($\mu = 0$, $\sigma = 1$) as starting values (argument \code{start}).
Assigning names to the vector of starting values is not required
but has the advantage
that the returned estimates have also names,
which improves the readability of the results.
<<>>=
mle <- maxLik( logLik = logLikFun, start = c( mu = 0,  sigma = 1 ) )
summary( mle )
@
As expected, the estimated parameters are equal to the mean
and the standard deviation (without correction for degrees of freedom)
of the values in vector~$x$.
<<>>=
all.equal( coef( mle ), c( mean( x ),
   sqrt( sum( ( x - mean( x ) )^2 ) / 100 ) ),
   check.attributes = FALSE )
@

If no analytical gradients are provided by the user,
numerical gradients and numerical Hessians are calculated
by the functions \code{numericGradient} and \code{numericNHessian},
which are also included in the \pkg{maxLik} package.
While the maximisation of the likelihood function of this simple model
works well with numerical gradients and Hessians,
providing analytical gradients could increase the speed and probability
of convergence in more complex models.
The gradients of the log-likelihood function of the standard normal distribution
are
\begin{align}
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \mu } & =
   \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^2}\\
\frac{\partial \log( L( x, \mu, \sigma ) )}{ \partial \sigma } & =
   - \frac{ N }{ \sigma }
   + \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^3}.
\end{align}
These gradients are calculated in \textsf{R} by following function:
<<>>=
logLikGrad <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikGradValues <- numeric( 2 )
   logLikGradValues[ 1 ] <- sum( ( x - mu ) / sigma^2 )
   logLikGradValues[ 2 ] <- - N / sigma + sum( ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
@
Now we call the \code{maxLik} function and use argument \code{grad}
to specify the function that calculates the gradients:
<<>>=
mleGrad <- maxLik( logLik = logLikFun, grad = logLikGrad,
   start = c( mu = 0, sigma = 1 ) )
summary( mleGrad )
all.equal( logLik( mleGrad ), logLik( mle ) )
all.equal( coef( mleGrad ), coef( mle ) )
all.equal( vcov( mleGrad ), vcov( mle ) )
@
Providing analytical gradients has no (relevant)%
\footnote{%
The function \code{all.equal} considers two elements as equal
if either the mean absolute difference or the mean relative difference
is smaller than the tolerance
(defaults to \code{.Machine\$double.eps\^{ }0.5},
usually around \Sexpr{round(.Machine[["double.eps"]]^0.5,9)}).
}
effect on the estimates
but their covariance matrix and standard errors are slightly different.

The user can use argument \code{hess} of the \code{maxLik} function
to provide a function that returns the Hessian matrix
of the log-likelihood function.
If the user provides a function to calculate the gradients
but does not use argument \code{hess},
the Hessians are calculated numerically by the function
\code{numericHessian}.
The elements of the Hessian matrix of the log-likelihood function
of the normal distribution are
\begin{align}
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \mu )^2 } & =
   - \frac{ N }{\sigma^2}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ \partial \mu \; \partial \sigma } & =
   - 2 \sum_{i=1}^N \frac{( x_i - \mu )}{\sigma^3}\\
\frac{\partial^2 \log( L( x, \mu, \sigma ) )}{ ( \partial \sigma )^2 } & =
   \frac{ N }{ \sigma^2 }
   - 3 \sum_{i=1}^N \frac{( x_i - \mu )^2}{\sigma^4}.
\end{align}
They can be calculated in \textsf{R} using the following function:
<<>>=
logLikHess <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   N <- length( x )
   logLikHessValues <- matrix( 0, nrow = 2, ncol = 2 )
   logLikHessValues[ 1, 1 ] <- - N / sigma^2
   logLikHessValues[ 1, 2 ] <-  - 2 * sum( ( x - mu ) / sigma^3 )
   logLikHessValues[ 2, 1 ] <- logLikHessValues[ 1, 2 ]
   logLikHessValues[ 2, 2 ] <- N / sigma^2 - 3 * sum( ( x - mu )^2 / sigma^4 )
   return( logLikHessValues )
}
@
Now we call the \code{maxLik} function with argument \code{hess}
set to this function:
<<>>=
mleHess <- maxLik( logLik = logLikFun, grad = logLikGrad,
   hess = logLikHess, start = c( mu = 0, sigma = 1 ) )
summary( mleHess )
all.equal( list( logLik( mleHess ), coef( mleHess ), vcov( mleHess ) ),
   list( logLik( mleGrad ), coef( mleGrad ), vcov( mleGrad ) ) )
@
Providing an analytical Hessian has no (relevant) effect
on the outcome of the ML estimation in our simple example.

\subsection{Optimisation Methods}

The user can use argument \code{method} to choose between
five different optimisation algorithms.
Argument \code{method} defaults to
\code{"NR"} for the Newton-Raphson algorithm, but it can be also
\code{"BHHH"} for Berndt-Hall-Hall-Hausman \citep{berndt74},
\code{"BFGS"} for Broyden-Fletcher-Goldfarb-Shanno
\citep{broyden70,fletcher70,goldfarb70,shanno70},
\code{"NM"} for Nelder-Mead \citep{nelder65}, or
\code{"SANN"} for simulated-annealing \citep{belisle92}.
The Newton-Raphson algorithm uses (numerical or analytical)
gradients and Hessians;
the BHHH and BFGS algorithms use only (numerical or analytical) gradients;
the NM and SANN algorithms use neither gradients nor Hessians
but only function values.
The gradients and Hessians provided by a user through
the arguments \code{grad} and \code{hess} are ignored
if the selected method does not require this information,
e.g.\ argument \code{hess} for the Berndt-Hall-Hall-Hausman method.
Hence, the user can easily change the optimisation method
without changing the arguments.

\subsubsection{Berndt-Hall-Hall-Hausman (BHHH)}
The BHHH method requires
that the log-likelihood function (argument \code{logLik})
must return a vector of log-likelihood values,
where each element corresponds to an individual observation.
<<>>=
logLikFunInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   logLikValue <- -0.5 * log( 2 * pi ) - log( sigma ) -
      0.5 * ( x - mu )^2 / sigma^2
   return( logLikValue )
}
mleBHHH <- maxLik( logLik = logLikFunInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
summary( mleBHHH )
all.equal( logLik( mleBHHH ), logLik( mle ) )
all.equal( coef( mleBHHH ), coef( mle ) )
all.equal( vcov( mleBHHH ), vcov( mle ) )
@
While the estimated parameters and the corresponding log-likelihood value
are (almost) equal to the previous estimates,
the covariance matrix of the estimated parameters is slightly different.

If the user wants to provide analytical gradients,
the function that calculates the gradients (argument \code{grad})
must return a numeric matrix,
where each column corresponds to an estimated parameter
and each row corresponds to an individual observation.
<<>>=
logLikGradInd <- function( param ) {
   mu <- param[ 1 ]
   sigma <- param[ 2 ]
   logLikGradValues <- cbind( ( x - mu ) / sigma^2,
      - 1 / sigma + ( x - mu )^2 / sigma^3 )
   return( logLikGradValues )
}
mleGradBHHH <- maxLik( logLik = logLikFun, grad = logLikGradInd,
   start = c( mu = 0, sigma = 1 ), method = "BHHH" )
all.equal( list( logLik( mleBHHH ), coef( mleBHHH ), vcov( mleBHHH ) ),
   list( logLik( mleGradBHHH ), coef( mleGradBHHH ), vcov( mleGradBHHH ) ) )
@
There are no (relevant) differences between
the estimates based on numerical gradients and
the estimates based on analytical gradients.

\subsubsection{Broyden-Fletcher-Goldfarb-Shannon (BFGS)}
<<>>=
mleGradBFGS <- maxLik( logLik = logLikFun, grad = logLikGrad,
   start = c( mu = 0, sigma = 1 ), method = "BFGS" )
summary( mleGradBFGS )
all.equal( logLik( mleGradBFGS ), logLik( mleGrad ) )
all.equal( coef( mleGradBFGS ), coef( mleGrad ) )
all.equal( vcov( mleGradBFGS ), vcov( mleGrad ) )
@
The differences between
the estimation results of the BFGS algorithm and
the results of the Newton-Raphson algorithm are negligible.

\subsubsection{Nelder-Mead (NM)}
<<>>=
mleNM <- maxLik( logLik = logLikFun,
   start = c( mu = 0, sigma = 1 ), method = "NM" )
summary( mleNM )
logLik( mleNM ) - logLik( mleGrad )
all.equal( coef( mleNM ), coef( mleGrad ) )
all.equal( vcov( mleNM ), vcov( mleGrad ) )
@
The estimates and the covariance matrix
obtained from the Nelder-Mead algorithm slightly differ
from previous results using other algorithms
and the fit (log-likelihood value) of the model
is slightly worse (smaller) than for the previous models.

\subsubsection{Simulated annealing (SANN)}
<<echo=FALSE,results=hide>>=
set.seed( 123 )
@
<<>>=
mleSANN <- maxLik( logLik = logLikFun,
   start = c( mu = 0, sigma = 1 ), method = "SANN" )
summary( mleSANN )
logLik( mleSANN ) - logLik( mleGrad )
all.equal( coef( mleSANN ), coef( mleGrad ) )
all.equal( vcov( mleSANN ), vcov( mleGrad ) )
@
Also the estimates and the covariance matrix
obtained from the SANN algorithm slightly differ
from previous results
and the fit (log-likelihood value) of the model
is also slightly worse (smaller) than for the previous models.


\section{Implementation}

The \code{maxLik} function delegates the actual optimisation
to the functions
\code{maxNR}, \code{maxBHHH}, \code{maxBFGS}, \code{maxNM}, and
\code{maxSANN},
for the ``NR'', ``BHHH'', ``BFGS'', ``NM'', and ``SANN'' algorithms,
respectively.
While the actual optimisation in
\code{maxBFGS}, \code{maxNM}, and \code{maxSANN}
is done by \code{optim},
the Newton-Raphson algorithm is implemented
in the function \code{maxNR} itself.
The actual optimisation in \code{maxBHHH}
is done by \code{maxNR}.

As already mentioned above, the \code{maxLik}
wrapper capabilities are designed in a transparent way, so that the
user can easily swap the methods without changing the
arguments.


The \pkg{maxLik} package is implemented using S3 classes.
The \code{maxLik} wrapper returns a list of class \code{"maxLik"}.
Corresponding methods can handle the likelihood-specific properties
of the estimate including the fact
that the inverse of the negative Hessian is the variance-covariance matrix
of the estimated parameters.
The most important methods for objects of class \code{"maxLik"} are:
\code{summary} for returning (and printing) summary results,
\code{coef} for extracting the estimated parameters,
\code{vcov} for calculating the variance covariance matrix
of the estimated parameters,
\code{logLik} for extracting the log-likelihood value, and
\code{AIC} for calculating the Akaike information criterion.


\section{Summary and Outlook}

The \pkg{maxLik} package provides a convenient interface
for maximum likelyhood estimations
--- both for end users and package developers.
Alternatively, the \code{mle} function
\citep[package \pkg{stats4},][]{r-project09}
and general-purpose solvers can be used for maximum likelyhood estimations
in \textsf{R}.
However, the \pkg{maxLik} package has three important features
that are not available in at least some of the alternatives:
First, the covariance matrix of the estimates can be calculated automatically.
Second, the user can easily switch between different optimisation algorithms.
Third, the package provides the Berndt-Hall-Hall-Hausman (BHHH) algorithm,
which is very popular for maximum likelihood estimations.

In the future,
we plan to add support for further optimisation algorithms,
e.g.\ function \code{nlm},
the "L-BFGS-B" algorithm in function \code{optim}
   that allows for box constraints,
function \code{nlminb}
   that uses PORT routines \citep{gay90}
   and also allows for box constraints,
function \code{constrOptim}
   that allows for linear inequality constraints, or
function \code{ucminf} of the \pkg{ucminf} package \citep{r-ucminf-1.0}.

We hope that these improvements make the \pkg{maxLik} package
even more attractive for users and package writers.

%\begin{acknowledgements}
%\end{acknowledgements}

\bibliographystyle{spbasic}
% \bibliography{agrarpol}
\bibliography{references}

\end{document}
